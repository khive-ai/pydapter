{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pydapter","text":"<p>pydapter is a powerful trait + adapter toolkit for pydantic models, featuring a comprehensive field system and protocol-based design patterns.</p>"},{"location":"#overview","title":"Overview","text":"<p>pydapter provides a lightweight, flexible way to adapt Pydantic models to various data sources and sinks. It enables seamless data transfer between different formats and storage systems while maintaining the type safety and validation that Pydantic provides.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":""},{"location":"#field-system-new-in-v030","title":"\ud83c\udfd7\ufe0f Field System (New in v0.3.0)","text":"<ul> <li>Field Templates: Reusable field definitions with flexible naming</li> <li>Field Families: Pre-defined collections for common patterns (Entity,   Audit, Soft Delete)</li> <li>Domain Model Builder: Fluent API for composing models</li> <li>Validation Patterns: Built-in regex patterns and constraints</li> </ul>"},{"location":"#protocol-system","title":"\ud83d\udd0c Protocol System","text":"<ul> <li>Type-Safe Constants: Use <code>IDENTIFIABLE</code>, <code>TEMPORAL</code> instead of strings</li> <li>Behavioral Mixins: Add methods like <code>update_timestamp()</code> to your models</li> <li>One-Step Creation: <code>create_protocol_model_class()</code> for fields + behaviors</li> </ul>"},{"location":"#adapters","title":"\ud83d\udd04 Adapters","text":"<ul> <li>Unified Interface: Consistent API across different data sources</li> <li>Type Safety: Full Pydantic validation support</li> <li>Async Support: Both synchronous and asynchronous interfaces</li> <li>Extensible: Easy to create custom adapters</li> </ul>"},{"location":"#additional-features","title":"\ud83d\ude80 Additional Features","text":"<ul> <li>Migrations: Database schema migration tools</li> <li>Minimal Dependencies: Core functionality has minimal requirements</li> <li>Production Ready: Battle-tested in real applications</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pydapter\n</code></pre> <p>With optional dependencies:</p> <pre><code># Database adapters\npip install \"pydapter[postgres]\"\npip install \"pydapter[mongo]\"\npip install \"pydapter[neo4j]\"\n\n# File formats\npip install \"pydapter[excel]\"\n\n# New modules\npip install \"pydapter[protocols]\"      # Standardized model interfaces\npip install \"pydapter[migrations-sql]\" # Database schema migrations with\n                                       # SQLAlchemy/Alembic\n\n# Combined packages\npip install \"pydapter[migrations]\"     # All migration components\npip install \"pydapter[migrations-all]\" # Migrations with protocols support\n\n# For all extras\npip install \"pydapter[all]\"\n</code></pre>"},{"location":"#quick-examples","title":"Quick Examples","text":""},{"location":"#using-the-field-system","title":"\ud83c\udfd7\ufe0f Using the Field System","text":"<pre><code>from pydapter.fields import DomainModelBuilder, FieldTemplate\nfrom pydapter.protocols import (\n    create_protocol_model_class,\n    IDENTIFIABLE,\n    TEMPORAL\n)\n\n# Build a model with field families\nUser = (\n    DomainModelBuilder(\"User\")\n    .with_entity_fields(timezone_aware=True)  # id, created_at, updated_at\n    .with_audit_fields()                      # created_by, updated_by, version\n    .add_field(\"username\", FieldTemplate(base_type=str, max_length=50))\n    .add_field(\"email\", FieldTemplate(base_type=str))\n    .build()\n)\n\n# Or create a protocol-compliant model with behaviors\nUser = create_protocol_model_class(\n    \"User\",\n    IDENTIFIABLE,  # Adds id field\n    TEMPORAL,      # Adds created_at, updated_at fields + methods\n    username=FieldTemplate(base_type=str),\n    email=FieldTemplate(base_type=str)\n)\n\n# Use the model\nuser = User(username=\"alice\", email=\"alice@example.com\")\nuser.update_timestamp()  # Method from TemporalMixin\n</code></pre>"},{"location":"#using-adapters","title":"\ud83d\udd04 Using Adapters","text":"<pre><code>from pydapter.adapters.json_ import JsonAdapter\n\n# Create an adapter for your model\nadapter = JsonAdapter[User](path=\"users.json\")\n\n# Read data\nusers = adapter.read_all()\n\n# Write data\nadapter.write_one(user)\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>\ud83d\ude80 Getting Started Guide - Your first steps with   pydapter</li> <li>\ud83c\udfd7\ufe0f Field System Overview - Learn about the powerful   field system</li> <li>\ud83d\udd0c Protocols Overview - Understand protocol-based design</li> </ul>"},{"location":"#core-concepts","title":"Core Concepts","text":"<ul> <li>\ud83d\udccb Field Families - Pre-built field   collections</li> <li>\ud83c\udfaf Best Practices - Field and   protocol patterns</li> <li>\u26a1 Error Handling - Robust error management</li> </ul>"},{"location":"#tutorials-guides","title":"Tutorials &amp; Guides","text":"<ul> <li>\ud83d\udd27 End-to-End Backend - Build a complete   backend</li> <li>\ud83d\udcd6 Using Protocols - Protocol tutorial</li> <li>\ud83d\udd04 Using Migrations - Database migrations</li> </ul>"},{"location":"#adapters_1","title":"Adapters","text":"<ul> <li>\ud83d\udc18 PostgreSQL - PostgreSQL adapter guide</li> <li>\ud83d\udd17 Neo4j - Graph database integration</li> <li>\ud83d\udd0d Qdrant - Vector database support</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! See our Contributing Guide for details.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>pydapter is released under the Apache-2.0 License. See the LICENSE file for details.</p>"},{"location":"async_mongo_tutorial/","title":"End-to-End Tutorial: Using Pydapter's Async MongoDB Adapter","text":"<p>This comprehensive tutorial will guide you through using Pydapter's async MongoDB adapter for seamless data operations between Pydantic models and MongoDB collections.</p>"},{"location":"async_mongo_tutorial/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Prerequisites</li> <li>Installation</li> <li>Setting Up MongoDB</li> <li>Basic Usage</li> <li>CRUD Operations</li> <li>Advanced Querying</li> <li>Error Handling</li> <li>Practical Example</li> <li>Performance Tips</li> <li>Best Practices</li> </ul>"},{"location":"async_mongo_tutorial/#overview","title":"Overview","text":"<p>The AsyncMongoAdapter provides asynchronous methods to:</p> <ul> <li>Query MongoDB collections and convert documents to Pydantic models</li> <li>Insert Pydantic models as documents into MongoDB collections</li> <li>Handle async MongoDB operations using Motor (async MongoDB driver)</li> <li>Support various async MongoDB operations (find, insert, update, delete)</li> </ul> <p>Key Features:</p> <ul> <li>Full async/await support</li> <li>Automatic Pydantic model validation</li> <li>Comprehensive error handling</li> <li>MongoDB query filter support</li> <li>Batch operations for performance</li> </ul>"},{"location":"async_mongo_tutorial/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>MongoDB server (local or remote)</li> <li>Basic knowledge of async/await in Python</li> <li>Familiarity with Pydantic models</li> <li>Understanding of MongoDB concepts</li> </ul>"},{"location":"async_mongo_tutorial/#installation","title":"Installation","text":"<p>Install the required dependencies:</p> <pre><code>pip install motor pymongo pydantic\n</code></pre>"},{"location":"async_mongo_tutorial/#setting-up-mongodb","title":"Setting Up MongoDB","text":""},{"location":"async_mongo_tutorial/#local-mongodb-with-docker","title":"Local MongoDB with Docker","text":"<pre><code># Start MongoDB container\ndocker run -d -p 27017:27017 --name mongodb mongo:latest\n\n# Or use docker-compose\nversion: '3.8'\nservices:\n  mongodb:\n    image: mongo:latest\n    ports:\n      - \"27017:27017\"\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: admin\n      MONGO_INITDB_ROOT_PASSWORD: password\n</code></pre>"},{"location":"async_mongo_tutorial/#mongodb-atlas-cloud","title":"MongoDB Atlas (Cloud)","text":"<p>For production environments, consider using MongoDB Atlas:</p> <ol> <li>Create account at MongoDB Atlas</li> <li>Create cluster and get connection string</li> <li>Update connection string in your code</li> </ol>"},{"location":"async_mongo_tutorial/#basic-usage","title":"Basic Usage","text":""},{"location":"async_mongo_tutorial/#import-and-setup","title":"Import and Setup","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom typing import Optional, List\nfrom pydantic import BaseModel, Field\nfrom pydapter.extras.async_mongo_ import AsyncMongoAdapter\nfrom pydapter.exceptions import ConnectionError, ResourceError, AdapterValidationError\n\n# Configuration\nMONGO_URL = \"mongodb://localhost:27017\"\nDATABASE_NAME = \"tutorial_db\"\n</code></pre>"},{"location":"async_mongo_tutorial/#defining-data-models","title":"Defining Data Models","text":"<pre><code>class User(BaseModel):\n    \"\"\"User model for our application.\"\"\"\n    id: int\n    username: str\n    email: str\n    age: int\n    is_active: bool = True\n    created_at: datetime = Field(default_factory=datetime.now)\n\nclass Product(BaseModel):\n    \"\"\"Product model for an e-commerce system.\"\"\"\n    id: int\n    name: str\n    description: str\n    price: float\n    category: str\n    in_stock: bool = True\n    tags: List[str] = []\n\nclass Order(BaseModel):\n    \"\"\"Order model linking users and products.\"\"\"\n    id: int\n    user_id: int\n    product_ids: List[int]\n    total_amount: float\n    status: str = \"pending\"\n    order_date: datetime = Field(default_factory=datetime.now)\n</code></pre>"},{"location":"async_mongo_tutorial/#crud-operations","title":"CRUD Operations","text":""},{"location":"async_mongo_tutorial/#creating-inserting-data","title":"Creating (Inserting) Data","text":""},{"location":"async_mongo_tutorial/#single-document-insert","title":"Single Document Insert","text":"<pre><code>async def create_single_user():\n    \"\"\"Create and insert a single user.\"\"\"\n    user = User(\n        id=1,\n        username=\"john_doe\",\n        email=\"john@example.com\",\n        age=30\n    )\n\n    try:\n        result = await AsyncMongoAdapter.to_obj(\n            user,\n            url=MONGO_URL,\n            db=DATABASE_NAME,\n            collection=\"users\",\n            many=False\n        )\n        print(f\"User created successfully\")\n        return result\n    except Exception as e:\n        print(f\"Error creating user: {e}\")\n        raise\n</code></pre>"},{"location":"async_mongo_tutorial/#batch-insert","title":"Batch Insert","text":"<pre><code>async def create_multiple_users():\n    \"\"\"Create and insert multiple users.\"\"\"\n    users = [\n        User(id=1, username=\"john_doe\", email=\"john@example.com\", age=30),\n        User(id=2, username=\"jane_smith\", email=\"jane@example.com\", age=25),\n        User(id=3, username=\"bob_wilson\", email=\"bob@example.com\", age=35),\n    ]\n\n    try:\n        result = await AsyncMongoAdapter.to_obj(\n            users,\n            url=MONGO_URL,\n            db=DATABASE_NAME,\n            collection=\"users\",\n            many=True\n        )\n        print(f\"Successfully inserted {result['inserted_count']} users\")\n        return result\n    except Exception as e:\n        print(f\"Error inserting users: {e}\")\n        raise\n</code></pre>"},{"location":"async_mongo_tutorial/#reading-querying-data","title":"Reading (Querying) Data","text":""},{"location":"async_mongo_tutorial/#get-all-documents","title":"Get All Documents","text":"<pre><code>async def get_all_users():\n    \"\"\"Retrieve all users from MongoDB.\"\"\"\n    try:\n        users = await AsyncMongoAdapter.from_obj(\n            User,\n            {\n                \"url\": MONGO_URL,\n                \"db\": DATABASE_NAME,\n                \"collection\": \"users\"\n            },\n            many=True\n        )\n\n        print(f\"Retrieved {len(users)} users\")\n        for user in users:\n            print(f\"  - {user.username} ({user.email})\")\n\n        return users\n    except Exception as e:\n        print(f\"Error retrieving users: {e}\")\n        raise\n</code></pre>"},{"location":"async_mongo_tutorial/#get-single-document","title":"Get Single Document","text":"<pre><code>async def get_user_by_id(user_id: int):\n    \"\"\"Get a specific user by ID.\"\"\"\n    try:\n        user = await AsyncMongoAdapter.from_obj(\n            User,\n            {\n                \"url\": MONGO_URL,\n                \"db\": DATABASE_NAME,\n                \"collection\": \"users\",\n                \"filter\": {\"id\": user_id}\n            },\n            many=False\n        )\n\n        print(f\"Found user: {user.username}\")\n        return user\n    except ResourceError:\n        print(f\"User with ID {user_id} not found\")\n        return None\n    except Exception as e:\n        print(f\"Error retrieving user: {e}\")\n        raise\n</code></pre>"},{"location":"async_mongo_tutorial/#advanced-querying","title":"Advanced Querying","text":""},{"location":"async_mongo_tutorial/#mongodb-query-filters","title":"MongoDB Query Filters","text":"<p>The async MongoDB adapter supports full MongoDB query syntax:</p> <pre><code>async def advanced_queries():\n    \"\"\"Demonstrate advanced MongoDB queries.\"\"\"\n\n    # Range queries\n    adult_users = await AsyncMongoAdapter.from_obj(\n        User,\n        {\n            \"url\": MONGO_URL,\n            \"db\": DATABASE_NAME,\n            \"collection\": \"users\",\n            \"filter\": {\"age\": {\"$gte\": 18}}\n        },\n        many=True\n    )\n\n    # Multiple conditions\n    active_adults = await AsyncMongoAdapter.from_obj(\n        User,\n        {\n            \"url\": MONGO_URL,\n            \"db\": DATABASE_NAME,\n            \"collection\": \"users\",\n            \"filter\": {\n                \"age\": {\"$gte\": 18},\n                \"is_active\": True\n            }\n        },\n        many=True\n    )\n\n    # Regular expressions\n    gmail_users = await AsyncMongoAdapter.from_obj(\n        User,\n        {\n            \"url\": MONGO_URL,\n            \"db\": DATABASE_NAME,\n            \"collection\": \"users\",\n            \"filter\": {\"email\": {\"$regex\": \"@gmail.com$\"}}\n        },\n        many=True\n    )\n\n    # Array queries\n    tech_products = await AsyncMongoAdapter.from_obj(\n        Product,\n        {\n            \"url\": MONGO_URL,\n            \"db\": DATABASE_NAME,\n            \"collection\": \"products\",\n            \"filter\": {\"tags\": {\"$in\": [\"tech\", \"electronics\"]}}\n        },\n        many=True\n    )\n\n    return adult_users, active_adults, gmail_users, tech_products\n</code></pre>"},{"location":"async_mongo_tutorial/#complex-aggregation-style-queries","title":"Complex Aggregation-style Queries","text":"<pre><code>async def complex_queries():\n    \"\"\"Demonstrate complex query patterns.\"\"\"\n\n    # Price range with category filter\n    affordable_electronics = await AsyncMongoAdapter.from_obj(\n        Product,\n        {\n            \"url\": MONGO_URL,\n            \"db\": DATABASE_NAME,\n            \"collection\": \"products\",\n            \"filter\": {\n                \"category\": \"Electronics\",\n                \"price\": {\"$gte\": 50, \"$lte\": 500},\n                \"in_stock\": True\n            }\n        },\n        many=True\n    )\n\n    # Text search in multiple fields\n    search_products = await AsyncMongoAdapter.from_obj(\n        Product,\n        {\n            \"url\": MONGO_URL,\n            \"db\": DATABASE_NAME,\n            \"collection\": \"products\",\n            \"filter\": {\n                \"$or\": [\n                    {\"name\": {\"$regex\": \"laptop\", \"$options\": \"i\"}},\n                    {\"description\": {\"$regex\": \"laptop\", \"$options\": \"i\"}}\n                ]\n            }\n        },\n        many=True\n    )\n\n    return affordable_electronics, search_products\n</code></pre>"},{"location":"async_mongo_tutorial/#error-handling","title":"Error Handling","text":""},{"location":"async_mongo_tutorial/#exception-types","title":"Exception Types","text":"<p>The async MongoDB adapter provides specific exception types:</p> <pre><code>from pydapter.exceptions import (\n    ConnectionError,      # Connection issues\n    ResourceError,        # Resource not found\n    AdapterValidationError,  # Validation errors\n    QueryError           # Query execution errors\n)\n</code></pre>"},{"location":"async_mongo_tutorial/#comprehensive-error-handling","title":"Comprehensive Error Handling","text":"<pre><code>async def robust_data_operation():\n    \"\"\"Demonstrate comprehensive error handling.\"\"\"\n\n    try:\n        # Attempt to get user\n        user = await AsyncMongoAdapter.from_obj(\n            User,\n            {\n                \"url\": MONGO_URL,\n                \"db\": DATABASE_NAME,\n                \"collection\": \"users\",\n                \"filter\": {\"id\": 1}\n            },\n            many=False\n        )\n        return user\n\n    except ConnectionError as e:\n        print(f\"Database connection failed: {e}\")\n        # Handle connection issues (retry, fallback, etc.)\n        return None\n\n    except ResourceError as e:\n        print(f\"User not found: {e}\")\n        # Handle missing resources\n        return None\n\n    except AdapterValidationError as e:\n        print(f\"Data validation failed: {e}\")\n        # Handle validation errors\n        return None\n\n    except QueryError as e:\n        print(f\"Query execution failed: {e}\")\n        # Handle query issues\n        return None\n\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        # Handle unexpected errors\n        raise\n</code></pre>"},{"location":"async_mongo_tutorial/#retry-logic","title":"Retry Logic","text":"<pre><code>import asyncio\nfrom typing import TypeVar, Callable, Any\n\nT = TypeVar('T')\n\nasync def retry_on_connection_error(\n    func: Callable[[], Awaitable[T]],\n    max_retries: int = 3,\n    delay: float = 1.0\n) -&gt; T:\n    \"\"\"Retry function on connection errors.\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            return await func()\n        except ConnectionError as e:\n            if attempt == max_retries - 1:\n                raise\n            print(f\"Connection failed (attempt {attempt + 1}/{max_retries}): {e}\")\n            await asyncio.sleep(delay * (2 ** attempt))  # Exponential backoff\n\n    raise ConnectionError(\"Max retries exceeded\")\n\n# Usage\nasync def get_users_with_retry():\n    \"\"\"Get users with automatic retry on connection errors.\"\"\"\n\n    async def _get_users():\n        return await AsyncMongoAdapter.from_obj(\n            User,\n            {\n                \"url\": MONGO_URL,\n                \"db\": DATABASE_NAME,\n                \"collection\": \"users\"\n            },\n            many=True\n        )\n\n    return await retry_on_connection_error(_get_users)\n</code></pre>"},{"location":"async_mongo_tutorial/#practical-example","title":"Practical Example","text":""},{"location":"async_mongo_tutorial/#order-management-system","title":"Order Management System","text":"<p>Here's a complete example showing how to build an order management system:</p> <pre><code>class OrderManager:\n    \"\"\"A comprehensive order management system.\"\"\"\n\n    def __init__(self, mongo_url: str, database: str):\n        self.mongo_url = mongo_url\n        self.database = database\n\n    async def create_order(\n        self,\n        user_id: int,\n        product_ids: List[int]\n    ) -&gt; Order:\n        \"\"\"Create a new order with validation.\"\"\"\n\n        # Verify user exists and is active\n        try:\n            user = await AsyncMongoAdapter.from_obj(\n                User,\n                {\n                    \"url\": self.mongo_url,\n                    \"db\": self.database,\n                    \"collection\": \"users\",\n                    \"filter\": {\"id\": user_id, \"is_active\": True}\n                },\n                many=False\n            )\n        except ResourceError:\n            raise ValueError(f\"Active user {user_id} not found\")\n\n        # Verify all products exist and are in stock\n        products = await AsyncMongoAdapter.from_obj(\n            Product,\n            {\n                \"url\": self.mongo_url,\n                \"db\": self.database,\n                \"collection\": \"products\",\n                \"filter\": {\n                    \"id\": {\"$in\": product_ids},\n                    \"in_stock\": True\n                }\n            },\n            many=True\n        )\n\n        if len(products) != len(product_ids):\n            found_ids = {p.id for p in products}\n            missing_ids = set(product_ids) - found_ids\n            raise ValueError(f\"Products not available: {missing_ids}\")\n\n        # Calculate total\n        total_amount = sum(p.price for p in products)\n\n        # Generate order ID\n        order_id = await self._generate_order_id()\n\n        # Create order\n        order = Order(\n            id=order_id,\n            user_id=user_id,\n            product_ids=product_ids,\n            total_amount=total_amount\n        )\n\n        # Save order\n        await AsyncMongoAdapter.to_obj(\n            order,\n            url=self.mongo_url,\n            db=self.database,\n            collection=\"orders\",\n            many=False\n        )\n\n        print(f\"Order {order.id} created for {user.username} - ${total_amount:.2f}\")\n        return order\n\n    async def get_user_orders(self, user_id: int) -&gt; List[Order]:\n        \"\"\"Get all orders for a user.\"\"\"\n\n        orders = await AsyncMongoAdapter.from_obj(\n            Order,\n            {\n                \"url\": self.mongo_url,\n                \"db\": self.database,\n                \"collection\": \"orders\",\n                \"filter\": {\"user_id\": user_id}\n            },\n            many=True\n        )\n\n        return sorted(orders, key=lambda o: o.order_date, reverse=True)\n\n    async def get_order_summary(self, order_id: int) -&gt; dict:\n        \"\"\"Get detailed order information.\"\"\"\n\n        try:\n            # Get order\n            order = await AsyncMongoAdapter.from_obj(\n                Order,\n                {\n                    \"url\": self.mongo_url,\n                    \"db\": self.database,\n                    \"collection\": \"orders\",\n                    \"filter\": {\"id\": order_id}\n                },\n                many=False\n            )\n\n            # Get user\n            user = await AsyncMongoAdapter.from_obj(\n                User,\n                {\n                    \"url\": self.mongo_url,\n                    \"db\": self.database,\n                    \"collection\": \"users\",\n                    \"filter\": {\"id\": order.user_id}\n                },\n                many=False\n            )\n\n            # Get products\n            products = await AsyncMongoAdapter.from_obj(\n                Product,\n                {\n                    \"url\": self.mongo_url,\n                    \"db\": self.database,\n                    \"collection\": \"products\",\n                    \"filter\": {\"id\": {\"$in\": order.product_ids}}\n                },\n                many=True\n            )\n\n            return {\n                \"order\": order,\n                \"user\": user,\n                \"products\": products,\n                \"summary\": {\n                    \"order_id\": order.id,\n                    \"customer\": user.username,\n                    \"items\": len(products),\n                    \"total\": order.total_amount,\n                    \"status\": order.status\n                }\n            }\n\n        except ResourceError:\n            raise ValueError(f\"Order {order_id} not found\")\n\n    async def _generate_order_id(self) -&gt; int:\n        \"\"\"Generate next order ID.\"\"\"\n\n        orders = await AsyncMongoAdapter.from_obj(\n            Order,\n            {\n                \"url\": self.mongo_url,\n                \"db\": self.database,\n                \"collection\": \"orders\"\n            },\n            many=True\n        )\n\n        if not orders:\n            return 1\n\n        return max(order.id for order in orders) + 1\n\n# Usage example\nasync def demo_order_system():\n    \"\"\"Demonstrate the order management system.\"\"\"\n\n    order_manager = OrderManager(MONGO_URL, DATABASE_NAME)\n\n    # Create an order\n    order = await order_manager.create_order(\n        user_id=1,\n        product_ids=[1, 2, 3]\n    )\n\n    # Get user's orders\n    user_orders = await order_manager.get_user_orders(1)\n    print(f\"User has {len(user_orders)} orders\")\n\n    # Get order summary\n    summary = await order_manager.get_order_summary(order.id)\n    print(f\"Order summary: {summary['summary']}\")\n</code></pre>"},{"location":"async_mongo_tutorial/#performance-tips","title":"Performance Tips","text":""},{"location":"async_mongo_tutorial/#1-batch-operations","title":"1. Batch Operations","text":"<p>Always prefer batch operations over individual operations:</p> <pre><code># \u2705 Good: Batch insert\nusers = [User(...) for _ in range(100)]\nawait AsyncMongoAdapter.to_obj(\n    users,\n    url=MONGO_URL,\n    db=DATABASE_NAME,\n    collection=\"users\",\n    many=True\n)\n\n# \u274c Bad: Individual inserts\nfor user in users:\n    await AsyncMongoAdapter.to_obj(\n        user,\n        url=MONGO_URL,\n        db=DATABASE_NAME,\n        collection=\"users\",\n        many=False\n    )\n</code></pre>"},{"location":"async_mongo_tutorial/#2-efficient-filtering","title":"2. Efficient Filtering","text":"<p>Use specific filters to reduce data transfer:</p> <pre><code># \u2705 Good: Specific filter\nusers = await AsyncMongoAdapter.from_obj(\n    User,\n    {\n        \"url\": MONGO_URL,\n        \"db\": DATABASE_NAME,\n        \"collection\": \"users\",\n        \"filter\": {\"id\": {\"$in\": [1, 2, 3]}}\n    },\n    many=True\n)\n\n# \u274c Bad: Broad query then filter in Python\nall_users = await AsyncMongoAdapter.from_obj(User, config, many=True)\nfiltered_users = [u for u in all_users if u.id in [1, 2, 3]]\n</code></pre>"},{"location":"async_mongo_tutorial/#3-connection-management","title":"3. Connection Management","text":"<pre><code># The adapter handles connections automatically\n# Each operation creates a new connection\n# Connections are properly closed after use\n\n# For high-frequency operations, consider connection pooling at the application level\n</code></pre>"},{"location":"async_mongo_tutorial/#4-memory-management","title":"4. Memory Management","text":"<pre><code># For large datasets, consider pagination\nasync def get_users_paginated(page: int, size: int = 100):\n    \"\"\"Get users with pagination.\"\"\"\n\n    skip = page * size\n    # Note: MongoDB skip/limit would be implemented differently\n    # This is a simplified example\n\n    users = await AsyncMongoAdapter.from_obj(\n        User,\n        {\n            \"url\": MONGO_URL,\n            \"db\": DATABASE_NAME,\n            \"collection\": \"users\",\n            \"filter\": {}  # Add pagination logic here\n        },\n        many=True\n    )\n\n    return users[skip:skip + size]\n</code></pre>"},{"location":"async_mongo_tutorial/#best-practices","title":"Best Practices","text":""},{"location":"async_mongo_tutorial/#1-model-design","title":"1. Model Design","text":"<pre><code># \u2705 Good: Clear, specific models\nclass User(BaseModel):\n    id: int\n    username: str\n    email: str\n    created_at: datetime = Field(default_factory=datetime.now)\n\n    class Config:\n        # Add any Pydantic config here\n        validate_assignment = True\n\n# \u274c Bad: Generic, unclear models\nclass Data(BaseModel):\n    stuff: dict\n    things: list\n</code></pre>"},{"location":"async_mongo_tutorial/#2-error-handling-strategy","title":"2. Error Handling Strategy","text":"<pre><code># \u2705 Good: Specific exception handling\ntry:\n    user = await get_user(user_id)\nexcept ResourceError:\n    # Handle missing resource\n    return None\nexcept ConnectionError:\n    # Handle connection issues\n    raise ServiceUnavailableError()\nexcept AdapterValidationError as e:\n    # Handle validation errors\n    raise BadRequestError(f\"Invalid data: {e}\")\n\n# \u274c Bad: Generic exception handling\ntry:\n    user = await get_user(user_id)\nexcept Exception:\n    return None\n</code></pre>"},{"location":"async_mongo_tutorial/#3-configuration-management","title":"3. Configuration Management","text":"<pre><code># \u2705 Good: Centralized configuration\nclass MongoConfig:\n    def __init__(self):\n        self.url = os.getenv(\"MONGO_URL\", \"mongodb://localhost:27017\")\n        self.database = os.getenv(\"MONGO_DB\", \"myapp\")\n        self.timeout = int(os.getenv(\"MONGO_TIMEOUT\", \"5000\"))\n\nconfig = MongoConfig()\n\n# \u274c Bad: Hardcoded values\nMONGO_URL = \"mongodb://localhost:27017\"  # Hardcoded\n</code></pre>"},{"location":"async_mongo_tutorial/#4-testing","title":"4. Testing","text":"<pre><code>import pytest\nfrom unittest.mock import patch\n\n@pytest.mark.asyncio\nasync def test_create_user():\n    \"\"\"Test user creation.\"\"\"\n\n    with patch('pydapter.extras.async_mongo_.AsyncMongoAdapter.to_obj') as mock_to_obj:\n        mock_to_obj.return_value = {\"inserted_count\": 1}\n\n        user = User(id=1, username=\"test\", email=\"test@example.com\", age=25)\n        result = await AsyncMongoAdapter.to_obj(\n            user,\n            url=\"mongodb://test\",\n            db=\"test_db\",\n            collection=\"users\",\n            many=False\n        )\n\n        assert result[\"inserted_count\"] == 1\n        mock_to_obj.assert_called_once()\n</code></pre>"},{"location":"async_mongo_tutorial/#5-production-considerations","title":"5. Production Considerations","text":"<pre><code># \u2705 Production setup\nasync def create_production_order_manager():\n    \"\"\"Create order manager with production settings.\"\"\"\n\n    return OrderManager(\n        mongo_url=os.getenv(\"MONGO_URL\"),\n        database=os.getenv(\"MONGO_DATABASE\")\n    )\n\n# Add proper logging\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nasync def safe_create_order(*args, **kwargs):\n    \"\"\"Create order with proper logging.\"\"\"\n\n    try:\n        order = await order_manager.create_order(*args, **kwargs)\n        logger.info(f\"Order {order.id} created successfully\")\n        return order\n    except Exception as e:\n        logger.error(f\"Failed to create order: {e}\", exc_info=True)\n        raise\n</code></pre>"},{"location":"async_mongo_tutorial/#summary","title":"Summary","text":"<p>The AsyncMongoAdapter provides a powerful, async-first approach to working with MongoDB in Python applications. Key benefits include:</p> <ul> <li>Seamless Integration: Direct conversion between Pydantic models and   MongoDB documents</li> <li>Async Performance: Full async/await support for non-blocking operations</li> <li>Robust Error Handling: Specific exceptions for different error scenarios</li> <li>MongoDB Query Support: Full MongoDB query syntax support</li> <li>Type Safety: Pydantic model validation ensures data integrity</li> </ul>"},{"location":"async_mongo_tutorial/#quick-reference","title":"Quick Reference","text":"<pre><code># Basic operations\nawait AsyncMongoAdapter.to_obj(\n    model, url=url, db=db, collection=coll, many=False\n)\nawait AsyncMongoAdapter.from_obj(Model, config, many=True)\n\n# Error handling\ntry:\n    result = await AsyncMongoAdapter.from_obj(...)\nexcept (ConnectionError, ResourceError, AdapterValidationError) as e:\n    handle_error(e)\n\n# Configuration\nconfig = {\n    \"url\": \"mongodb://localhost:27017\",\n    \"db\": \"database_name\",\n    \"collection\": \"collection_name\",\n    \"filter\": {\"field\": \"value\"}  # Optional\n}\n</code></pre> <p>This tutorial provides a comprehensive foundation for using Pydapter's async MongoDB adapter in real-world applications. Remember to always handle errors appropriately, use batch operations for performance, and follow MongoDB best practices for schema design and querying.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#023-2025-05-29","title":"0.2.3 - 2025-05-29","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Field Families and Common Patterns Library (Issue #114): Introduced a   comprehensive field system with:</li> <li><code>FieldTemplate</code>: Reusable field definitions with flexible naming</li> <li><code>FieldFamilies</code>: Core database pattern collections (ENTITY, SOFT_DELETE, AUDIT)</li> <li><code>DomainModelBuilder</code>: Fluent API for building models with method chaining</li> <li><code>ProtocolFieldFamilies</code>: Field sets that ensure protocol compliance</li> <li><code>ValidationPatterns</code>: Common regex patterns and constraint builders</li> <li><code>create_protocol_model()</code>: Function to create protocol-compliant models (structure only)</li> <li>Protocol Enhancements:</li> <li>Added protocol constants (<code>IDENTIFIABLE</code>, <code>TEMPORAL</code>, etc.) for type-safe protocol selection</li> <li>Added <code>create_protocol_model_class()</code>: Factory function that creates models with both     structural fields AND behavioral methods in one step</li> <li>Added <code>combine_with_mixins()</code>: Helper to easily add protocol behaviors to existing models</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>BREAKING: Removed \"event\" from the protocol system in <code>ProtocolFieldFamilies</code>.   The <code>Event</code> class remains available but is no longer part of the protocol   selection system since it's a concrete class, not a protocol interface.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed import organization issues (E402 errors)</li> <li>Updated tests to reflect simplified field families</li> <li>Fixed email validation test expectations</li> <li>Updated documentation to align with new architecture</li> <li>Fixed SQLAlchemy primary key mapping issue in test_model_adapter_enhancements.py</li> </ul>"},{"location":"changelog/#020-2025-05-24","title":"0.2.0 - 2025-05-24","text":""},{"location":"changelog/#highlights","title":"Highlights","text":"<p>This release introduces two major foundational modules: Fields and Protocols. These modules provide a robust and extensible framework for defining data structures and behaviors within <code>pydapter</code>.</p> <ul> <li>Fields Module (<code>pydapter.fields</code>): A powerful system for defining typed,   validated, and serializable fields. It includes pre-defined field types for   common use cases like IDs, datetimes, embeddings, and execution tracking,   along with a flexible <code>Field</code> class for custom definitions and a   <code>create_model</code> utility for dynamic Pydantic model creation.</li> <li>Protocols Module (<code>pydapter.protocols</code>): A set of composable interfaces   (e.g., <code>Identifiable</code>, <code>Temporal</code>, <code>Embeddable</code>, <code>Invokable</code>,   <code>Cryptographical</code>) that define standard behaviors for Pydantic models. The   <code>Event</code> protocol combines these to offer comprehensive event tracking   capabilities, enhanced by the <code>@as_event</code> decorator for easily instrumenting   functions.</li> </ul> <p>These additions significantly enhance <code>pydapter</code>'s ability to model complex data interactions and workflows in a standardized and maintainable way.</p>"},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>New <code>pydapter.fields</code> module: Introduced a robust system for defining   typed, validated, and serializable fields (e.g., IDs, datetimes, embeddings,   execution tracking) and a <code>create_model</code> utility for dynamic Pydantic model   creation. (Related to Issue #100, PR #99)</li> <li>New <code>pydapter.protocols</code> module: Added composable protocol interfaces   (<code>Identifiable</code>, <code>Temporal</code>, <code>Embeddable</code>, <code>Invokable</code>, <code>Cryptographical</code>) and   an <code>Event</code> protocol with an <code>@as_event</code> decorator for comprehensive event   modeling and function instrumentation. (Related to Issue #100, PR #99)</li> <li>Hybrid Documentation System: Implemented a new documentation system   combining auto-generated API skeletons with rich manual content and automated   validation (markdown linting, link checking). (Issue #103, PR #104)</li> <li>Updated CI to install documentation validation tools. (Issue #105)</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Resolved Python 3.10 compatibility issues related to <code>datetime.timezone.utc</code>.   (Part of PR #99 fixes)</li> <li>Addressed various <code>mkdocs</code> build warnings and broken links in the   documentation. (Part of PR #104 fixes)</li> </ul>"},{"location":"changelog/#015-2025-05-14","title":"0.1.5 - 2025-05-14","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>New adapter implementations:</li> <li><code>AsyncNeo4jAdapter</code> - Asynchronous adapter for Neo4j graph database with     comprehensive error handling</li> <li><code>WeaviateAdapter</code> - Synchronous adapter for Weaviate vector database with     vector search capabilities</li> <li><code>AsyncWeaviateAdapter</code> - Asynchronous adapter for Weaviate vector database     using aiohttp for REST API calls</li> </ul>"},{"location":"changelog/#011-2025-05-04","title":"0.1.1 - 2025-05-04","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Integration tests for database adapters using TestContainers</li> <li>PostgreSQL integration tests</li> <li>MongoDB integration tests</li> <li>Neo4j integration tests</li> <li>Qdrant vector database integration tests</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Neo4j adapter now supports authentication</li> <li>Qdrant adapter improved connection error handling</li> <li>SQL adapter enhanced error handling for connection issues</li> <li>Improved error handling in core adapter classes</li> </ul>"},{"location":"changelog/#010-2025-05-03","title":"0.1.0 - 2025-05-03","text":"<ul> <li>Initial public release.</li> <li><code>core.Adapter</code>, <code>AdapterRegistry</code>, <code>Adaptable</code></li> <li>Built-in JSON adapter</li> </ul>"},{"location":"ci/","title":"Continuous Integration","text":"<p>This document describes the continuous integration (CI) setup for the pydapter project and how to use it locally.</p>"},{"location":"ci/#overview","title":"Overview","text":"<p>The pydapter project uses a comprehensive CI system that runs:</p> <ul> <li>Linting checks (using ruff)</li> <li>Code formatting checks (using ruff format)</li> <li>Type checking (using mypy)</li> <li>Unit tests (using pytest)</li> <li>Integration tests (using pytest)</li> <li>Coverage reporting</li> </ul> <p>The CI system is implemented as a Python script that can be run both locally and in GitHub Actions workflows, ensuring consistency between local development and CI environments.</p>"},{"location":"ci/#prerequisites","title":"Prerequisites","text":"<p>Before running the CI script, you need to ensure all dependencies are installed. The easiest way to do this is to use the <code>uv sync</code> command with the <code>--extra all</code> option:</p> <pre><code># Install all dependencies including those needed for testing\nuv sync --extra all\n</code></pre> <p>This will install all the dependencies defined in the <code>pyproject.toml</code> file, including those needed for testing and integration with external services.</p>"},{"location":"ci/#running-ci-locally","title":"Running CI Locally","text":"<p>The CI script is located at <code>scripts/ci.py</code> and can be run with various options to customize the checks performed.</p>"},{"location":"ci/#basic-usage","title":"Basic Usage","text":"<p>To run all CI checks:</p> <pre><code>python scripts/ci.py\n</code></pre> <p>This will run all linting, type checking, unit tests, integration tests, and coverage reporting.</p>"},{"location":"ci/#running-specific-components","title":"Running Specific Components","text":"<p>You can run only specific components using the <code>--only</code> option:</p> <pre><code># Run only linting checks\npython scripts/ci.py --only lint\n\n# Run only type checking\npython scripts/ci.py --only type\n\n# Run only unit tests\npython scripts/ci.py --only unit\n\n# Run only integration tests\npython scripts/ci.py --only integration\n\n# Run only coverage report\npython scripts/ci.py --only coverage\n</code></pre>"},{"location":"ci/#skipping-specific-checks","title":"Skipping Specific Checks","text":"<p>You can skip specific checks using the following options:</p> <pre><code># Skip linting checks\npython scripts/ci.py --skip-lint\n\n# Skip type checking\npython scripts/ci.py --skip-type-check\n\n# Skip unit tests\npython scripts/ci.py --skip-unit\n\n# Skip integration tests\npython scripts/ci.py --skip-integration\n\n# Skip coverage reporting\npython scripts/ci.py --skip-coverage\n</code></pre> <p>You can combine these options to run only the checks you're interested in:</p> <pre><code># Run only unit tests\npython scripts/ci.py --skip-lint --skip-type-check --skip-integration --skip-coverage\n</code></pre>"},{"location":"ci/#handling-external-dependencies","title":"Handling External Dependencies","text":"<p>Some tests require external dependencies like databases (MongoDB, Neo4j, Qdrant, etc.). You can skip these tests using the <code>--skip-external-deps</code> option:</p> <pre><code>python scripts/ci.py --skip-external-deps\n</code></pre> <p>This is useful when you want to run the tests locally without setting up all the external dependencies. The script will automatically exclude tests that require external services.</p>"},{"location":"ci/#parallel-test-execution","title":"Parallel Test Execution","text":"<p>To speed up test execution, you can run tests in parallel:</p> <pre><code>python scripts/ci.py --parallel 4\n</code></pre> <p>This will use 4 processes to run the tests.</p>"},{"location":"ci/#python-version","title":"Python Version","text":"<p>You can specify a Python version to use:</p> <pre><code>python scripts/ci.py --python-version 3.10\n</code></pre> <p>Or specify a path to a Python executable:</p> <pre><code>python scripts/ci.py --python-path /path/to/python\n</code></pre>"},{"location":"ci/#dry-run","title":"Dry Run","text":"<p>To see what commands would be executed without actually running them:</p> <pre><code>python scripts/ci.py --dry-run\n</code></pre>"},{"location":"ci/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<p>The CI script is also used in GitHub Actions workflows, defined in <code>.github/workflows/ci.yml</code>. The workflow runs on push to main, pull requests to main, and can be triggered manually.</p> <p>The workflow includes the following jobs:</p> <ul> <li>test: Runs the full CI script on multiple Python versions (3.10, 3.11,   3.12)</li> <li>lint: Runs only linting and formatting checks</li> <li>type-check: Runs only type checking</li> <li>integration: Runs only integration tests</li> <li>coverage: Runs tests with coverage reporting and uploads to Codecov</li> </ul>"},{"location":"ci/#adding-new-checks","title":"Adding New Checks","text":"<p>To add new checks to the CI script:</p> <ol> <li>Add a new method to the <code>CIRunner</code> class in <code>scripts/ci.py</code></li> <li>Call the method from the <code>run_all</code> method</li> <li>Add the result to the <code>results</code> list</li> <li>Add any necessary command-line options to the <code>parse_args</code> function</li> <li>Update the <code>REQUIRED_DEPS</code> dictionary with any new dependencies</li> <li>If the check requires external dependencies, update the <code>EXTERNAL_DEPS_FILES</code>    list</li> </ol>"},{"location":"ci/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with the CI script:</p> <ul> <li>Make sure you have all the required dependencies installed:   <code>uv sync --extra all</code></li> <li>For integration tests, you may need to have Docker running if you're using   testcontainers</li> <li>Try running specific checks individually to isolate the issue</li> <li>Use the <code>--skip-external-deps</code> option to skip tests that require external   dependencies</li> <li>Check the output for missing dependencies and install them as needed</li> </ul>"},{"location":"ci/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Missing Dependencies: The script will attempt to install missing    dependencies automatically, but if it fails, you'll need to install them    manually.</p> </li> <li> <p>External Dependencies: Tests that require external services like    databases will fail if those services are not available. Use    <code>--skip-external-deps</code> to skip these tests.</p> </li> <li> <p>Type Checking Errors: The type checking step may fail due to missing type    stubs for dependencies. You can install these with    <code>uv pip install types-&lt;package&gt;</code>.</p> </li> <li> <p>Docker Not Running: If you're running integration tests that use    testcontainers, make sure Docker is running on your system.</p> </li> </ol>"},{"location":"contributing/","title":"Contributing to Pydapter","text":"<p>Thank you for your interest in contributing to Pydapter! This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"contributing/#development-environment-setup","title":"Development Environment Setup","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:</li> </ol> <pre><code>git clone https://github.com/your-username/pydapter.git\ncd pydapter\n</code></pre> <ol> <li>Set up a development environment:</li> </ol> <pre><code># Using uv (recommended)\nuv pip install -e \".[dev,all]\"\n\n# Or using pip\npip install -e \".[dev,all]\"\n</code></pre> <ol> <li>Install pre-commit hooks:</li> </ol> <pre><code>pre-commit install\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li>Create a new branch for your feature or bugfix:</li> </ol> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <ol> <li> <p>Make your changes, following the project's coding standards</p> </li> <li> <p>Run the CI script locally to ensure all tests pass:</p> </li> </ol> <pre><code>python scripts/ci.py\n</code></pre> <ol> <li>Commit your changes using conventional commit messages:</li> </ol> <pre><code>git commit -m \"feat: add new feature\"\n</code></pre> <ol> <li>Push your branch to your fork:</li> </ol> <pre><code>git push origin feature/your-feature-name\n</code></pre> <ol> <li>Open a pull request on GitHub</li> </ol>"},{"location":"contributing/#continuous-integration","title":"Continuous Integration","text":"<p>The project uses a comprehensive CI system that runs:</p> <ul> <li>Linting checks (using ruff)</li> <li>Code formatting checks (using ruff format)</li> <li>Type checking (using mypy)</li> <li>Unit tests (using pytest)</li> <li>Integration tests (using pytest)</li> <li>Coverage reporting</li> <li>Documentation validation (using markdownlint and markdown-link-check)</li> </ul> <p>You can run the CI script locally with various options:</p> <pre><code># Run all checks\npython scripts/ci.py\n\n# Skip integration tests (which require Docker)\npython scripts/ci.py --skip-integration\n\n# Run only documentation validation\npython scripts/ci.py --only docs\n\n# Run only linting and formatting checks\npython scripts/ci.py --skip-unit --skip-integration --skip-coverage --skip-docs\n\n# Run tests in parallel\npython scripts/ci.py --parallel 4\n</code></pre> <p>For more information, see the CI documentation.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>This project follows these coding standards:</p> <ul> <li>Code formatting with ruff format</li> <li>Linting with ruff</li> <li>Type annotations for all functions and classes</li> <li>Comprehensive docstrings in   Google style</li> <li>Test coverage for all new features</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<p>All new features and bug fixes should include tests. The project uses pytest for testing:</p> <pre><code># Run all tests\nuv run pytest\n\n# Run specific tests\nuv run pytest tests/test_specific_file.py\n\n# Run with coverage\nuv run pytest --cov=pydapter\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Documentation is written in Markdown and built with MkDocs using a hybrid approach that combines auto-generated API references with enhanced manual content.</p>"},{"location":"contributing/#documentation-standards","title":"Documentation Standards","text":"<p>All documentation must follow these standards:</p> <ol> <li>Markdown Quality: All markdown files must pass <code>markdownlint</code> validation</li> <li>Link Integrity: All internal and external links must be valid</li> <li>API Documentation: Use the hybrid approach with enhanced manual content</li> <li>Code Examples: Include working code examples with proper syntax    highlighting</li> <li>Cross-References: Link related concepts and maintain navigation    consistency</li> </ol>"},{"location":"contributing/#validation-tools","title":"Validation Tools","text":"<p>The project uses automated validation tools:</p> <ul> <li>markdownlint: Ensures consistent markdown formatting</li> <li>markdown-link-check: Validates all links in documentation</li> <li>Pre-commit hooks: Automatic validation before commits</li> </ul>"},{"location":"contributing/#writing-documentation","title":"Writing Documentation","text":"<p>When contributing documentation:</p> <ol> <li>API Reference: Follow the pattern established in <code>docs/api/protocols.md</code>    and <code>docs/api/core.md</code></li> <li>Manual Enhancement: Add examples, best practices, and cross-references    beyond basic API extraction</li> <li>User Personas: Consider different user needs (new users, API users,    contributors)</li> <li>Code Examples: Provide complete, runnable examples</li> <li>Navigation: Ensure proper cross-linking between related sections</li> </ol>"},{"location":"contributing/#documentation-workflow","title":"Documentation Workflow","text":"<pre><code># Preview documentation locally\nuv run mkdocs serve\n\n# Validate documentation\npython scripts/ci.py --only docs\n\n# Check specific files\nmarkdownlint docs/**/*.md\nmarkdown-link-check docs/api/core.md --config .markdownlinkcheck.json\n\n# Fix common issues automatically (when possible)\nmarkdownlint --fix docs/**/*.md\n</code></pre>"},{"location":"contributing/#documentation-structure","title":"Documentation Structure","text":"<ul> <li><code>docs/api/</code>: API reference documentation (hybrid approach)</li> <li><code>docs/tutorials/</code>: Step-by-step guides</li> <li><code>docs/</code>: General guides and concepts</li> <li>Examples should be complete and testable</li> <li>Cross-references should use relative links</li> </ul> <p>Then open http://127.0.0.1:8000/ in your browser to preview changes.</p>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure your code passes all CI checks</li> <li>Update documentation if necessary</li> <li>Add tests for new features</li> <li>Make sure your PR description clearly describes the changes and their purpose</li> <li>Wait for review and address any feedback</li> </ol>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to Pydapter, you agree that your contributions will be licensed under the project's MIT License.</p>"},{"location":"crud_operations/","title":"CRUD Operations with Pydapter","text":"<p>This guide covers the enhanced CRUD (Create, Read, Update, Delete) operations available in pydapter's async SQL adapters.</p>"},{"location":"crud_operations/#overview","title":"Overview","text":"<p>Pydapter's async SQL adapters now support full CRUD operations through a clean, config-driven interface. All operations are controlled through configuration dictionaries, maintaining backward compatibility while adding powerful new capabilities.</p>"},{"location":"crud_operations/#installation","title":"Installation","text":"<pre><code># For PostgreSQL support\npip install pydapter[postgres]\n\n# For generic SQL support\npip install pydapter[sql]\n</code></pre>"},{"location":"crud_operations/#configuration","title":"Configuration","text":""},{"location":"crud_operations/#typeddict-support","title":"TypedDict Support","text":"<p>Pydapter provides TypedDict classes for better IDE support and type safety:</p> <pre><code>from pydapter.extras.async_sql_ import AsyncSQLAdapter, SQLReadConfig, SQLWriteConfig\n\n# Type-safe configuration\nconfig: SQLReadConfig = {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"table\": \"users\",\n    \"selectors\": {\"active\": True},\n    \"limit\": 10\n}\n</code></pre>"},{"location":"crud_operations/#connection-options","title":"Connection Options","text":"<p>You can provide database connection in three ways:</p> <ol> <li>DSN (recommended): Database connection string</li> <li>engine_url (legacy): Backward compatibility</li> <li>engine: Pre-existing SQLAlchemy AsyncEngine for connection reuse</li> </ol> <pre><code># Option 1: DSN (recommended)\nconfig = {\"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\", \"table\": \"users\"}\n\n# Option 2: Legacy engine_url\nconfig = {\"engine_url\": \"postgresql+asyncpg://user:pass@localhost/db\", \"table\": \"users\"}\n\n# Option 3: Reuse existing engine (most efficient)\nfrom sqlalchemy.ext.asyncio import create_async_engine\nengine = create_async_engine(\"postgresql+asyncpg://user:pass@localhost/db\")\nconfig = {\"engine\": engine, \"table\": \"users\"}\n</code></pre>"},{"location":"crud_operations/#crud-operations","title":"CRUD Operations","text":""},{"location":"crud_operations/#select-read","title":"SELECT (Read)","text":"<pre><code>from pydantic import BaseModel\nfrom pydapter.extras.async_sql_ import AsyncSQLAdapter\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n    age: int\n\n# Select all users (default operation for from_obj)\nusers = await AsyncSQLAdapter.from_obj(User, {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"table\": \"users\"\n}, many=True)\n\n# Select with filters\nconfig = {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"table\": \"users\",\n    \"selectors\": {\"age\": 30, \"active\": True}\n}\nfiltered_users = await AsyncSQLAdapter.from_obj(User, config, many=True)\n\n# Select with limit and offset\nconfig = {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"table\": \"users\",\n    \"limit\": 10,\n    \"offset\": 20\n}\npaginated_users = await AsyncSQLAdapter.from_obj(User, config, many=True)\n</code></pre>"},{"location":"crud_operations/#insert-create","title":"INSERT (Create)","text":"<pre><code># Single insert (default operation for to_obj)\nnew_user = User(name=\"John Doe\", email=\"john@example.com\", age=30)\nresult = await AsyncSQLAdapter.to_obj(\n    new_user,\n    dsn=\"postgresql+asyncpg://user:pass@localhost/db\",\n    table=\"users\"\n)\n# Returns: {\"inserted_count\": 1}\n\n# Bulk insert\nusers = [\n    User(name=\"Alice\", email=\"alice@example.com\", age=25),\n    User(name=\"Bob\", email=\"bob@example.com\", age=30),\n]\nresult = await AsyncSQLAdapter.to_obj(\n    users,\n    dsn=\"postgresql+asyncpg://user:pass@localhost/db\",\n    table=\"users\"\n)\n# Returns: {\"inserted_count\": 2}\n</code></pre>"},{"location":"crud_operations/#update","title":"UPDATE","text":"<pre><code># Update specific records\nupdated_user = User(name=\"John Updated\", email=\"john@example.com\", age=31)\nresult = await AsyncSQLAdapter.to_obj(\n    updated_user,\n    dsn=\"postgresql+asyncpg://user:pass@localhost/db\",\n    table=\"users\",\n    operation=\"update\",\n    where={\"email\": \"john@example.com\"}\n)\n# Returns: {\"updated_count\": 1}\n\n# Update multiple records\nresult = await AsyncSQLAdapter.to_obj(\n    {\"status\": \"inactive\"},\n    dsn=\"postgresql+asyncpg://user:pass@localhost/db\",\n    table=\"users\",\n    operation=\"update\",\n    where={\"last_login\": {\"&lt;\": \"2024-01-01\"}}\n)\n</code></pre>"},{"location":"crud_operations/#delete","title":"DELETE","text":"<pre><code># Delete specific records\nconfig = {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"table\": \"users\",\n    \"operation\": \"delete\",\n    \"selectors\": {\"id\": 123}\n}\nresult = await AsyncSQLAdapter.from_obj(User, config)\n# Returns: {\"deleted_count\": 1}\n\n# Delete with multiple conditions\nconfig = {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"table\": \"users\",\n    \"operation\": \"delete\",\n    \"selectors\": {\"status\": \"inactive\", \"created_at\": {\"&lt;\": \"2023-01-01\"}}\n}\nresult = await AsyncSQLAdapter.from_obj(User, config)\n</code></pre>"},{"location":"crud_operations/#upsert-insert-or-update","title":"UPSERT (Insert or Update)","text":"<pre><code># Upsert - insert if not exists, update if exists\nuser = User(name=\"Jane Doe\", email=\"jane@example.com\", age=28)\nresult = await AsyncSQLAdapter.to_obj(\n    user,\n    dsn=\"postgresql+asyncpg://user:pass@localhost/db\",\n    table=\"users\",\n    operation=\"upsert\",\n    conflict_columns=[\"email\"]  # Columns that define uniqueness\n)\n# Returns: {\"inserted_count\": 1, \"updated_count\": 0, \"total_count\": 1}\n\n# Subsequent upsert with same email will update\nuser.age = 29\nresult = await AsyncSQLAdapter.to_obj(\n    user,\n    dsn=\"postgresql+asyncpg://user:pass@localhost/db\",\n    table=\"users\",\n    operation=\"upsert\",\n    conflict_columns=[\"email\"]\n)\n# Returns: {\"inserted_count\": 0, \"updated_count\": 1, \"total_count\": 1}\n</code></pre>"},{"location":"crud_operations/#raw-sql-execution","title":"Raw SQL Execution","text":"<p>Important: Raw SQL operations do NOT require a <code>table</code> parameter and do NOT perform table inspection. This makes them ideal for: - Complex queries across multiple tables - Aggregations and analytical queries - DDL operations (CREATE, ALTER, DROP) - Working with databases where async table inspection isn't supported</p> <pre><code># Execute custom SQL with parameters (no table parameter needed!)\nconfig = {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"operation\": \"raw_sql\",\n    \"sql\": \"\"\"\n        SELECT\n            department,\n            COUNT(*) as user_count,\n            AVG(age) as avg_age\n        FROM users\n        WHERE active = :active\n        GROUP BY department\n        HAVING COUNT(*) &gt; :min_count\n    \"\"\",\n    \"params\": {\"active\": True, \"min_count\": 5}\n}\nresults = await AsyncSQLAdapter.from_obj(dict, config, many=True)\n\n# ORDER BY queries (common use case)\nconfig = {\n    \"dsn\": \"sqlite:///app.db\",  # Works with SQLite too!\n    \"operation\": \"raw_sql\",\n    \"sql\": \"SELECT * FROM events ORDER BY created_at DESC LIMIT :limit\",\n    \"params\": {\"limit\": 10}\n}\nrecent_events = await AsyncSQLAdapter.from_obj(dict, config, many=True)\n\n# DDL operations (CREATE, ALTER, DROP)\nconfig = {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"operation\": \"raw_sql\",\n    \"sql\": \"CREATE INDEX idx_users_email ON users(email)\",\n    \"fetch_results\": False  # Don't try to fetch results for DDL\n}\nresult = await AsyncSQLAdapter.from_obj(dict, config)\n# Returns: {\"affected_rows\": 0}\n</code></pre>"},{"location":"crud_operations/#postgresql-specific-features","title":"PostgreSQL Specific Features","text":"<p>The <code>AsyncPostgresAdapter</code> provides PostgreSQL-specific optimizations:</p> <pre><code>from pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\n# Automatic DSN format conversion\nconfig = {\n    \"dsn\": \"postgresql://user:pass@localhost/db\",  # Standard PostgreSQL format\n    \"table\": \"users\"\n}\n# Automatically converted to: postgresql+asyncpg://user:pass@localhost/db\n\nusers = await AsyncPostgresAdapter.from_obj(User, config, many=True)\n\n# PostgreSQL-specific error handling\ntry:\n    await AsyncPostgresAdapter.to_obj(user, dsn=dsn, table=\"users\")\nexcept ConnectionError as e:\n    if \"authentication\" in str(e):\n        print(\"Authentication failed\")\n    elif \"database does not exist\" in str(e):\n        print(\"Database not found\")\n</code></pre>"},{"location":"crud_operations/#real-world-example","title":"Real-World Example","text":"<pre><code>import asyncio\nfrom datetime import datetime\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom pydapter.extras.async_sql_ import AsyncSQLAdapter\n\nclass Customer(BaseModel):\n    id: Optional[int] = None\n    username: str\n    email: str\n    full_name: str\n    created_at: Optional[datetime] = None\n    updated_at: Optional[datetime] = None\n\nasync def manage_customers():\n    # Create a reusable engine for efficiency\n    engine = create_async_engine(\"postgresql+asyncpg://postgres:postgres@localhost/db\")\n\n    try:\n        # 1. Insert new customer\n        customer = Customer(\n            username=\"johndoe\",\n            email=\"john@example.com\",\n            full_name=\"John Doe\"\n        )\n        result = await AsyncSQLAdapter.to_obj(\n            customer,\n            engine=engine,\n            table=\"customers\"\n        )\n        print(f\"Created customer: {result}\")\n\n        # 2. Find customer by username\n        config = {\n            \"engine\": engine,\n            \"table\": \"customers\",\n            \"selectors\": {\"username\": \"johndoe\"}\n        }\n        found_customer = await AsyncSQLAdapter.from_obj(Customer, config, many=False)\n        print(f\"Found customer: {found_customer}\")\n\n        # 3. Update customer email\n        found_customer.email = \"newemail@example.com\"\n        result = await AsyncSQLAdapter.to_obj(\n            found_customer,\n            engine=engine,\n            table=\"customers\",\n            operation=\"update\",\n            where={\"username\": \"johndoe\"}\n        )\n        print(f\"Updated customer: {result}\")\n\n        # 4. Get customer statistics\n        config = {\n            \"engine\": engine,\n            \"operation\": \"raw_sql\",\n            \"sql\": \"\"\"\n                SELECT\n                    COUNT(*) as total_customers,\n                    COUNT(DISTINCT DATE(created_at)) as signup_days\n                FROM customers\n                WHERE created_at &gt; :since\n            \"\"\",\n            \"params\": {\"since\": \"2024-01-01\"}\n        }\n        stats = await AsyncSQLAdapter.from_obj(dict, config, many=False)\n        print(f\"Customer stats: {stats}\")\n\n    finally:\n        await engine.dispose()\n\n# Run the example\nasyncio.run(manage_customers())\n</code></pre>"},{"location":"crud_operations/#best-practices","title":"Best Practices","text":"<ol> <li>Use TypedDict for Configuration: Provides better IDE support and type safety</li> </ol> <pre><code>config: SQLReadConfig = {\"dsn\": dsn, \"table\": \"users\"}\n</code></pre> <ol> <li>Reuse Engines for Bulk Operations: Create one engine and pass it to    multiple operations for efficiency</li> </ol> <pre><code># Create engine once for the entire batch operation\nengine = create_async_engine(\"postgresql+asyncpg://user:pass@localhost/db\")\n\ntry:\n    # Process multiple batches using the same engine\n    for batch in data_batches:\n        # Pass the engine directly - avoids creating new connections\n        result = await AsyncSQLAdapter.to_obj(\n            batch,\n            engine=engine,  # Reuses connection pool\n            table=\"users\"\n        )\n        print(f\"Inserted {result['inserted_count']} records\")\n\n    # Or for multiple different operations\n    await AsyncSQLAdapter.to_obj(users, engine=engine, table=\"users\")\n    await AsyncSQLAdapter.to_obj(orders, engine=engine, table=\"orders\")\n    await AsyncSQLAdapter.to_obj(items, engine=engine, table=\"items\")\n\nfinally:\n    # Always dispose of the engine when done\n    await engine.dispose()\n</code></pre> <p>Note: SQLAlchemy engines manage their own connection pools internally.    You don't use <code>async with engine:</code> context managers with engines - that pattern    is for connections/sessions.</p> <ol> <li>Handle None Values: The adapter automatically excludes None values from    INSERT/UPDATE operations</li> </ol> <pre><code>user = User(id=None, name=\"John\")  # id=None will be excluded\n</code></pre> <ol> <li>Use Parameterized Queries: Always use parameters for raw SQL to prevent    SQL injection</li> </ol> <pre><code>config = {\n    \"operation\": \"raw_sql\",\n    \"sql\": \"SELECT * FROM users WHERE email = :email\",\n    \"params\": {\"email\": user_input}  # Safe parameterization\n}\n</code></pre> <ol> <li>Error Handling: Use specific exception types for better error handling</li> </ol> <pre><code>from pydapter.exceptions import ConnectionError, QueryError, ValidationError\n\ntry:\n    result = await AsyncSQLAdapter.from_obj(User, config)\nexcept ConnectionError:\n    # Handle connection issues\nexcept QueryError:\n    # Handle query errors\nexcept ValidationError:\n    # Handle validation errors\n</code></pre>"},{"location":"crud_operations/#testing","title":"Testing","text":"<p>Pydapter includes comprehensive tests for all CRUD operations:</p> <pre><code># Run tests\npytest libs/pydapter/tests/test_async_sql_crud.py -v\n\n# Run with coverage\npytest libs/pydapter/tests/test_async_sql_crud.py --cov=pydapter.extras\n</code></pre>"},{"location":"crud_operations/#migration-from-basic-operations","title":"Migration from Basic Operations","text":"<p>If you're currently using basic pydapter operations, migration is simple:</p> <pre><code># Old: Basic insert only\nawait adapter.to_obj(user, engine_url=url, table=\"users\")\n\n# New: Full CRUD support (backward compatible)\nawait adapter.to_obj(user, dsn=url, table=\"users\")  # INSERT (default)\nawait adapter.to_obj(user, dsn=url, table=\"users\", operation=\"update\", where={\"id\": 1})\nawait adapter.to_obj(user, dsn=url, table=\"users\", operation=\"upsert\", conflict_columns=[\"email\"])\n</code></pre>"},{"location":"crud_operations/#troubleshooting","title":"Troubleshooting","text":""},{"location":"crud_operations/#common-issues","title":"Common Issues","text":"<ol> <li>\"Multiple engine parameters provided\": Only use one of <code>dsn</code>, <code>engine_url</code>, or <code>engine</code></li> <li>\"Missing required parameter\": For select/delete operations, ensure you    provide a connection parameter and table name. For raw_sql operations, only    the connection parameter and sql are required</li> <li>SQL parameter syntax: Use <code>:param_name</code> for SQLAlchemy, not <code>%(param_name)s</code></li> <li>None values in primary keys: The adapter automatically filters out None values</li> </ol>"},{"location":"crud_operations/#performance-tips","title":"Performance Tips","text":"<ol> <li>Connection Pooling: Reuse engines instead of creating new ones</li> <li>Batch Operations: Use bulk insert/update for multiple records</li> <li>Indexing: Ensure proper database indexes for WHERE clause columns</li> <li>Raw SQL: Use raw SQL for complex queries that don't fit the CRUD pattern</li> </ol>"},{"location":"crud_operations/#api-reference","title":"API Reference","text":""},{"location":"crud_operations/#sqlreadconfig","title":"SQLReadConfig","text":"<p>Configuration for read operations (<code>from_obj</code>):</p> <ul> <li><code>dsn</code> / <code>engine_url</code> / <code>engine</code>: Database connection (one required)</li> <li><code>table</code>: Table name (required for select/delete, NOT required for raw_sql)</li> <li><code>operation</code>: \"select\" (default), \"delete\", or \"raw_sql\"</li> <li><code>selectors</code>: WHERE conditions for select/delete</li> <li><code>limit</code>: Maximum records to return (select only)</li> <li><code>offset</code>: Number of records to skip (select only)</li> <li><code>order_by</code>: ORDER BY clause (select only)</li> <li><code>sql</code>: Raw SQL statement (required for raw_sql operation)</li> <li><code>params</code>: SQL parameters (for raw_sql operation)</li> <li><code>fetch_results</code>: Whether to fetch results (raw_sql only, default: True)</li> </ul>"},{"location":"crud_operations/#sqlwriteconfig","title":"SQLWriteConfig","text":"<p>Configuration for write operations (<code>to_obj</code> kwargs):</p> <ul> <li><code>dsn</code> / <code>engine_url</code> / <code>engine</code>: Database connection (one required)</li> <li><code>table</code>: Table name (required)</li> <li><code>operation</code>: \"insert\" (default), \"update\", or \"upsert\"</li> <li><code>where</code>: WHERE conditions for UPDATE</li> <li><code>conflict_columns</code>: Columns defining uniqueness for UPSERT</li> </ul>"},{"location":"crud_operations/#support","title":"Support","text":"<p>For issues or questions, please refer to the pydapter GitHub repository or the comprehensive test suite in <code>tests/test_async_sql_crud.py</code>.</p>"},{"location":"error_handling/","title":"Error Handling in pydapter","text":"<p>pydapter provides a comprehensive error handling system to help you diagnose and resolve issues when working with adapters. This document explains the exception hierarchy and how to handle errors effectively in your applications.</p>"},{"location":"error_handling/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>All pydapter exceptions inherit from the base <code>AdapterError</code> class, which provides context-rich error messages and a consistent interface for error handling.</p> <pre><code>AdapterError\n\u251c\u2500\u2500 ValidationError\n\u251c\u2500\u2500 ParseError\n\u251c\u2500\u2500 ConnectionError\n\u251c\u2500\u2500 QueryError\n\u251c\u2500\u2500 ResourceError\n\u251c\u2500\u2500 ConfigurationError\n\u2514\u2500\u2500 AdapterNotFoundError\n</code></pre>"},{"location":"error_handling/#adaptererror","title":"AdapterError","text":"<p>The base exception class for all pydapter errors. It provides a mechanism to attach context information to errors.</p> <pre><code>try:\n    # Some adapter operation\nexcept AdapterError as e:\n    print(f\"Error message: {e.message}\")\n    print(f\"Error context: {e.context}\")\n</code></pre>"},{"location":"error_handling/#validationerror","title":"ValidationError","text":"<p>Raised when data validation fails, such as when required fields are missing or have incorrect types.</p> <pre><code>try:\n    model = MyModel.adapt_from(invalid_data, obj_key=\"json\")\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\n    print(f\"Invalid data: {e.data}\")\n</code></pre>"},{"location":"error_handling/#parseerror","title":"ParseError","text":"<p>Raised when data parsing fails, such as when trying to parse invalid JSON, CSV, or TOML.</p> <pre><code>try:\n    model = MyModel.adapt_from('{\"invalid\": json', obj_key=\"json\")\nexcept ParseError as e:\n    print(f\"Parse error: {e}\")\n    print(f\"Source: {e.source}\")\n</code></pre>"},{"location":"error_handling/#connectionerror","title":"ConnectionError","text":"<p>Raised when a connection to a data source fails, such as when a database is unavailable.</p> <pre><code>try:\n    model = MyModel.adapt_from({\"engine_url\": \"invalid://url\", \"table\": \"test\"}, obj_key=\"sql\")\nexcept ConnectionError as e:\n    print(f\"Connection failed: {e}\")\n    print(f\"Adapter: {e.adapter}\")\n    print(f\"URL: {e.url}\")\n</code></pre>"},{"location":"error_handling/#queryerror","title":"QueryError","text":"<p>Raised when a query to a data source fails, such as when an SQL query contains errors.</p> <pre><code>try:\n    model = MyModel.adapt_from({\"engine_url\": \"sqlite://\", \"table\": \"test\", \"query\": \"INVALID SQL\"}, obj_key=\"sql\")\nexcept QueryError as e:\n    print(f\"Query failed: {e}\")\n    print(f\"Query: {e.query}\")\n    print(f\"Adapter: {e.adapter}\")\n</code></pre>"},{"location":"error_handling/#resourceerror","title":"ResourceError","text":"<p>Raised when a resource (file, database table, etc.) cannot be accessed.</p> <pre><code>try:\n    model = MyModel.adapt_from(Path(\"nonexistent.json\"), obj_key=\"json\")\nexcept ResourceError as e:\n    print(f\"Resource error: {e}\")\n    print(f\"Resource: {e.resource}\")\n</code></pre>"},{"location":"error_handling/#configurationerror","title":"ConfigurationError","text":"<p>Raised when adapter configuration is invalid, such as when required parameters are missing.</p> <pre><code>try:\n    model = MyModel.adapt_from({\"missing\": \"required_params\"}, obj_key=\"sql\")\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\n    print(f\"Config: {e.config}\")\n</code></pre>"},{"location":"error_handling/#adapternotfounderror","title":"AdapterNotFoundError","text":"<p>Raised when an adapter is not found for a given <code>obj_key</code>.</p> <pre><code>try:\n    model = MyModel.adapt_from({}, obj_key=\"nonexistent\")\nexcept AdapterNotFoundError as e:\n    print(f\"Adapter not found: {e}\")\n    print(f\"Object key: {e.obj_key}\")\n</code></pre>"},{"location":"error_handling/#error-handling-best-practices","title":"Error Handling Best Practices","text":""},{"location":"error_handling/#catch-specific-exceptions","title":"Catch Specific Exceptions","text":"<p>Catch the most specific exception type that applies to your situation:</p> <pre><code>try:\n    model = MyModel.adapt_from(data, obj_key=\"json\")\nexcept ParseError:\n    # Handle parsing errors\nexcept ValidationError:\n    # Handle validation errors\nexcept AdapterError:\n    # Handle any other adapter errors\n</code></pre>"},{"location":"error_handling/#provide-context-in-error-messages","title":"Provide Context in Error Messages","text":"<p>When raising custom exceptions, provide as much context as possible:</p> <pre><code>raise ConnectionError(\n    \"Failed to connect to database\",\n    adapter=\"postgres\",\n    url=\"postgresql://localhost:5432/mydb\",\n    timeout=30\n)\n</code></pre>"},{"location":"error_handling/#handle-asynchronous-errors","title":"Handle Asynchronous Errors","text":"<p>For asynchronous adapters, use try/except blocks within async functions:</p> <pre><code>async def fetch_data():\n    try:\n        return await MyModel.adapt_from_async(data, obj_key=\"async_mongo\")\n    except ConnectionError as e:\n        logger.error(f\"Connection failed: {e}\")\n        # Handle connection error\n    except AdapterError as e:\n        logger.error(f\"Adapter error: {e}\")\n        # Handle other adapter errors\n</code></pre>"},{"location":"error_handling/#resource-cleanup","title":"Resource Cleanup","text":"<p>Ensure resources are properly cleaned up, even in error scenarios:</p> <pre><code>try:\n    # Some operation that acquires resources\n    result = perform_operation()\n    return result\nexcept AdapterError:\n    # Handle the error\n    raise\nfinally:\n    # Clean up resources\n    cleanup_resources()\n</code></pre>"},{"location":"error_handling/#common-error-scenarios-and-solutions","title":"Common Error Scenarios and Solutions","text":""},{"location":"error_handling/#json-parsing-errors","title":"JSON Parsing Errors","text":"<pre><code>try:\n    model = MyModel.adapt_from(json_data, obj_key=\"json\")\nexcept ParseError as e:\n    if \"Expecting property name\" in str(e):\n        # Handle malformed JSON\n    elif \"Expecting value\" in str(e):\n        # Handle empty JSON\n</code></pre>"},{"location":"error_handling/#database-connection-errors","title":"Database Connection Errors","text":"<pre><code>try:\n    model = MyModel.adapt_from(db_config, obj_key=\"postgres\")\nexcept ConnectionError as e:\n    if \"authentication failed\" in str(e):\n        # Handle authentication issues\n    elif \"connection refused\" in str(e):\n        # Handle server unavailable\n    elif \"database does not exist\" in str(e):\n        # Handle missing database\n</code></pre>"},{"location":"error_handling/#empty-result-sets","title":"Empty Result Sets","text":"<pre><code>try:\n    model = MyModel.adapt_from(query_params, obj_key=\"mongo\", many=False)\nexcept ResourceError as e:\n    if \"No documents found\" in str(e):\n        # Handle empty result\n        return default_value\n</code></pre>"},{"location":"error_handling/#conclusion","title":"Conclusion","text":"<p>Proper error handling is essential for building robust applications with pydapter. By understanding the exception hierarchy and following best practices, you can create more resilient code that gracefully handles failure scenarios and provides clear feedback to users.</p>"},{"location":"getting_started/","title":"Getting Started with Pydapter","text":"<p>Pydapter is a powerful adapter library that lets you easily convert between Pydantic models and various data formats. With the new field system (v0.3.0), creating robust models is easier than ever!</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>First, let's install pydapter and its dependencies:</p> <pre><code># Create a virtual environment (optional but recommended)\npython -m venv pydapter-demo\nsource pydapter-demo/bin/activate  # On Windows: pydapter-demo\\Scripts\\activate\n\n# Install pydapter and dependencies\nuv pip install pydapter\nuv pip install pandas  # For DataFrameAdapter and SeriesAdapter\nuv pip install xlsxwriter  # For ExcelAdapter\nuv pip install openpyxl  # Also needed for Excel support\n\n# Install optional modules\nuv pip install \"pydapter[protocols]\"      # For standardized model interfaces\nuv pip install \"pydapter[migrations-sql]\" # For database schema migrations\n\n# or install all adapters at once\nuv pip install \"pydapter[all]\"\n</code></pre>"},{"location":"getting_started/#creating-models-with-the-field-system","title":"Creating Models with the Field System","text":""},{"location":"getting_started/#option-1-using-field-families-new","title":"Option 1: Using Field Families (New!)","text":"<pre><code>from pydapter.fields import DomainModelBuilder, FieldTemplate\nfrom pydapter.protocols import (\n    create_protocol_model_class,\n    IDENTIFIABLE,\n    TEMPORAL\n)\n\n# Build a model with field families\nUser = (\n    DomainModelBuilder(\"User\")\n    .with_entity_fields()  # Adds id, created_at, updated_at\n    .add_field(\"name\", FieldTemplate(base_type=str))\n    .add_field(\"email\", FieldTemplate(base_type=str))\n    .add_field(\"active\", FieldTemplate(base_type=bool, default=True))\n    .add_field(\"tags\", FieldTemplate(base_type=list[str], default_factory=list))\n    .build()\n)\n\n# Or create a protocol-compliant model with behaviors\nUser = create_protocol_model_class(\n    \"User\",\n    IDENTIFIABLE,  # Adds id field\n    TEMPORAL,      # Adds created_at, updated_at + update_timestamp() method\n    name=FieldTemplate(base_type=str),\n    email=FieldTemplate(base_type=str),\n    active=FieldTemplate(base_type=bool, default=True),\n    tags=FieldTemplate(base_type=list[str], default_factory=list)\n)\n</code></pre>"},{"location":"getting_started/#option-2-traditional-pydantic-models","title":"Option 2: Traditional Pydantic Models","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import List\nfrom pydapter.adapters.json_ import JsonAdapter\n\n# Define a traditional Pydantic model\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n    active: bool = True\n    tags: List[str] = []\n</code></pre>"},{"location":"getting_started/#using-adapters","title":"Using Adapters","text":"<p>Once you have your models, you can use pydapter's adapters to convert data:</p> <pre><code>from pydapter.adapters.json_ import JsonAdapter\n\n# Create some test data\nusers = [\n    User(id=1, name=\"Alice\", email=\"alice@example.com\", tags=[\"admin\", \"staff\"]),\n    User(id=2, name=\"Bob\", email=\"bob@example.com\", active=False),\n    User(id=3, name=\"Charlie\", email=\"charlie@example.com\", tags=[\"staff\"]),\n]\n\n# If using protocol models with behaviors\nif hasattr(users[0], 'update_timestamp'):\n    users[0].update_timestamp()  # Updates the updated_at field\n\n# Convert models to JSON\njson_data = JsonAdapter.to_obj(users, many=True)\nprint(\"JSON Output:\")\nprint(json_data)\n\n# Convert JSON back to models\nloaded_users = JsonAdapter.from_obj(User, json_data, many=True)\nprint(\"\\nLoaded users:\")\nfor user in loaded_users:\n    print(f\"{user.name} ({user.email}): Active={user.active}, Tags={user.tags}\")\n</code></pre>"},{"location":"getting_started/#using-the-adaptable-mixin-for-better-ergonomics","title":"Using the Adaptable Mixin for Better Ergonomics","text":"<p>Pydapter provides an <code>Adaptable</code> mixin that makes the API more ergonomic:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List\nfrom pydapter.core import Adaptable\nfrom pydapter.adapters.json_ import JsonAdapter\n\n# Define a model with the Adaptable mixin\nclass Product(BaseModel, Adaptable):\n    id: int\n    name: str\n    price: float\n    in_stock: bool = True\n\n# Register the JSON adapter\nProduct.register_adapter(JsonAdapter)\n\n# Create a product\nproduct = Product(id=101, name=\"Laptop\", price=999.99)\n\n# Convert to JSON using the mixin method\njson_data = product.adapt_to(obj_key=\"json\")\nprint(\"JSON Output:\")\nprint(json_data)\n\n# Convert back to a model\nloaded_product = Product.adapt_from(json_data, obj_key=\"json\")\nprint(f\"\\nLoaded product: {loaded_product.name} (${loaded_product.price})\")\n</code></pre>"},{"location":"getting_started/#working-with-csv","title":"Working with CSV","text":"<p>Here's how to use the CSV adapter:</p> <pre><code>from pydantic import BaseModel\nfrom pydapter.adapters.csv_ import CsvAdapter\n\n# Define a Pydantic model\nclass Employee(Adaptable, BaseModel):\n    id: int\n    name: str\n    department: str\n    salary: float\n    hire_date: str\n\n# Create some sample data\nemployees = [\n    Employee(\n        id=1, name=\"Alice\", department=\"Engineering\",\n        salary=85000, hire_date=\"2020-01-15\"\n    ),\n    Employee(\n        id=2, name=\"Bob\", department=\"Marketing\",\n        salary=75000, hire_date=\"2021-03-20\"\n    ),\n    Employee(\n        id=3, name=\"Charlie\", department=\"Finance\",\n        salary=95000, hire_date=\"2019-11-01\"\n    ),\n]\n\ncsv_data = CsvAdapter.to_obj(employees, many=True)\nprint(\"CSV Output:\")\nprint(csv_data)\n\n# Convert CSV back to models\nloaded_employees = CsvAdapter.from_obj(Employee, csv_data, many=True)\nprint(\"\\nLoaded employees:\")\nfor employee in loaded_employees:\n    print(f\"{employee.name} - {employee.department} (${employee.salary})\")\n\n# You can also save to a file and read from a file\nfrom pathlib import Path\n\n# Save to file\nPath(\"employees.csv\").write_text(csv_data)\n\n# Read from file\nfile_employees = CsvAdapter.from_obj(Employee, Path(\"employees.csv\"), many=True)\n</code></pre>"},{"location":"getting_started/#working-with-toml","title":"Working with TOML","text":"<p>Here's how to use the TOML adapter:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Dict, Optional\nfrom pydapter.adapters.toml_ import TomlAdapter\n\n# Define a Pydantic model\n\nclass AppConfig(BaseModel):\n    app_name: str\n    version: str\n    debug: bool = False\n    database: Dict[str, str] = {}\n    allowed_hosts: List[str] = []\n\n# Create a config\n\nconfig = AppConfig(\n    app_name=\"MyApp\",\n    version=\"1.0.0\",\n    debug=True,\n    database={\"host\": \"localhost\", \"port\": \"5432\", \"name\": \"myapp\"},\n    allowed_hosts=[\"localhost\", \"example.com\"]\n)\n\n# Convert to TOML\ntoml_data = TomlAdapter.to_obj(config)\nprint(\"TOML Output:\")\nprint(toml_data)\n\n# Convert TOML back to model\nloaded_config = TomlAdapter.from_obj(AppConfig, toml_data)\nprint(\"\\nLoaded config:\")\nprint(f\"App: {loaded_config.app_name} v{loaded_config.version}\")\nprint(f\"Debug mode: {loaded_config.debug}\")\nprint(f\"Database: {loaded_config.database}\")\nprint(f\"Allowed hosts: {loaded_config.allowed_hosts}\")\n\n# Save to file\nPath(\"config.toml\").write_text(toml_data)\n\n# Read from file\nfile_config = TomlAdapter.from_obj(AppConfig, Path(\"config.toml\"))\n</code></pre>"},{"location":"getting_started/#working-with-pandas-dataframe","title":"Working with Pandas DataFrame","text":"<p>Here's how to use the DataFrame adapter:</p> <pre><code>import pandas as pd\nfrom pydantic import BaseModel\nfrom pydapter.extras.pandas_ import DataFrameAdapter\n\n# Define a Pydantic model\nclass SalesRecord(BaseModel):\n    id: int\n    product: str\n    quantity: int\n    price: float\n    date: str\n\n# Create a sample DataFrame\n\ndf = pd.DataFrame([\n    {\n        \"id\": 1, \"product\": \"Laptop\", \"quantity\": 2,\n        \"price\": 999.99, \"date\": \"2023-01-15\"\n    },\n    {\n        \"id\": 2, \"product\": \"Monitor\", \"quantity\": 3,\n        \"price\": 249.99, \"date\": \"2023-01-20\"\n    },\n    {\n        \"id\": 3, \"product\": \"Mouse\", \"quantity\": 5,\n        \"price\": 29.99, \"date\": \"2023-01-25\"\n    }\n])\n\n# Convert DataFrame to models\nsales_records = DataFrameAdapter.from_obj(SalesRecord, df, many=True)\nprint(\"DataFrame to Models:\")\nfor record in sales_records:\n    print(f\"{record.id}: {record.quantity} x {record.product} at ${record.price}\")\n\n# Convert models back to DataFrame\nnew_df = DataFrameAdapter.to_obj(sales_records, many=True)\nprint(\"\\nModels to DataFrame:\")\nprint(new_df)\n</code></pre>"},{"location":"getting_started/#working-with-excel-files","title":"Working with Excel Files","text":"<p>Here's how to use the Excel adapter:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\nfrom pydapter.extras.excel_ import ExcelAdapter\nfrom pathlib import Path\n\n# Define a Pydantic model\nclass Student(BaseModel):\n    id: int\n    name: str\n    grade: str\n    score: float\n\n# Create some sample data\n\nstudents = [\n    Student(id=1, name=\"Alice\", grade=\"A\", score=92.5),\n    Student(id=2, name=\"Bob\", grade=\"B\", score=85.0),\n    Student(id=3, name=\"Charlie\", grade=\"A-\", score=90.0),\n]\n\n# Convert to Excel and save to file\n\nexcel_data = ExcelAdapter.to_obj(students, many=True, sheet_name=\"Students\")\nwith open(\"students.xlsx\", \"wb\") as f:\n    f.write(excel_data)\n\nprint(\"Excel file saved as 'students.xlsx'\")\n\n# Read from Excel file\n\nloaded_students = ExcelAdapter.from_obj(\n    Student, Path(\"students.xlsx\"), many=True\n)\nprint(\"\\nLoaded students:\")\nfor student in loaded_students:\n    print(f\"{student.name}: {student.grade} ({student.score})\")\n</code></pre>"},{"location":"getting_started/#error-handling","title":"Error Handling","text":"<p>Let's demonstrate proper error handling:</p> <pre><code>from pydantic import BaseModel, Field\nfrom pydapter.adapters.json_ import JsonAdapter\nfrom pydapter.exceptions import ParseError, ValidationError as AdapterValidationError\n\n# Define a model with validation constraints\nclass Product(BaseModel):\n    id: int = Field(gt=0)  # Must be greater than 0\n    name: str = Field(min_length=3)  # Must be at least 3 characters\n    price: float = Field(gt=0.0)  # Must be greater than 0\n\n# Handle parsing errors\n\ntry:\n    # Try to parse invalid JSON\n    invalid_json = \"{ 'id': 1, 'name': 'Laptop', price: 999.99 }\"  # Note the\n                                                                   # missing\n                                                                   # quotes\n                                                                   # around\n                                                                   # 'price'\n    product = JsonAdapter.from_obj(Product, invalid_json)\nexcept ParseError as e:\n    print(f\"Parsing error: {e}\")\n\n# Handle validation errors\n\ntry:\n    # Try to create a model with invalid data\n    valid_json = '{\"id\": 0, \"name\": \"A\", \"price\": -10.0}'  # All fields\n                                                           # violate\n                                                           # constraints\n    product = JsonAdapter.from_obj(Product, valid_json)\nexcept AdapterValidationError as e:\n    print(f\"Validation error: {e}\")\n    if hasattr(e, 'errors') and callable(e.errors):\n        for error in e.errors():\n            print(f\"  - {error['loc']}: {error['msg']}\")\n</code></pre>"},{"location":"getting_started/#using-protocols","title":"Using Protocols","text":"<p>Pydapter provides a set of standardized interfaces through the protocols module. These protocols allow you to add common capabilities to your models:</p> <pre><code>from pydapter.protocols import Identifiable, Temporal\n\n# Define a model with standardized interfaces\n\nclass User(Identifiable, Temporal):\n    name: str\n    email: str\n\n# Create a user\n\nuser = User(name=\"Alice\", email=\"alice@example.com\")\n\n# Access standardized properties\nprint(f\"User ID: {user.id}\")  # Automatically generated UUID\nprint(f\"Created at: {user.created_at}\")  # Automatically set timestamp\n\n# Update the timestamp\n\nuser.name = \"Alicia\"\nuser.update_timestamp()\nprint(f\"Updated at: {user.updated_at}\")\n</code></pre> <p>For more details, see the Protocols documentation and the Using Protocols tutorial.</p>"},{"location":"getting_started/#using-migrations","title":"Using Migrations","text":"<p>Pydapter provides tools for managing database schema changes through the migrations module:</p> <pre><code>from pydapter.migrations import AlembicAdapter\nimport mymodels  # Module containing your SQLAlchemy models\n\n# Initialize migrations\n\nAlembicAdapter.init_migrations(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\",\n    models_module=mymodels\n)\n\n# Create a migration\n\nrevision = AlembicAdapter.create_migration(\n    message=\"Create users table\",\n    autogenerate=True,\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\n\n# Apply migrations\n\nAlembicAdapter.upgrade(\n    revision=\"head\",\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\n</code></pre> <p>For more details, see the Migrations documentation and the Using Migrations tutorial.</p>"},{"location":"migration_guide/","title":"Migration Guide: Transitioning from dev/ Directory","text":"<p>This guide is for users who have been using the experimental protocols and migrations modules from the <code>dev/</code> directory. These modules have now been integrated into the main package as optional dependencies.</p>"},{"location":"migration_guide/#overview-of-changes","title":"Overview of Changes","text":"<p>The protocols and migrations modules have been moved from the <code>dev/</code> directory to the main package structure:</p> <ul> <li><code>dev/protocols/</code> \u2192 <code>pydapter.protocols</code></li> <li><code>dev/migrations/</code> \u2192 <code>pydapter.migrations</code></li> </ul> <p>The functionality remains largely the same, but there are some important changes to be aware of:</p> <ol> <li>The modules are now optional dependencies</li> <li>Import paths have changed</li> <li>Backward compatibility is maintained temporarily</li> <li>Type checking works regardless of installed dependencies</li> </ol>"},{"location":"migration_guide/#installation","title":"Installation","text":"<p>To use the new modules, you need to install them as optional dependencies:</p> <pre><code># For protocols module\npip install \"pydapter[protocols]\"\n\n# For migrations core functionality\npip install \"pydapter[migrations-core]\"\n\n# For SQL migrations with Alembic\npip install \"pydapter[migrations-sql]\"\n\n# For all migrations components\npip install \"pydapter[migrations]\"\n\n# For both protocols and migrations\npip install \"pydapter[migrations-all]\"\n\n# For all pydapter features\npip install \"pydapter[all]\"\n</code></pre>"},{"location":"migration_guide/#core-system","title":"Core System","text":"<p>The core adapter system remains unchanged in its API and functionality. The main changes are related to import paths and enhanced error handling. If you're using the core system directly, no code changes are required beyond updating import statements.</p>"},{"location":"migration_guide/#fields-system","title":"Fields System","text":"<p>The fields system has been integrated from the experimental <code>dev/</code> directory with enhanced validation and protocol integration. Field definitions and usage patterns remain the same, with improved type safety and documentation.</p>"},{"location":"migration_guide/#protocols-and-fields","title":"Protocols and Fields","text":"<p>The protocols and fields systems now work together seamlessly, with protocols leveraging pre-defined field definitions for consistency and reusability.</p>"},{"location":"migration_guide/#updating-import-statements","title":"Updating Import Statements","text":""},{"location":"migration_guide/#protocols-module","title":"Protocols Module","text":"<p>Old imports:</p> <pre><code>from dev.protocols import Identifiable, Temporal, Embedable, Invokable, Event\nfrom dev.protocols.types import Embedding, ExecutionStatus, Execution, Log\n</code></pre> <p>New imports:</p> <pre><code>from pydapter.protocols import Identifiable, Temporal, Embedable, Invokable, Event\nfrom pydapter.protocols import Embedding, ExecutionStatus, Execution, Log\n</code></pre>"},{"location":"migration_guide/#migrations-module","title":"Migrations Module","text":"<p>Old imports:</p> <pre><code>from dev.migrations import BaseMigrationAdapter, SyncMigrationAdapter, AsyncMigrationAdapter\nfrom dev.migrations.protocols import MigrationProtocol, AsyncMigrationProtocol\nfrom dev.migrations.exceptions import MigrationError\nfrom dev.migrations.sql.alembic_adapter import AlembicAdapter, AsyncAlembicAdapter\n</code></pre> <p>New imports:</p> <pre><code>from pydapter.migrations import BaseMigrationAdapter, SyncMigrationAdapter, AsyncMigrationAdapter\nfrom pydapter.migrations import MigrationProtocol, AsyncMigrationProtocol\nfrom pydapter.migrations import MigrationError\nfrom pydapter.migrations import AlembicAdapter, AsyncAlembicAdapter\n</code></pre>"},{"location":"migration_guide/#backward-compatibility","title":"Backward Compatibility","text":"<p>For a transitional period, the modules in the <code>dev/</code> directory will continue to work by re-exporting from the new locations. However, you will see deprecation warnings:</p> <pre><code>DeprecationWarning: Importing from dev.protocols is deprecated and will be\nremoved in a future version. Please use pydapter.protocols instead.\n</code></pre> <p>It's recommended to update your import statements as soon as possible to avoid issues when the backward compatibility is removed in a future version.</p>"},{"location":"migration_guide/#handling-missing-dependencies","title":"Handling Missing Dependencies","text":"<p>If you try to import from the new modules without installing the required dependencies, you'll get a clear error message:</p> <pre><code>ImportError: The 'protocols' feature requires the 'typing_extensions' package.\nInstall it with: pip install pydapter[protocols]\n</code></pre> <p>This helps guide you to install the correct dependencies.</p>"},{"location":"migration_guide/#type-checking","title":"Type Checking","text":"<p>The new modules are designed to work well with static type checkers like mypy, even if the optional dependencies are not installed. This is achieved through conditional imports with <code>TYPE_CHECKING</code>.</p> <pre><code>from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from pydapter.protocols import Identifiable, Temporal\n</code></pre>"},{"location":"migration_guide/#examples","title":"Examples","text":""},{"location":"migration_guide/#using-protocols","title":"Using Protocols","text":"<pre><code># Old code\nfrom dev.protocols import Identifiable, Temporal\n\nclass User(Identifiable, Temporal):\n    name: str\n    email: str\n\n# New code\nfrom pydapter.protocols import Identifiable, Temporal\n\nclass User(Identifiable, Temporal):\n    name: str\n    email: str\n</code></pre>"},{"location":"migration_guide/#using-migrations","title":"Using Migrations","text":"<pre><code># Old code\nfrom dev.migrations.sql.alembic_adapter import AlembicAdapter\n\nAlembicAdapter.init_migrations(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\",\n    models_module=models\n)\n\n# New code\nfrom pydapter.migrations import AlembicAdapter\n\nAlembicAdapter.init_migrations(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\",\n    models_module=models\n)\n</code></pre>"},{"location":"migration_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"migration_guide/#missing-dependencies","title":"Missing Dependencies","text":"<p>If you encounter import errors, make sure you've installed the required dependencies:</p> <pre><code>pip install \"pydapter[protocols]\"\npip install \"pydapter[migrations-sql]\"\n</code></pre>"},{"location":"migration_guide/#type-checking-issues","title":"Type Checking Issues","text":"<p>If you encounter type checking issues, make sure your type checker is configured to handle conditional imports with <code>TYPE_CHECKING</code>. For mypy, this should work out of the box.</p>"},{"location":"migration_guide/#circular-imports","title":"Circular Imports","text":"<p>If you encounter circular import errors, try using relative imports within your modules:</p> <pre><code># Instead of\nfrom pydapter.protocols import Identifiable\n\n# Use\nfrom ..protocols import Identifiable\n</code></pre>"},{"location":"migration_guide/#timeline-for-deprecation","title":"Timeline for Deprecation","text":"<p>The backward compatibility with the <code>dev/</code> directory will be maintained for at least one minor version release. After that, the modules in the <code>dev/</code> directory will be removed, and you'll need to use the new import paths.</p>"},{"location":"migration_guide/#additional-resources","title":"Additional Resources","text":"<ul> <li>Protocols Documentation</li> <li>Migrations Documentation</li> <li>Using Protocols Tutorial</li> <li>Using Migrations Tutorial</li> </ul>"},{"location":"migrations/","title":"Migrations Module","text":"<p>The Migrations module provides a framework for managing database schema changes in a controlled, versioned manner. It follows pydapter's adapter pattern philosophy, offering both synchronous and asynchronous interfaces for different database backends.</p>"},{"location":"migrations/#installation","title":"Installation","text":"<p>The Migrations module is available as an optional dependency with different components:</p> <pre><code># Core migrations functionality (minimal dependencies)\npip install pydapter[migrations-core]\n\n# SQL migrations with Alembic support\npip install pydapter[migrations-sql]\n\n# All migrations components\npip install pydapter[migrations]\n\n# Migrations with protocols support\npip install pydapter[migrations-all]\n</code></pre>"},{"location":"migrations/#key-concepts","title":"Key Concepts","text":""},{"location":"migrations/#migration-adapters","title":"Migration Adapters","text":"<p>Migration adapters implement the migration protocols and provide concrete functionality for specific database backends. The base module includes:</p> <ul> <li><code>BaseMigrationAdapter</code>: Abstract base class for all migration adapters</li> <li><code>SyncMigrationAdapter</code>: Base class for synchronous migration adapters</li> <li><code>AsyncMigrationAdapter</code>: Base class for asynchronous migration adapters</li> </ul>"},{"location":"migrations/#migration-protocols","title":"Migration Protocols","text":"<p>The module defines protocols that specify the interface for migration operations:</p> <ul> <li><code>MigrationProtocol</code>: Protocol for synchronous migration operations</li> <li><code>AsyncMigrationProtocol</code>: Protocol for asynchronous migration operations</li> </ul>"},{"location":"migrations/#sql-migrations-with-alembic","title":"SQL Migrations with Alembic","text":"<p>The SQL migrations implementation uses Alembic, a database migration tool for SQLAlchemy. It provides:</p> <ul> <li><code>AlembicAdapter</code>: Synchronous Alembic-based migration adapter</li> <li><code>AsyncAlembicAdapter</code>: Asynchronous Alembic-based migration adapter</li> </ul>"},{"location":"migrations/#basic-usage","title":"Basic Usage","text":""},{"location":"migrations/#initializing-migrations","title":"Initializing Migrations","text":"<p>Before you can create and apply migrations, you need to initialize the migration environment:</p> <pre><code>from pydapter.migrations import AlembicAdapter\nimport mymodels  # Module containing your SQLAlchemy models\n\n# Initialize migrations\nAlembicAdapter.init_migrations(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\",\n    models_module=mymodels\n)\n</code></pre> <p>This creates the necessary directory structure and configuration files for Alembic.</p>"},{"location":"migrations/#creating-migrations","title":"Creating Migrations","text":"<p>You can create migrations manually or automatically based on model changes:</p> <pre><code># Create a migration with auto-generation based on model changes\nrevision = AlembicAdapter.create_migration(\n    message=\"Add users table\",\n    autogenerate=True,\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\nprint(f\"Created migration: {revision}\")\n</code></pre> <p>The <code>autogenerate</code> parameter tells Alembic to compare your models with the current database schema and generate the necessary changes.</p>"},{"location":"migrations/#applying-migrations","title":"Applying Migrations","text":"<p>To apply migrations and update your database schema:</p> <pre><code># Upgrade to the latest version\nAlembicAdapter.upgrade(\n    revision=\"head\",  # \"head\" means the latest version\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\n</code></pre>"},{"location":"migrations/#reverting-migrations","title":"Reverting Migrations","text":"<p>If you need to revert to a previous version:</p> <pre><code># Downgrade to a specific revision\nAlembicAdapter.downgrade(\n    revision=\"ae1027a6acf\",  # Specific revision identifier\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\n</code></pre>"},{"location":"migrations/#checking-migration-status","title":"Checking Migration Status","text":"<p>You can check the current migration status:</p> <pre><code># Get the current revision\ncurrent = AlembicAdapter.get_current_revision(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\nprint(f\"Current revision: {current}\")\n\n# Get the full migration history\nhistory = AlembicAdapter.get_migration_history(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\nfor migration in history:\n    print(f\"{migration['revision']}: {migration['description']}\")\n</code></pre>"},{"location":"migrations/#asynchronous-migrations","title":"Asynchronous Migrations","text":"<p>For applications using asynchronous database connections, you can use the async migration adapter:</p> <pre><code>from pydapter.migrations import AsyncAlembicAdapter\nimport mymodels\n\n# Initialize migrations\nawait AsyncAlembicAdapter.init_migrations(\n    directory=\"migrations\",\n    connection_string=\"postgresql+asyncpg://user:pass@localhost/mydb\",\n    models_module=mymodels\n)\n\n# Create a migration\nrevision = await AsyncAlembicAdapter.create_migration(\n    message=\"Add users table\",\n    autogenerate=True,\n    directory=\"migrations\",\n    connection_string=\"postgresql+asyncpg://user:pass@localhost/mydb\"\n)\n\n# Apply migrations\nawait AsyncAlembicAdapter.upgrade(\n    revision=\"head\",\n    directory=\"migrations\",\n    connection_string=\"postgresql+asyncpg://user:pass@localhost/mydb\"\n)\n</code></pre>"},{"location":"migrations/#error-handling","title":"Error Handling","text":"<p>The migrations module provides a comprehensive error hierarchy:</p> <ul> <li><code>MigrationError</code>: Base exception for all migration errors</li> <li><code>MigrationInitError</code>: Raised when initialization fails</li> <li><code>MigrationCreationError</code>: Raised when migration creation fails</li> <li><code>MigrationUpgradeError</code>: Raised when upgrade fails</li> <li><code>MigrationDowngradeError</code>: Raised when downgrade fails</li> <li><code>MigrationNotFoundError</code>: Raised when a specified revision is not found</li> </ul> <p>Example of handling migration errors:</p> <pre><code>from pydapter.migrations import AlembicAdapter, MigrationError, MigrationUpgradeError\n\ntry:\n    AlembicAdapter.upgrade(\n        revision=\"head\",\n        directory=\"migrations\",\n        connection_string=\"postgresql://user:pass@localhost/mydb\"\n    )\nexcept MigrationUpgradeError as e:\n    print(f\"Failed to upgrade: {e}\")\n    # Handle specific upgrade error\nexcept MigrationError as e:\n    print(f\"Migration error: {e}\")\n    # Handle general migration error\n</code></pre>"},{"location":"migrations/#advanced-usage","title":"Advanced Usage","text":""},{"location":"migrations/#custom-migration-scripts","title":"Custom Migration Scripts","text":"<p>While auto-generated migrations work for many cases, you might need to write custom migration scripts for complex changes:</p> <ol> <li>Create a migration without auto-generation:</li> </ol> <pre><code>revision = AlembicAdapter.create_migration(\n    message=\"Custom data migration\",\n    autogenerate=False,\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\n</code></pre> <ol> <li>Edit the generated script in the <code>migrations/versions/</code> directory:</li> </ol> <pre><code>\"\"\"Custom data migration\n\nRevision ID: ae1027a6acf\nRevises: 1a2b3c4d5e6f\nCreate Date: 2025-05-16 10:30:00.000000\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n# revision identifiers\nrevision = 'ae1027a6acf'\ndown_revision = '1a2b3c4d5e6f'\nbranch_labels = None\ndepends_on = None\n\ndef upgrade():\n    # Custom upgrade operations\n    op.execute(\"\"\"\n        UPDATE users\n        SET status = 'active'\n        WHERE status = 'pending' AND created_at &lt; NOW() - INTERVAL '7 days'\n    \"\"\")\n\ndef downgrade():\n    # Custom downgrade operations\n    # Note: Data migrations are often not reversible\n    pass\n</code></pre>"},{"location":"migrations/#working-with-multiple-databases","title":"Working with Multiple Databases","text":"<p>If your application uses multiple databases, you can create separate migration directories for each:</p> <pre><code># Initialize migrations for the main database\nAlembicAdapter.init_migrations(\n    directory=\"migrations/main\",\n    connection_string=\"postgresql://user:pass@localhost/main_db\",\n    models_module=main_models\n)\n\n# Initialize migrations for the analytics database\nAlembicAdapter.init_migrations(\n    directory=\"migrations/analytics\",\n    connection_string=\"postgresql://user:pass@localhost/analytics_db\",\n    models_module=analytics_models\n)\n</code></pre> <p>Then apply migrations to each database separately:</p> <pre><code># Upgrade main database\nAlembicAdapter.upgrade(\n    revision=\"head\",\n    directory=\"migrations/main\",\n    connection_string=\"postgresql://user:pass@localhost/main_db\"\n)\n\n# Upgrade analytics database\nAlembicAdapter.upgrade(\n    revision=\"head\",\n    directory=\"migrations/analytics\",\n    connection_string=\"postgresql://user:pass@localhost/analytics_db\"\n)\n</code></pre>"},{"location":"migrations/#integration-with-sqlalchemy-models","title":"Integration with SQLAlchemy Models","text":"<p>The migrations module works best with SQLAlchemy models that follow the declarative base pattern:</p> <pre><code>from sqlalchemy import Column, Integer, String, create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = \"users\"\n\n    id = Column(Integer, primary_key=True)\n    username = Column(String, unique=True, nullable=False)\n    email = Column(String, unique=True, nullable=False)\n\n# Later, when initializing migrations:\nAlembicAdapter.init_migrations(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\",\n    models_module=sys.modules[__name__]  # Current module containing models\n)\n</code></pre>"},{"location":"migrations/#best-practices","title":"Best Practices","text":""},{"location":"migrations/#migration-workflow","title":"Migration Workflow","text":"<p>Follow these best practices for a smooth migration workflow:</p> <ol> <li>Always back up your database before applying migrations</li> <li>Test migrations in a development environment first</li> <li>Keep migrations small and focused on specific changes</li> <li>Include descriptive messages for each migration</li> <li>Ensure both upgrade and downgrade functions are implemented</li> <li>Use transactions for data safety</li> </ol>"},{"location":"migrations/#organizing-migrations","title":"Organizing Migrations","text":"<p>For larger projects, consider these organizational tips:</p> <ol> <li>Use a consistent naming convention for migration files</li> <li>Group related migrations in branches when appropriate</li> <li>Document complex migrations with comments</li> <li>Include the related issue or ticket number in migration messages</li> </ol>"},{"location":"migrations/#deployment-considerations","title":"Deployment Considerations","text":"<p>When deploying migrations to production:</p> <ol> <li>Include migrations in your CI/CD pipeline</li> <li>Apply migrations during maintenance windows when possible</li> <li>Have a rollback plan for each migration</li> <li>Monitor database performance during and after migrations</li> <li>Consider using a separate deployment step for migrations</li> </ol>"},{"location":"migrations/#extending-the-migrations-framework","title":"Extending the Migrations Framework","text":"<p>You can extend the migrations framework by creating custom adapters for other database systems:</p> <pre><code>from pydapter.migrations.base import SyncMigrationAdapter\nfrom typing import ClassVar, Optional, List, Dict, Any\n\nclass CustomDatabaseAdapter(SyncMigrationAdapter):\n    \"\"\"Custom migration adapter for a specific database system.\"\"\"\n\n    migration_key: ClassVar[str] = \"custom_db\"\n\n    @classmethod\n    def init_migrations(cls, directory: str, **kwargs) -&gt; None:\n        # Implementation for initializing migrations\n        pass\n\n    @classmethod\n    def create_migration(cls, message: str, autogenerate: bool = True, **kwargs) -&gt; str:\n        # Implementation for creating migrations\n        pass\n\n    @classmethod\n    def upgrade(cls, revision: str = \"head\", **kwargs) -&gt; None:\n        # Implementation for upgrading\n        pass\n\n    @classmethod\n    def downgrade(cls, revision: str, **kwargs) -&gt; None:\n        # Implementation for downgrading\n        pass\n\n    @classmethod\n    def get_current_revision(cls, **kwargs) -&gt; Optional[str]:\n        # Implementation for getting current revision\n        pass\n\n    @classmethod\n    def get_migration_history(cls, **kwargs) -&gt; List[Dict[str, Any]]:\n        # Implementation for getting migration history\n        pass\n</code></pre>"},{"location":"migrations/#troubleshooting","title":"Troubleshooting","text":""},{"location":"migrations/#common-issues","title":"Common Issues","text":"<ol> <li>Missing dependencies:</li> </ol> <pre><code>ImportError: The 'migrations-sql' feature requires the 'sqlalchemy' package.\n</code></pre> <p>Solution: Install the required dependencies with    <code>pip install pydapter[migrations-sql]</code></p> <ol> <li>Alembic command not found:</li> </ol> <pre><code>ModuleNotFoundError: No module named 'alembic'\n</code></pre> <p>Solution: Install Alembic with <code>pip install alembic</code></p> <ol> <li> <p>Autogeneration not detecting changes: Solution: Ensure your models are    imported and accessible in the environment where migrations are created</p> </li> <li> <p>Conflicts between migrations: Solution: Ensure you're working with the    latest revision before creating new migrations</p> </li> </ol>"},{"location":"migrations/#getting-help","title":"Getting Help","text":"<p>If you encounter issues with migrations, check:</p> <ol> <li>The Alembic documentation: https://alembic.sqlalchemy.org/</li> <li>SQLAlchemy documentation: https://docs.sqlalchemy.org/</li> <li>pydapter GitHub issues: https://github.com/yourusername/pydapter/issues</li> </ol>"},{"location":"neo4j_adapter/","title":"Neo4j Adapter Tutorial for Pydapter","text":"<p>This tutorial will show you how to use pydapter's Neo4j adapter to seamlessly convert between Pydantic models and Neo4j graph databases. You'll learn how to model, store, and query graph data using Pydantic's validation capabilities.</p>"},{"location":"neo4j_adapter/#prerequisites","title":"Prerequisites","text":""},{"location":"neo4j_adapter/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code># Create a virtual environment if you haven't already\npython -m venv pydapter-demo\nsource pydapter-demo/bin/activate  # On Windows: pydapter-demo\\Scripts\\activate\n\n# Install dependencies\npip install pydantic neo4j\n\n# Install pydapter (if you haven't done so already)\n# Either from PyPI when available:\n# pip install pydapter\n# Or from the repository:\ngit clone https://github.com/ohdearquant/pydapter.git\ncd pydapter\npip install -e .\n</code></pre>"},{"location":"neo4j_adapter/#2-set-up-neo4j","title":"2. Set Up Neo4j","text":"<p>The easiest way to set up Neo4j is using Docker:</p> <pre><code># Run Neo4j in Docker with a password\ndocker run \\\n    --name neo4j-pydapter \\\n    -p 7474:7474 -p 7687:7687 \\\n    -e NEO4J_AUTH=neo4j/password \\\n    -d neo4j:latest\n</code></pre> <p>Alternatively, you can:</p> <ul> <li>Download and install Neo4j Desktop from   Neo4j's website</li> <li>Use Neo4j AuraDB cloud service</li> <li>Install Neo4j directly on your system</li> </ul> <p>With Docker, you can access:</p> <ul> <li>Neo4j Browser UI at http://localhost:7474</li> <li>Bolt protocol at bolt://localhost:7687</li> </ul>"},{"location":"neo4j_adapter/#basic-example-person-management-system","title":"Basic Example: Person Management System","text":"<p>Let's build a simple person management system using Neo4j and pydapter:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\n\n# Neo4j connection settings\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_AUTH = (\"neo4j\", \"password\")  # Default credentials, change if different\n\n# Define a Pydantic model\nclass Person(BaseModel):\n    id: str\n    name: str\n    age: int\n    email: Optional[str] = None\n    interests: List[str] = []\n\n# Create some test data\npeople = [\n    Person(id=\"p1\", name=\"Alice\", age=30, email=\"alice@example.com\", interests=[\"coding\", \"hiking\"]),\n    Person(id=\"p2\", name=\"Bob\", age=25, email=\"bob@example.com\", interests=[\"gaming\", \"cooking\"]),\n    Person(id=\"p3\", name=\"Charlie\", age=35, email=\"charlie@example.com\", interests=[\"reading\", \"travel\"])\n]\n\n# Store data in Neo4j\ndef store_people(people_list):\n    print(f\"Storing {len(people_list)} people in Neo4j...\")\n\n    for person in people_list:\n        result = Neo4jAdapter.to_obj(\n            person,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Person\",  # Node label in Neo4j\n            merge_on=\"id\"    # Property to use for MERGE operation\n        )\n        print(f\"Stored {person.name}: {result}\")\n\n# Retrieve all people\ndef get_all_people():\n    print(\"Retrieving all people from Neo4j...\")\n\n    people = Neo4jAdapter.from_obj(\n        Person,\n        {\n            \"url\": NEO4J_URI,\n            \"auth\": NEO4J_AUTH,\n            \"label\": \"Person\"\n        },\n        many=True\n    )\n\n    print(f\"Found {len(people)} people:\")\n    for person in people:\n        print(f\"  - {person.name} (Age: {person.age}, Email: {person.email})\")\n        if person.interests:\n            print(f\"    Interests: {', '.join(person.interests)}\")\n\n    return people\n\n# Find people by property\ndef find_people_by_property(property_name, property_value):\n    print(f\"Finding people with {property_name}={property_value}...\")\n\n    where_clause = f\"n.{property_name} = '{property_value}'\"\n\n    people = Neo4jAdapter.from_obj(\n        Person,\n        {\n            \"url\": NEO4J_URI,\n            \"auth\": NEO4J_AUTH,\n            \"label\": \"Person\",\n            \"where\": where_clause\n        },\n        many=True\n    )\n\n    print(f\"Found {len(people)} matching people:\")\n    for person in people:\n        print(f\"  - {person.name} (Age: {person.age}, Email: {person.email})\")\n\n    return people\n\n# Main function to demo the adapter\ndef main():\n    # First, store people\n    store_people(people)\n\n    # Retrieve all people\n    all_people = get_all_people()\n\n    # Find people with specific properties\n    young_people = find_people_by_property(\"age\", \"25\")\n\n    # Find by email domain (using ENDS WITH in Cypher)\n    print(\"\\nFinding people with example.com email addresses...\")\n    example_emails = Neo4jAdapter.from_obj(\n        Person,\n        {\n            \"url\": NEO4J_URI,\n            \"auth\": NEO4J_AUTH,\n            \"label\": \"Person\",\n            \"where\": \"n.email ENDS WITH 'example.com'\"\n        },\n        many=True\n    )\n\n    print(f\"Found {len(example_emails)} people with example.com emails:\")\n    for person in example_emails:\n        print(f\"  - {person.name}: {person.email}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"neo4j_adapter/#working-with-relationships","title":"Working with Relationships","text":"<p>One of Neo4j's key features is its ability to model relationships between nodes. Let's expand our example to include relationships:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\nfrom neo4j import GraphDatabase\n\n# Neo4j connection settings\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_AUTH = (\"neo4j\", \"password\")\n\n# Define models\nclass Person(BaseModel):\n    id: str\n    name: str\n    age: int\n    email: Optional[str] = None\n\nclass Hobby(BaseModel):\n    id: str\n    name: str\n    category: Optional[str] = None\n\n# Custom function to create relationships\n# (Since pydapter doesn't directly handle relationships yet)\ndef create_relationship(person_id, hobby_id, relationship_type=\"ENJOYS\"):\n    \"\"\"Create a relationship between a Person and a Hobby\"\"\"\n    driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n\n    with driver.session() as session:\n        result = session.run(\n            f\"\"\"\n            MATCH (p:Person {{id: $person_id}})\n            MATCH (h:Hobby {{id: $hobby_id}})\n            MERGE (p)-[r:{relationship_type}]-&gt;(h)\n            RETURN p.name, h.name\n            \"\"\",\n            person_id=person_id,\n            hobby_id=hobby_id\n        )\n\n        for record in result:\n            print(f\"Created relationship: {record['p.name']} {relationship_type} {record['h.name']}\")\n\n    driver.close()\n\n# Function to find people who enjoy a specific hobby\ndef find_people_by_hobby(hobby_name):\n    \"\"\"Find all people who enjoy a specific hobby\"\"\"\n    driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n\n    people_list = []\n\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (p:Person)-[:ENJOYS]-&gt;(h:Hobby {name: $hobby_name})\n            RETURN p\n            \"\"\",\n            hobby_name=hobby_name\n        )\n\n        for record in result:\n            # Convert Neo4j node properties to dict\n            person_data = dict(record[\"p\"].items())\n            # Create Pydantic model from data\n            person = Person(**person_data)\n            people_list.append(person)\n\n    driver.close()\n    return people_list\n\n# Function to find hobbies for a specific person\ndef find_hobbies_for_person(person_id):\n    \"\"\"Find all hobbies for a specific person\"\"\"\n    driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n\n    hobbies_list = []\n\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (p:Person {id: $person_id})-[:ENJOYS]-&gt;(h:Hobby)\n            RETURN h\n            \"\"\",\n            person_id=person_id\n        )\n\n        for record in result:\n            hobby_data = dict(record[\"h\"].items())\n            hobby = Hobby(**hobby_data)\n            hobbies_list.append(hobby)\n\n    driver.close()\n    return hobbies_list\n\n# Main function to demo relationships\ndef main():\n    # Create people\n    people = [\n        Person(id=\"p1\", name=\"Alice\", age=30, email=\"alice@example.com\"),\n        Person(id=\"p2\", name=\"Bob\", age=25, email=\"bob@example.com\"),\n        Person(id=\"p3\", name=\"Charlie\", age=35, email=\"charlie@example.com\")\n    ]\n\n    # Create hobbies\n    hobbies = [\n        Hobby(id=\"h1\", name=\"Coding\", category=\"Technical\"),\n        Hobby(id=\"h2\", name=\"Hiking\", category=\"Outdoor\"),\n        Hobby(id=\"h3\", name=\"Reading\", category=\"Indoor\"),\n        Hobby(id=\"h4\", name=\"Cooking\", category=\"Indoor\"),\n        Hobby(id=\"h5\", name=\"Gaming\", category=\"Entertainment\")\n    ]\n\n    # Store people in Neo4j\n    print(\"Storing people...\")\n    for person in people:\n        Neo4jAdapter.to_obj(\n            person,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Person\",\n            merge_on=\"id\"\n        )\n\n    # Store hobbies in Neo4j\n    print(\"\\nStoring hobbies...\")\n    for hobby in hobbies:\n        Neo4jAdapter.to_obj(\n            hobby,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Hobby\",\n            merge_on=\"id\"\n        )\n\n    # Create relationships\n    print(\"\\nCreating relationships...\")\n    # Alice enjoys Coding, Hiking, and Reading\n    create_relationship(\"p1\", \"h1\")\n    create_relationship(\"p1\", \"h2\")\n    create_relationship(\"p1\", \"h3\")\n\n    # Bob enjoys Gaming and Cooking\n    create_relationship(\"p2\", \"h4\")\n    create_relationship(\"p2\", \"h5\")\n\n    # Charlie enjoys Reading and Hiking\n    create_relationship(\"p3\", \"h2\")\n    create_relationship(\"p3\", \"h3\")\n\n    # Find people who enjoy Hiking\n    print(\"\\nPeople who enjoy Hiking:\")\n    hikers = find_people_by_hobby(\"Hiking\")\n    for person in hikers:\n        print(f\"  - {person.name} (Age: {person.age})\")\n\n    # Find hobbies for Alice\n    print(\"\\nAlice's hobbies:\")\n    alice_hobbies = find_hobbies_for_person(\"p1\")\n    for hobby in alice_hobbies:\n        print(f\"  - {hobby.name} (Category: {hobby.category})\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"neo4j_adapter/#advanced-example-movie-recommendation-system","title":"Advanced Example: Movie Recommendation System","text":"<p>Let's build a more complex example - a movie recommendation system that demonstrates advanced Neo4j features and pydapter integration:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional, Dict, Any\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\nfrom neo4j import GraphDatabase\nimport random\n\n# Neo4j connection settings\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_AUTH = (\"neo4j\", \"password\")\n\n# Define our models\nclass Person(BaseModel):\n    id: str\n    name: str\n    age: Optional[int] = None\n\nclass Movie(BaseModel):\n    id: str\n    title: str\n    year: int\n    genre: List[str] = []\n    rating: Optional[float] = None\n\nclass Actor(Person):\n    roles: List[str] = []\n\nclass Director(Person):\n    movies_directed: int = 0\n\n# Helper function to create Neo4j driver\ndef get_driver():\n    return GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n\n# Initialize the database with schema and constraints\ndef initialize_database():\n    driver = get_driver()\n\n    with driver.session() as session:\n        # Create constraints to ensure uniqueness\n        session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (p:Person) REQUIRE p.id IS UNIQUE\")\n        session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (m:Movie) REQUIRE m.id IS UNIQUE\")\n\n    driver.close()\n    print(\"Database initialized with constraints\")\n\n# Helper function to create relationships\ndef create_relationship(start_id, end_id, start_label, end_label, rel_type, properties=None):\n    driver = get_driver()\n\n    props_str = \"\"\n    if properties:\n        props_list = [f\"{k}: ${k}\" for k in properties.keys()]\n        props_str = \"{\" + \", \".join(props_list) + \"}\"\n\n    with driver.session() as session:\n        query = f\"\"\"\n        MATCH (a:{start_label} {{id: $start_id}})\n        MATCH (b:{end_label} {{id: $end_id}})\n        MERGE (a)-[r:{rel_type} {props_str}]-&gt;(b)\n        RETURN a.name, b.title\n        \"\"\"\n\n        params = {\"start_id\": start_id, \"end_id\": end_id}\n        if properties:\n            params.update(properties)\n\n        result = session.run(query, params)\n        data = result.single()\n        if data:\n            print(f\"Created relationship: {data[0]} {rel_type} {data[1]}\")\n\n    driver.close()\n\n# Populate the database with sample data\ndef populate_database():\n    # Create some movies\n    movies = [\n        Movie(id=\"m1\", title=\"The Matrix\", year=1999,\n              genre=[\"Sci-Fi\", \"Action\"], rating=8.7),\n        Movie(id=\"m2\", title=\"Inception\", year=2010,\n              genre=[\"Sci-Fi\", \"Action\", \"Thriller\"], rating=8.8),\n        Movie(id=\"m3\", title=\"The Shawshank Redemption\", year=1994,\n              genre=[\"Drama\"], rating=9.3),\n        Movie(id=\"m4\", title=\"Pulp Fiction\", year=1994,\n              genre=[\"Crime\", \"Drama\"], rating=8.9),\n        Movie(id=\"m5\", title=\"The Dark Knight\", year=2008,\n              genre=[\"Action\", \"Crime\", \"Drama\"], rating=9.0),\n    ]\n\n    # Create some actors\n    actors = [\n        Actor(id=\"a1\", name=\"Keanu Reeves\", age=57, roles=[\"Neo\", \"John Wick\"]),\n        Actor(id=\"a2\", name=\"Leonardo DiCaprio\", age=46, roles=[\"Dom Cobb\", \"Jack Dawson\"]),\n        Actor(id=\"a3\", name=\"Morgan Freeman\", age=84, roles=[\"Ellis Boyd 'Red' Redding\"]),\n        Actor(id=\"a4\", name=\"Tim Robbins\", age=62, roles=[\"Andy Dufresne\"]),\n        Actor(id=\"a5\", name=\"John Travolta\", age=67, roles=[\"Vincent Vega\"]),\n        Actor(id=\"a6\", name=\"Samuel L. Jackson\", age=72, roles=[\"Jules Winnfield\"]),\n        Actor(id=\"a7\", name=\"Christian Bale\", age=47, roles=[\"Bruce Wayne\"]),\n    ]\n\n    # Create some directors\n    directors = [\n        Director(id=\"d1\", name=\"Lana Wachowski\", age=56, movies_directed=5),\n        Director(id=\"d2\", name=\"Christopher Nolan\", age=51, movies_directed=11),\n        Director(id=\"d3\", name=\"Frank Darabont\", age=62, movies_directed=4),\n        Director(id=\"d4\", name=\"Quentin Tarantino\", age=58, movies_directed=9),\n    ]\n\n    # Store movies in Neo4j\n    print(\"Storing movies...\")\n    for movie in movies:\n        Neo4jAdapter.to_obj(\n            movie,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Movie\",\n            merge_on=\"id\"\n        )\n\n    # Store actors in Neo4j\n    print(\"\\nStoring actors...\")\n    for actor in actors:\n        # Convert to dict and add label\n        actor_dict = actor.model_dump()\n\n        # Store using Neo4jAdapter\n        Neo4jAdapter.to_obj(\n            actor,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Actor\",  # Use Actor label\n            merge_on=\"id\"\n        )\n\n    # Store directors in Neo4j\n    print(\"\\nStoring directors...\")\n    for director in directors:\n        Neo4jAdapter.to_obj(\n            director,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Director\",  # Use Director label\n            merge_on=\"id\"\n        )\n\n    # Create relationships\n    print(\"\\nCreating relationships...\")\n\n    # Matrix relationships\n    create_relationship(\"a1\", \"m1\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Neo\"})\n    create_relationship(\"d1\", \"m1\", \"Director\", \"Movie\", \"DIRECTED\")\n\n    # Inception relationships\n    create_relationship(\"a2\", \"m2\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Dom Cobb\"})\n    create_relationship(\"d2\", \"m2\", \"Director\", \"Movie\", \"DIRECTED\")\n\n    # Shawshank Redemption relationships\n    create_relationship(\"a3\", \"m3\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Ellis Boyd 'Red' Redding\"})\n    create_relationship(\"a4\", \"m3\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Andy Dufresne\"})\n    create_relationship(\"d3\", \"m3\", \"Director\", \"Movie\", \"DIRECTED\")\n\n    # Pulp Fiction relationships\n    create_relationship(\"a5\", \"m4\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Vincent Vega\"})\n    create_relationship(\"a6\", \"m4\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Jules Winnfield\"})\n    create_relationship(\"d4\", \"m4\", \"Director\", \"Movie\", \"DIRECTED\")\n\n    # Dark Knight relationships\n    create_relationship(\"a7\", \"m5\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Bruce Wayne\"})\n    create_relationship(\"d2\", \"m5\", \"Director\", \"Movie\", \"DIRECTED\")\n\n    # Create user ratings\n    create_user_ratings()\n\n    print(\"Database populated with sample data\")\n\n# Create some users and their ratings\ndef create_user_ratings():\n    # Create users\n    users = [\n        Person(id=\"u1\", name=\"User One\", age=25),\n        Person(id=\"u2\", name=\"User Two\", age=35),\n        Person(id=\"u3\", name=\"User Three\", age=45),\n    ]\n\n    # Store users\n    print(\"\\nStoring users...\")\n    for user in users:\n        Neo4jAdapter.to_obj(\n            user,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"User\",\n            merge_on=\"id\"\n        )\n\n    # Create rating relationships\n    driver = get_driver()\n\n    with driver.session() as session:\n        # Get all movie IDs\n        result = session.run(\"MATCH (m:Movie) RETURN m.id AS id\")\n        movie_ids = [record[\"id\"] for record in result]\n\n        # For each user, create some random ratings\n        for user_id in [\"u1\", \"u2\", \"u3\"]:\n            for movie_id in movie_ids:\n                # Randomly decide if user rated this movie\n                if random.random() &gt; 0.3:  # 70% chance of rating\n                    rating = round(random.uniform(1, 5) * 2) / 2  # Rating from 1 to 5, in 0.5 steps\n\n                    session.run(\n                        \"\"\"\n                        MATCH (u:User {id: $user_id})\n                        MATCH (m:Movie {id: $movie_id})\n                        MERGE (u)-[r:RATED]-&gt;(m)\n                        SET r.rating = $rating\n                        \"\"\",\n                        user_id=user_id,\n                        movie_id=movie_id,\n                        rating=rating\n                    )\n                    print(f\"User {user_id} rated movie {movie_id} with {rating}\")\n\n    driver.close()\n\n# Function to get movie recommendations for a user\ndef get_movie_recommendations(user_id):\n    \"\"\"\n    Get movie recommendations for a user based on:\n    1. Movies they haven't seen\n    2. Movies liked by users with similar tastes\n    3. Movies in genres they like\n    \"\"\"\n    driver = get_driver()\n\n    recommendations = []\n\n    with driver.session() as session:\n        # Get movies the user hasn't rated,\n        # but are highly rated by users with similar tastes\n        result = session.run(\n            \"\"\"\n            MATCH (target:User {id: $user_id})-[r1:RATED]-&gt;(m:Movie)\n            MATCH (other:User)-[r2:RATED]-&gt;(m)\n            WHERE other.id &lt;&gt; $user_id AND abs(r1.rating - r2.rating) &lt; 1\n            MATCH (other)-[r3:RATED]-&gt;(rec:Movie)\n            WHERE r3.rating &gt;= 4\n            AND NOT EXISTS { MATCH (target)-[:RATED]-&gt;(rec) }\n            WITH rec, count(*) AS strength, avg(r3.rating) AS avg_rating\n            ORDER BY strength DESC, avg_rating DESC\n            LIMIT 5\n            RETURN rec\n            \"\"\",\n            user_id=user_id\n        )\n\n        for record in result:\n            movie_data = dict(record[\"rec\"].items())\n            movie = Movie(**movie_data)\n            recommendations.append(movie)\n\n    driver.close()\n    return recommendations\n\n# Get movies directed by a specific director\ndef get_movies_by_director(director_name):\n    \"\"\"Get all movies directed by a specific director\"\"\"\n    driver = get_driver()\n\n    movies_list = []\n\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (d:Director {name: $director_name})-[:DIRECTED]-&gt;(m:Movie)\n            RETURN m\n            \"\"\",\n            director_name=director_name\n        )\n\n        for record in result:\n            movie_data = dict(record[\"m\"].items())\n            movie = Movie(**movie_data)\n            movies_list.append(movie)\n\n    driver.close()\n    return movies_list\n\n# Get actors who worked with a specific actor\ndef get_co_actors(actor_name):\n    \"\"\"Get all actors who acted in the same movie as the specified actor\"\"\"\n    driver = get_driver()\n\n    co_actors = []\n\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (a:Actor {name: $actor_name})-[:ACTED_IN]-&gt;(m:Movie)&lt;-[:ACTED_IN]-(co:Actor)\n            WHERE co.name &lt;&gt; $actor_name\n            RETURN DISTINCT co\n            \"\"\",\n            actor_name=actor_name\n        )\n\n        for record in result:\n            actor_data = dict(record[\"co\"].items())\n            actor = Actor(**actor_data)\n            co_actors.append(actor)\n\n    driver.close()\n    return co_actors\n\n# Main function to demo the movie recommendation system\ndef main():\n    # Initialize and populate the database\n    initialize_database()\n    populate_database()\n\n    # Get movie recommendations for User One\n    print(\"\\nMovie recommendations for User One:\")\n    recommendations = get_movie_recommendations(\"u1\")\n    for movie in recommendations:\n        print(f\"  - {movie.title} ({movie.year}) - Rating: {movie.rating}\")\n\n    # Get movies directed by Christopher Nolan\n    print(\"\\nMovies directed by Christopher Nolan:\")\n    nolan_movies = get_movies_by_director(\"Christopher Nolan\")\n    for movie in nolan_movies:\n        print(f\"  - {movie.title} ({movie.year}) - Rating: {movie.rating}\")\n\n    # Get actors who worked with Keanu Reeves\n    print(\"\\nActors who worked with Keanu Reeves:\")\n    keanu_co_actors = get_co_actors(\"Keanu Reeves\")\n    for actor in keanu_co_actors:\n        print(f\"  - {actor.name} (Age: {actor.age})\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"neo4j_adapter/#error-handling-with-neo4j-adapter","title":"Error Handling with Neo4j Adapter","text":"<p>Let's demonstrate proper error handling for common Neo4j operations:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\nfrom pydapter.exceptions import ConnectionError, QueryError, ResourceError, ValidationError\n\n# Define a simple model\nclass Person(BaseModel):\n    id: str\n    name: str\n    age: int\n\ndef neo4j_error_handling():\n    print(\"Testing error handling for Neo4j operations...\")\n\n    # 1. Connection error - wrong authentication\n    try:\n        Neo4jAdapter.from_obj(\n            Person,\n            {\n                \"url\": \"bolt://localhost:7687\",\n                \"auth\": (\"neo4j\", \"wrong_password\"),\n                \"label\": \"Person\"\n            }\n        )\n    except ConnectionError as e:\n        print(f\"Authentication error handled: {e}\")\n\n    # 2. Connection error - wrong host\n    try:\n        Neo4jAdapter.from_obj(\n            Person,\n            {\n                \"url\": \"bolt://nonexistent-host:7687\",\n                \"auth\": (\"neo4j\", \"password\"),\n                \"label\": \"Person\"\n            }\n        )\n    except ConnectionError as e:\n        print(f\"Host connection error handled: {e}\")\n\n    # 3. Query error - Cypher syntax error\n    try:\n        # Create a valid connection but inject a syntax error\n        # Note: The adapter validates basic Cypher, but we can still get Neo4j errors\n        Neo4jAdapter.from_obj(\n            Person,\n            {\n                \"url\": \"bolt://localhost:7687\",\n                \"auth\": (\"neo4j\", \"password\"),\n                \"label\": \"Person\",\n                \"where\": \"n.age ==\" # Invalid Cypher syntax (missing value)\n            }\n        )\n    except QueryError as e:\n        print(f\"Cypher syntax error handled: {e}\")\n\n    # 4. Resource error - nonexistent label\n    try:\n        # This assumes the database is empty or this label doesn't exist\n        Neo4jAdapter.from_obj(\n            Person,\n            {\n                \"url\": \"bolt://localhost:7687\",\n                \"auth\": (\"neo4j\", \"password\"),\n                \"label\": \"NonexistentLabel\"\n            }\n        )\n    except ResourceError as e:\n        print(f\"Resource error handled: {e}\")\n\n# Run the error handling examples\nneo4j_error_handling()\n</code></pre>"},{"location":"neo4j_adapter/#using-neo4j-with-adaptable-mixin","title":"Using Neo4j with Adaptable Mixin","text":"<p>For a more ergonomic API, you can use the <code>Adaptable</code> mixin with the Neo4j adapter:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\nfrom pydapter.core import Adaptable\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\n\n# Neo4j connection settings\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_AUTH = (\"neo4j\", \"password\")\n\n# Define a model with the Adaptable mixin\nclass Product(BaseModel, Adaptable):\n    id: str\n    name: str\n    price: float\n    category: str\n    in_stock: bool = True\n    tags: List[str] = []\n\n# Register the Neo4j adapter\nProduct.register_adapter(Neo4jAdapter)\n\ndef adaptable_mixin_demo():\n    # Create products\n    products = [\n        Product(id=\"prod1\", name=\"Laptop\", price=1299.99, category=\"Electronics\", tags=[\"computer\", \"portable\"]),\n        Product(id=\"prod2\", name=\"Smartphone\", price=899.99, category=\"Electronics\", tags=[\"mobile\", \"portable\"]),\n        Product(id=\"prod3\", name=\"Headphones\", price=199.99, category=\"Audio\", tags=[\"audio\", \"portable\"])\n    ]\n\n    # Store products using the mixin\n    print(\"Storing products using Adaptable mixin...\")\n    for product in products:\n        result = product.adapt_to(\n            obj_key=\"neo4j\",\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Product\",\n            merge_on=\"id\"\n        )\n        print(f\"Stored {product.name}: {result}\")\n\n    # Retrieve products by category\n    print(\"\\nRetrieving electronics products...\")\n    electronics = Product.adapt_from(\n        {\n            \"url\": NEO4J_URI,\n            \"auth\": NEO4J_AUTH,\n            \"label\": \"Product\",\n            \"where\": \"n.category = 'Electronics'\"\n        },\n        obj_key=\"neo4j\",\n        many=True\n    )\n\n    print(f\"Found {len(electronics)} electronics products:\")\n    for product in electronics:\n        print(f\"  - {product.name}: ${product.price}\")\n        print(f\"    Tags: {', '.join(product.tags)}\")\n\n# Run the adaptable mixin demo\nadaptable_mixin_demo()\n</code></pre>"},{"location":"neo4j_adapter/#complete-example-social-network-analysis","title":"Complete Example: Social Network Analysis","text":"<p>Let's build a more complete example that showcases Neo4j's strengths for social network analysis:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nfrom pydapter.core import Adaptable\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\nfrom neo4j import GraphDatabase\nimport random\n\n# Neo4j connection settings\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_AUTH = (\"neo4j\", \"password\")\n\n# Define our models\nclass User(BaseModel, Adaptable):\n    id: str\n    username: str\n    full_name: Optional[str] = None\n    email: Optional[str] = None\n    location: Optional[str] = None\n    joined_date: Optional[str] = None\n\nclass Post(BaseModel, Adaptable):\n    id: str\n    content: str\n    created_at: str\n    likes: int = 0\n    user_id: str  # Author of the post\n\n# Register adapters\nUser.register_adapter(Neo4jAdapter)\nPost.register_adapter(Neo4jAdapter)\n\n# Helper function to create Neo4j driver\ndef get_driver():\n    return GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n\n# Initialize the database with schema and constraints\ndef initialize_database():\n    driver = get_driver()\n\n    with driver.session() as session:\n        # Create constraints for uniqueness\n        session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (u:User) REQUIRE u.id IS UNIQUE\")\n        session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (p:Post) REQUIRE p.id IS UNIQUE\")\n\n    driver.close()\n    print(\"Database initialized with constraints\")\n\n# Create relationships between users (follows) and between users and posts\ndef create_relationships(users, posts):\n    driver = get_driver()\n\n    with driver.session() as session:\n        # Connect users with their posts\n        print(\"\\nConnecting users with their posts...\")\n        for post in posts:\n            session.run(\n                \"\"\"\n                MATCH (u:User {id: $user_id})\n                MATCH (p:Post {id: $post_id})\n                MERGE (u)-[:POSTED]-&gt;(p)\n                \"\"\",\n                user_id=post.user_id,\n                post_id=post.id\n            )\n            print(f\"Connected user {post.user_id} with post {post.id}\")\n\n        # Create random follow relationships between users\n        print(\"\\nCreating follow relationships...\")\n        user_ids = [user.id for user in users]\n\n        for user_id in user_ids:\n            # Each user follows a random subset of other users\n            for other_id in user_ids:\n                if user_id != other_id and random.random() &lt; 0.3:  # 30% chance to follow\n                    session.run(\n                        \"\"\"\n                        MATCH (u1:User {id: $user_id})\n                        MATCH (u2:User {id: $other_id})\n                        MERGE (u1)-[:FOLLOWS]-&gt;(u2)\n                        \"\"\",\n                        user_id=user_id,\n                        other_id=other_id\n                    )\n                    print(f\"User {user_id} follows User {other_id}\")\n\n        # Create some likes on posts\n        print(\"\\nCreating likes on posts...\")\n        for user_id in user_ids:\n            for post in posts:\n                # Users don't like their own posts, and random chance to like others\n                if post.user_id != user_id and random.random() &lt; 0.4:  # 40% chance to like\n                    session.run(\n                        \"\"\"\n                        MATCH (u:User {id: $user_id})\n                        MATCH (p:Post {id: $post_id})\n                        MERGE (u)-[:LIKES]-&gt;(p)\n                        \"\"\",\n                        user_id=user_id,\n                        post_id=post.id\n                    )\n\n                    # Also update the likes count on the post\n                    session.run(\n                        \"\"\"\n                        MATCH (p:Post {id: $post_id})\n                        SET p.likes = p.likes + 1\n                        \"\"\",\n                        post_id=post.id\n                    )\n\n                    print(f\"User {user_id} likes Post {post.id}\")\n\n    driver.close()\n\n# Populate the database with users and posts\ndef populate_database():\n    # Create some users\n    users = [\n        User(\n            id=\"u1\",\n            username=\"alice_wonder\",\n            full_name=\"Alice Wonderland\",\n            email=\"alice@example.com\",\n            location=\"New York\",\n            joined_date=datetime(2022, 1, 15).isoformat()\n        ),\n        User(\n            id=\"u2\",\n            username=\"bob_builder\",\n            full_name=\"Bob Builder\",\n            email=\"bob@example.com\",\n            location=\"San Francisco\",\n            joined_date=datetime(2022, 2, 20).isoformat()\n        ),\n        User(\n            id=\"u3\",\n            username=\"charlie_brown\",\n            full_name=\"Charlie Brown\",\n            email=\"charlie@example.com\",\n            location=\"Chicago\",\n            joined_date=datetime(2022, 3, 10).isoformat()\n        ),\n        User(\n            id=\"u4\",\n            username=\"david_jones\",\n            full_name=\"David Jones\",\n            email=\"david@example.com\",\n            location=\"Miami\",\n            joined_date=datetime(2022, 4, 5).isoformat()\n        ),\n        User(\n            id=\"u5\",\n            username=\"emma_stone\",\n            full_name=\"Emma Stone\",\n            email=\"emma@example.com\",\n            location=\"Los Angeles\",\n            joined_date=datetime(2022, 5, 1).isoformat()\n        ),\n    ]\n\n    # Create some posts\n    posts = [\n        Post(\n            id=\"p1\",\n            content=\"Just learned about Neo4j and graph databases!\",\n            created_at=datetime(2023, 1, 5).isoformat(),\n            user_id=\"u1\"\n        ),\n        Post(\n            id=\"p2\",\n            content=\"Excited to start my new project with Python\",\n            created_at=datetime(2023, 1, 10).isoformat(),\n            user_id=\"u1\"\n        ),\n        Post(\n            id=\"p3\",\n            content=\"San Francisco has the best views!\",\n            created_at=datetime(2023, 1, 8).isoformat(),\n            user_id=\"u2\"\n        ),\n        Post(\n            id=\"p4\",\n            content=\"Working on a new recommendation algorithm\",\n            created_at=datetime(2023, 1, 12).isoformat(),\n            user_id=\"u3\"\n        ),\n        Post(\n            id=\"p5\",\n            content=\"Just finished reading a great book about AI\",\n            created_at=datetime(2023, 1, 15).isoformat(),\n            user_id=\"u3\"\n        ),\n        Post(\n            id=\"p6\",\n            content=\"Miami sunsets are unbeatable!\",\n            created_at=datetime(2023, 1, 14).isoformat(),\n            user_id=\"u4\"\n        ),\n        Post(\n            id=\"p7\",\n            content=\"Excited about new movie roles coming up\",\n            created_at=datetime(2023, 1, 18).isoformat(),\n            user_id=\"u5\"\n        ),\n    ]\n\n    # Store users in Neo4j\n    print(\"Storing users...\")\n    for user in users:\n        user.adapt_to(\n            obj_key=\"neo4j\",\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"User\",\n            merge_on=\"id\"\n        )\n\n    # Store posts in Neo4j\n    print(\"\\nStoring posts...\")\n    for post in posts:\n        post.adapt_to(\n            obj_key=\"neo4j\",\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Post\",\n            merge_on=\"id\"\n        )\n\n    # Create relationships\n    create_relationships(users, posts)\n\n    print(\"Database populated with sample data\")\n\n# Function to get a user's feed (posts from users they follow)\ndef get_user_feed(user_id):\n    \"\"\"Get posts from users that this user follows\"\"\"\n    driver = get_driver()\n\n    feed_posts = []\n\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (u:User {id: $user_id})-[:FOLLOWS]-&gt;(friend:User)-[:POSTED]-&gt;(p:Post)\n            RETURN p, friend.username AS author\n            ORDER BY p.created_at DESC\n            LIMIT 10\n            \"\"\",\n            user_id=user_id\n        )\n\n        for record in result:\n            post_data = dict(record[\"p\"].items())\n            post = Post(**post_data)\n            author = record[\"author\"]\n            feed_posts.append((post, author))\n\n    driver.close()\n    return feed_posts\n\n# Function to get recommended users to follow\ndef get_follow_recommendations(user_id):\n    \"\"\"Recommend users to follow based on mutual connections\"\"\"\n    driver = get_driver()\n\n    recommended_users = []\n\n    with driver.session() as session:\n        # Find users who are followed by people the user follows,\n        # but the user doesn't follow yet\n        result = session.run(\n            \"\"\"\n            MATCH (user:User {id: $user_id})-[:FOLLOWS]-&gt;(mutual:User)-[:FOLLOWS]-&gt;(recommended:User)\n            WHERE NOT (user)-[:FOLLOWS]-&gt;(recommended)\n            AND user.id &lt;&gt; recommended.id\n            WITH recommended, count(mutual) AS mutualCount\n            ORDER BY mutualCount DESC\n            LIMIT 5\n            RETURN recommended\n            \"\"\",\n            user_id=user_id\n        )\n\n        for record in result:\n            user_data = dict(record[\"recommended\"].items())\n            user = User(**user_data)\n            recommended_users.append(user)\n\n    driver.close()\n    return recommended_users\n\n# Function to get popular posts\ndef get_popular_posts():\n    \"\"\"Get posts with the most likes\"\"\"\n    driver = get_driver()\n\n    popular_posts = []\n\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (p:Post)\n            WITH p, p.likes AS likes\n            ORDER BY likes DESC\n            LIMIT 5\n            MATCH (author:User)-[:POSTED]-&gt;(p)\n            RETURN p, author.username AS author\n            \"\"\"\n        )\n\n        for record in result:\n            post_data = dict(record[\"p\"].items())\n            post = Post(**post_data)\n            author = record[\"author\"]\n            popular_posts.append((post, author))\n\n    driver.close()\n    return popular_posts\n\n# Main function to demo the social network\ndef main():\n    # Initialize and populate the database\n    initialize_database()\n    populate_database()\n\n    # Get user feed for Alice\n    print(\"\\nAlice's feed (posts from people she follows):\")\n    feed = get_user_feed(\"u1\")\n    for post, author in feed:\n        print(f\"@{author}: {post.content}\")\n        print(f\"  Likes: {post.likes} | Posted: {post.created_at}\")\n\n    # Get recommended users for Bob to follow\n    print(\"\\nRecommended users for Bob to follow:\")\n    recommendations = get_follow_recommendations(\"u2\")\n    for user in recommendations:\n        print(f\"  - {user.full_name} (@{user.username}) from {user.location}\")\n\n    # Get popular posts\n    print(\"\\nPopular posts across the network:\")\n    popular = get_popular_posts()\n    for i, (post, author) in enumerate(popular):\n        print(f\"{i+1}. @{author}: {post.content}\")\n        print(f\"   Likes: {post.likes}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"neo4j_adapter/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you've learned how to use pydapter's Neo4j adapter to seamlessly work with graph databases. We've covered:</p> <ol> <li>Basic setup and connection to Neo4j</li> <li>Modeling entities as Pydantic models</li> <li>Storing and retrieving data using the Neo4j adapter</li> <li>Creating and traversing relationships</li> <li>Building more complex graph applications</li> <li>Error handling and best practices</li> </ol> <p>Neo4j's graph structure is particularly powerful for data with complex relationships, like social networks, recommendation systems, and knowledge graphs. The pydapter adapter makes it easy to integrate Neo4j with your Pydantic-based Python applications, providing a clean interface for graph database operations.</p> <p>Some key advantages of using pydapter's Neo4j adapter include:</p> <ol> <li>Type safety and validation through Pydantic models</li> <li>Consistent error handling</li> <li>Simplified node creation and retrieval</li> <li>Integration with other pydapter adapters for multi-database applications</li> </ol> <p>Keep in mind that while the adapter handles nodes well, for relationship operations you'll often need to use the Neo4j driver directly for more complex graph traversals and Cypher queries.</p> <p>To learn more about Neo4j and graph modeling, check out the Neo4j documentation and Cypher query language.</p>"},{"location":"postgres_adapter/","title":"PostgreSQL Adapter Tutorial for Pydapter","text":"<p>This tutorial will show you how to use the PostgreSQL adapters in pydapter to seamlessly convert between Pydantic models and PostgreSQL databases. We'll cover both synchronous and asynchronous adapters.</p>"},{"location":"postgres_adapter/#prerequisites","title":"Prerequisites","text":""},{"location":"postgres_adapter/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code># Create a virtual environment if you haven't already\npython -m venv pydapter-demo\nsource pydapter-demo/bin/activate  # On Windows: pydapter-demo\\Scripts\\activate\n\n# Install dependencies\npip install pydantic sqlalchemy psycopg  # For synchronous adapter\npip install asyncpg  # For asynchronous adapter\n\n# Install pydapter (if you haven't done so already)\n# Either from PyPI when available:\n# pip install pydapter\n# Or from the repository:\ngit clone https://github.com/ohdearquant/pydapter.git\ncd pydapter\npip install -e .\n</code></pre>"},{"location":"postgres_adapter/#2-set-up-postgresql","title":"2. Set Up PostgreSQL","text":"<p>Make sure you have PostgreSQL installed and running. You can use a local installation or a Docker container:</p> <pre><code># Using Docker to run PostgreSQL\ndocker run --name pydapter-postgres -e POSTGRES_PASSWORD=password \\\n  -e POSTGRES_USER=pydapter -e POSTGRES_DB=pydapter_demo \\\n  -p 5432:5432 -d postgres:14\n\n# Alternatively, install PostgreSQL locally and create a database\n# createuser -s pydapter\n# createdb -O pydapter pydapter_demo\n</code></pre>"},{"location":"postgres_adapter/#3-create-a-test-table","title":"3. Create a Test Table","text":"<p>Connect to your PostgreSQL instance and create a test table:</p> <pre><code>CREATE TABLE IF NOT EXISTS users (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    email VARCHAR(100) UNIQUE NOT NULL,\n    active BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Optional: Add some test data\nINSERT INTO users (name, email) VALUES\n    ('Alice', 'alice@example.com'),\n    ('Bob', 'bob@example.com'),\n    ('Charlie', 'charlie@example.com');\n</code></pre>"},{"location":"postgres_adapter/#synchronous-postgresql-adapter","title":"Synchronous PostgreSQL Adapter","text":"<p>Let's start with the synchronous PostgreSQL adapter:</p> <pre><code>from pydantic import BaseModel\nfrom datetime import datetime\nfrom typing import Optional, List\nfrom pydapter.extras.postgres_ import PostgresAdapter\n\n# Define a Pydantic model that maps to our database table\nclass User(BaseModel):\n    id: Optional[int] = None\n    name: str\n    email: str\n    active: bool = True\n    created_at: Optional[datetime] = None\n\n# Connection details\ndb_config = {\n    \"engine_url\": \"postgresql+psycopg://pydapter:password@localhost/pydapter_demo\"\n    # You can also use:\n    # \"engine_url\": \"postgresql://pydapter:password@localhost/pydapter_demo\"\n    # Pydapter will convert it to the correct format\n}\n\n# Read data from the database\ndef read_users():\n    # Query all users\n    users = PostgresAdapter.from_obj(\n        User,\n        {\n            **db_config,\n            \"table\": \"users\",\n            \"selectors\": {}  # Empty selectors means \"select all\"\n        },\n        many=True  # Return a list of users\n    )\n\n    print(f\"Found {len(users)} users:\")\n    for user in users:\n        print(f\"  - {user.name} ({user.email}): Active={user.active}\")\n\n    return users\n\n# Query a specific user\ndef get_user_by_email(email):\n    try:\n        user = PostgresAdapter.from_obj(\n            User,\n            {\n                **db_config,\n                \"table\": \"users\",\n                \"selectors\": {\"email\": email}\n            },\n            many=False  # Return a single user\n        )\n        print(f\"Found user: {user.name} ({user.email})\")\n        return user\n    except Exception as e:\n        print(f\"Error finding user: {e}\")\n        return None\n\n# Create a new user\ndef create_user(name, email):\n    user = User(name=name, email=email)\n\n    result = PostgresAdapter.to_obj(\n        user,\n        **db_config,\n        table=\"users\",\n        many=False\n    )\n\n    print(f\"Created user: {result}\")\n    return user\n\n# Main function to demo the adapter\ndef main():\n    print(\"Reading all users:\")\n    users = read_users()\n\n    print(\"\\nFinding user by email:\")\n    alice = get_user_by_email(\"alice@example.com\")\n\n    print(\"\\nCreating a new user:\")\n    new_user = create_user(\"Dave\", \"dave@example.com\")\n\n    print(\"\\nVerifying new user was added:\")\n    read_users()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"postgres_adapter/#asynchronous-postgresql-adapter","title":"Asynchronous PostgreSQL Adapter","text":"<p>Now let's use the asynchronous version with asyncpg:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom datetime import datetime\nfrom typing import Optional, List\nfrom pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\n# Define a Pydantic model that maps to our database table\nclass User(BaseModel):\n    id: Optional[int] = None\n    name: str\n    email: str\n    active: bool = True\n    created_at: Optional[datetime] = None\n\n# Connection details\ndb_config = {\n    \"engine_url\": \"postgresql+asyncpg://pydapter:password@localhost/pydapter_demo\"\n    # You can also use:\n    # \"engine_url\": \"postgresql://pydapter:password@localhost/pydapter_demo\"\n    # Pydapter will convert it to the correct format with asyncpg\n}\n\n# Read data from the database asynchronously\nasync def read_users():\n    # Query all users\n    users = await AsyncPostgresAdapter.from_obj(\n        User,\n        {\n            **db_config,\n            \"table\": \"users\",\n            \"selectors\": {}  # Empty selectors means \"select all\"\n        },\n        many=True  # Return a list of users\n    )\n\n    print(f\"Found {len(users)} users:\")\n    for user in users:\n        print(f\"  - {user.name} ({user.email}): Active={user.active}\")\n\n    return users\n\n# Query a specific user asynchronously\nasync def get_user_by_email(email):\n    try:\n        user = await AsyncPostgresAdapter.from_obj(\n            User,\n            {\n                **db_config,\n                \"table\": \"users\",\n                \"selectors\": {\"email\": email}\n            },\n            many=False  # Return a single user\n        )\n        print(f\"Found user: {user.name} ({user.email})\")\n        return user\n    except Exception as e:\n        print(f\"Error finding user: {e}\")\n        return None\n\n# Create a new user asynchronously\nasync def create_user(name, email):\n    user = User(name=name, email=email)\n\n    result = await AsyncPostgresAdapter.to_obj(\n        user,\n        **db_config,\n        table=\"users\",\n        many=False\n    )\n\n    print(f\"Created user: {result}\")\n    return user\n\n# Main function to demo the async adapter\nasync def main():\n    print(\"Reading all users:\")\n    users = await read_users()\n\n    print(\"\\nFinding user by email:\")\n    alice = await get_user_by_email(\"alice@example.com\")\n\n    print(\"\\nCreating a new user:\")\n    new_user = await create_user(\"Eve\", \"eve@example.com\")\n\n    print(\"\\nVerifying new user was added:\")\n    await read_users()\n\n# Run the async main function\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"postgres_adapter/#advanced-usage-using-adaptable-mixin","title":"Advanced Usage: Using Adaptable Mixin","text":"<p>For a more ergonomic API, you can use the <code>AsyncAdaptable</code> mixin:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom datetime import datetime\nfrom typing import Optional, List\nfrom pydapter.async_core import AsyncAdaptable\nfrom pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\n# Define a model with the AsyncAdaptable mixin\nclass User(BaseModel, AsyncAdaptable):\n    id: Optional[int] = None\n    name: str\n    email: str\n    active: bool = True\n    created_at: Optional[datetime] = None\n\n# Register the async PostgreSQL adapter\nUser.register_async_adapter(AsyncPostgresAdapter)\n\n# Main async function\nasync def main():\n    # Connection configuration\n    db_config = {\n        \"engine_url\": \"postgresql+asyncpg://pydapter:password@localhost/pydapter_demo\",\n        \"table\": \"users\",\n        \"selectors\": {}\n    }\n\n    # Read users using the mixin methods\n    users = await User.adapt_from_async(db_config, obj_key=\"async_pg\", many=True)\n\n    print(f\"Found {len(users)} users:\")\n    for user in users:\n        print(f\"  - {user.name} ({user.email})\")\n\n    # Create a new user\n    new_user = User(name=\"Frank\", email=\"frank@example.com\")\n\n    # Save to database\n    result = await new_user.adapt_to_async(\n        obj_key=\"async_pg\",\n        engine_url=\"postgresql+asyncpg://pydapter:password@localhost/pydapter_demo\",\n        table=\"users\"\n    )\n\n    print(f\"\\nCreated new user: {result}\")\n\n    # Verify the user was added\n    updated_users = await User.adapt_from_async(db_config, obj_key=\"async_pg\", many=True)\n    print(f\"\\nUpdated user count: {len(updated_users)}\")\n\n# Run the async main function\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"postgres_adapter/#error-handling","title":"Error Handling","text":"<p>Let's demonstrate proper error handling for common PostgreSQL errors:</p> <pre><code>from pydapter.exceptions import ConnectionError, QueryError, ResourceError\nfrom pydapter.extras.postgres_ import PostgresAdapter\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    id: Optional[int] = None\n    name: str\n    email: str\n\ndef handle_postgres_errors():\n    # 1. Connection error - wrong password\n    try:\n        PostgresAdapter.from_obj(\n            User,\n            {\n                \"engine_url\": \"postgresql://pydapter:wrong_password@localhost/pydapter_demo\",\n                \"table\": \"users\"\n            }\n        )\n    except ConnectionError as e:\n        print(f\"Authentication error handled: {e}\")\n\n    # 2. Connection error - wrong host\n    try:\n        PostgresAdapter.from_obj(\n            User,\n            {\n                \"engine_url\": \"postgresql://pydapter:password@nonexistent_host/pydapter_demo\",\n                \"table\": \"users\"\n            }\n        )\n    except ConnectionError as e:\n        print(f\"Host connection error handled: {e}\")\n\n    # 3. Resource error - table doesn't exist\n    try:\n        PostgresAdapter.from_obj(\n            User,\n            {\n                \"engine_url\": \"postgresql://pydapter:password@localhost/pydapter_demo\",\n                \"table\": \"nonexistent_table\"\n            }\n        )\n    except ResourceError as e:\n        print(f\"Table resource error handled: {e}\")\n\n    # 4. Query error - SQL syntax error\n    try:\n        # This would normally be handled internally, but for demonstration\n        # you might encounter this when using raw SQL\n        pass\n    except QueryError as e:\n        print(f\"Query error handled: {e}\")\n\n# Run the error handling examples\nhandle_postgres_errors()\n</code></pre>"},{"location":"postgres_adapter/#practical-example-a-task-management-system","title":"Practical Example: A Task Management System","text":"<p>Let's create a more complete example of a task management system:</p> <pre><code>import asyncio\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom pydapter.async_core import AsyncAdaptable\nfrom pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\n# Database setup - Run this SQL in your PostgreSQL database first\n\"\"\"\nCREATE TABLE IF NOT EXISTS projects (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE IF NOT EXISTS tasks (\n    id SERIAL PRIMARY KEY,\n    project_id INTEGER REFERENCES projects(id),\n    title VARCHAR(200) NOT NULL,\n    description TEXT,\n    status VARCHAR(20) DEFAULT 'pending',\n    due_date TIMESTAMP,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\"\"\"\n\n# Define models with AsyncAdaptable\nclass Project(BaseModel, AsyncAdaptable):\n    id: Optional[int] = None\n    name: str\n    description: Optional[str] = None\n    created_at: Optional[datetime] = None\n\nclass Task(BaseModel, AsyncAdaptable):\n    id: Optional[int] = None\n    project_id: int\n    title: str\n    description: Optional[str] = None\n    status: str = \"pending\"\n    due_date: Optional[datetime] = None\n    created_at: Optional[datetime] = None\n\n# Register adapters\nProject.register_async_adapter(AsyncPostgresAdapter)\nTask.register_async_adapter(AsyncPostgresAdapter)\n\n# Database configuration\nDB_CONFIG = {\n    \"engine_url\": \"postgresql+asyncpg://pydapter:password@localhost/pydapter_demo\"\n}\n\n# Task management system class\nclass TaskManager:\n    def __init__(self, db_config):\n        self.db_config = db_config\n\n    async def create_project(self, name, description=None):\n        project = Project(name=name, description=description)\n        result = await project.adapt_to_async(\n            obj_key=\"async_pg\",\n            **self.db_config,\n            table=\"projects\"\n        )\n\n        # Get the new project with its ID\n        projects = await Project.adapt_from_async(\n            {\n                **self.db_config,\n                \"table\": \"projects\",\n                \"selectors\": {\"name\": name}\n            },\n            obj_key=\"async_pg\",\n            many=True\n        )\n\n        if projects:\n            return projects[0]\n        return None\n\n    async def get_projects(self):\n        return await Project.adapt_from_async(\n            {\n                **self.db_config,\n                \"table\": \"projects\"\n            },\n            obj_key=\"async_pg\",\n            many=True\n        )\n\n    async def create_task(self, project_id, title, description=None, due_date=None):\n        task = Task(\n            project_id=project_id,\n            title=title,\n            description=description,\n            due_date=due_date\n        )\n\n        result = await task.adapt_to_async(\n            obj_key=\"async_pg\",\n            **self.db_config,\n            table=\"tasks\"\n        )\n\n        return task\n\n    async def get_tasks_for_project(self, project_id):\n        return await Task.adapt_from_async(\n            {\n                **self.db_config,\n                \"table\": \"tasks\",\n                \"selectors\": {\"project_id\": project_id}\n            },\n            obj_key=\"async_pg\",\n            many=True\n        )\n\n    async def update_task_status(self, task_id, new_status):\n        # First, get the task\n        task = await Task.adapt_from_async(\n            {\n                **self.db_config,\n                \"table\": \"tasks\",\n                \"selectors\": {\"id\": task_id}\n            },\n            obj_key=\"async_pg\",\n            many=False\n        )\n\n        # Update the status\n        task.status = new_status\n\n        # Save back to database\n        result = await task.adapt_to_async(\n            obj_key=\"async_pg\",\n            **self.db_config,\n            table=\"tasks\"\n        )\n\n        return task\n\n# Main function to demo the task manager\nasync def main():\n    manager = TaskManager(DB_CONFIG)\n\n    # Create a new project\n    print(\"Creating a new project...\")\n    project = await manager.create_project(\n        \"Website Redesign\",\n        \"Redesign the company website with modern UI/UX\"\n    )\n    print(f\"Project created: {project.id} - {project.name}\")\n\n    # Add tasks to the project\n    print(\"\\nAdding tasks to the project...\")\n    tasks = [\n        await manager.create_task(\n            project.id,\n            \"Design mockups\",\n            \"Create initial design mockups for homepage\",\n            datetime.now().replace(day=datetime.now().day + 7)\n        ),\n        await manager.create_task(\n            project.id,\n            \"Frontend implementation\",\n            \"Implement the design in React\",\n            datetime.now().replace(day=datetime.now().day + 14)\n        ),\n        await manager.create_task(\n            project.id,\n            \"Backend API\",\n            \"Implement the required API endpoints\",\n            datetime.now().replace(day=datetime.now().day + 10)\n        )\n    ]\n\n    # Get all projects\n    print(\"\\nListing all projects:\")\n    projects = await manager.get_projects()\n    for proj in projects:\n        print(f\"  - {proj.id}: {proj.name}\")\n\n        # Get tasks for this project\n        proj_tasks = await manager.get_tasks_for_project(proj.id)\n        for task in proj_tasks:\n            print(f\"      - {task.title} [{task.status}] \" +\n                  (f\"(Due: {task.due_date.strftime('%Y-%m-%d')})\" if task.due_date else \"\"))\n\n    # Update a task status\n    print(\"\\nUpdating task status...\")\n    updated_task = await manager.update_task_status(tasks[0].id, \"in_progress\")\n    print(f\"Updated task: {updated_task.title} - Status: {updated_task.status}\")\n\n    # Final task list\n    print(\"\\nFinal task list:\")\n    final_tasks = await manager.get_tasks_for_project(project.id)\n    for task in final_tasks:\n        print(f\"  - {task.title} [{task.status}]\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"postgres_adapter/#handling-advanced-postgresql-features","title":"Handling Advanced PostgreSQL Features","text":"<p>PostgreSQL has many advanced features like JSON/JSONB fields, arrays, and full-text search. Here's how to work with some of these using pydapter:</p> <pre><code>import asyncio\nimport json\nfrom typing import List, Dict, Any, Optional\nfrom pydantic import BaseModel, Field\nfrom pydapter.async_core import AsyncAdaptable\nfrom pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\n# Database setup - Run this SQL in your PostgreSQL database first\n\"\"\"\nCREATE TABLE IF NOT EXISTS products (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    price NUMERIC(10, 2) NOT NULL,\n    categories TEXT[] DEFAULT '{}',\n    metadata JSONB DEFAULT '{}'\n);\n\"\"\"\n\n# Define our model with PostgreSQL-specific types\nclass Product(BaseModel, AsyncAdaptable):\n    id: Optional[int] = None\n    name: str\n    description: Optional[str] = None\n    price: float\n    categories: List[str] = []\n    metadata: Dict[str, Any] = {}\n\n# Register adapter\nProduct.register_async_adapter(AsyncPostgresAdapter)\n\n# Database config\nDB_CONFIG = {\n    \"engine_url\": \"postgresql+asyncpg://pydapter:password@localhost/pydapter_demo\"\n}\n\nasync def demo_advanced_postgres_features():\n    # Create products with array and JSON data\n    products = [\n        Product(\n            name=\"Smartphone\",\n            description=\"Latest model with advanced features\",\n            price=999.99,\n            categories=[\"electronics\", \"mobile\", \"gadgets\"],\n            metadata={\n                \"brand\": \"TechX\",\n                \"model\": \"TX-2000\",\n                \"specs\": {\n                    \"cpu\": \"Octa-core\",\n                    \"ram\": \"8GB\",\n                    \"storage\": \"256GB\"\n                }\n            }\n        ),\n        Product(\n            name=\"Coffee Maker\",\n            description=\"Premium coffee machine for home or office\",\n            price=199.99,\n            categories=[\"appliances\", \"kitchen\", \"coffee\"],\n            metadata={\n                \"brand\": \"BrewMaster\",\n                \"features\": [\"programmable\", \"thermal carafe\", \"auto-clean\"],\n                \"dimensions\": {\n                    \"width\": 30,\n                    \"height\": 40,\n                    \"depth\": 20\n                }\n            }\n        )\n    ]\n\n    # Save products to database\n    print(\"Saving products with arrays and JSON data...\")\n    for product in products:\n        await product.adapt_to_async(\n            obj_key=\"async_pg\",\n            **DB_CONFIG,\n            table=\"products\"\n        )\n\n    # Query all products\n    print(\"\\nRetrieving products from database:\")\n    db_products = await Product.adapt_from_async(\n        {\n            **DB_CONFIG,\n            \"table\": \"products\"\n        },\n        obj_key=\"async_pg\",\n        many=True\n    )\n\n    # Display products with their array and JSON data\n    for product in db_products:\n        print(f\"\\n{product.name} - ${product.price}\")\n        print(f\"  Description: {product.description}\")\n        print(f\"  Categories: {', '.join(product.categories)}\")\n        print(f\"  Metadata: {json.dumps(product.metadata, indent=2)}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(demo_advanced_postgres_features())\n</code></pre>"},{"location":"postgres_adapter/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you learned how to use pydapter's PostgreSQL adapters to seamlessly work with both synchronous and asynchronous database operations. These adapters provide a clean interface for converting between Pydantic models and PostgreSQL database records, with specialized error handling for PostgreSQL-specific issues.</p> <p>The asynchronous adapter is particularly useful for high-performance applications where you want to avoid blocking I/O operations. By using the <code>AsyncAdaptable</code> mixin, you can create a more ergonomic API that makes your code cleaner and more maintainable.</p> <p>The key advantages of using pydapter's PostgreSQL adapters include:</p> <ol> <li>Automatic validation through Pydantic models</li> <li>Consistent error handling</li> <li>Support for both synchronous and asynchronous operations</li> <li>Easy conversion between models and database records</li> <li>Support for PostgreSQL-specific data types like arrays and JSONB</li> </ol> <p>Try experimenting with these adapters in your own projects to see how they can simplify your database interactions!</p>"},{"location":"protocols/","title":"Protocols Module","text":"<p>The Protocols module provides a set of standardized interfaces that can be used to add common capabilities to your models. These protocols follow a clean inheritance hierarchy and are designed to be composable, allowing you to mix and match capabilities as needed.</p>"},{"location":"protocols/#installation","title":"Installation","text":"<p>The Protocols module is available as an optional dependency. To use it, install pydapter with the <code>protocols</code> extra:</p> <pre><code>pip install pydapter[protocols]\n</code></pre> <p>This will install the required dependencies, including <code>typing-extensions</code>.</p>"},{"location":"protocols/#available-protocols","title":"Available Protocols","text":"<p>The Protocols module provides the following interfaces:</p> <ul> <li>Identifiable: Unique identification using UUID</li> <li>Temporal: Creation and update timestamps</li> <li>Embeddable: Vector embeddings for ML applications</li> <li>Invokable: Function execution with state tracking</li> <li>Event: Comprehensive event tracking (combines multiple protocols)</li> <li>Auditable: User tracking and versioning for audit trails</li> <li>SoftDeletable: Soft deletion with restore capabilities</li> <li>Cryptographical: Content hashing capabilities</li> </ul>"},{"location":"protocols/#identifiable","title":"Identifiable","text":"<p>The <code>Identifiable</code> protocol provides a unique identifier for objects. It's the foundation of the protocol hierarchy.</p> <p>Key features:</p> <ul> <li>Automatic UUID generation</li> <li>String serialization of UUIDs</li> <li>UUID validation</li> <li>Hash implementation for use in sets and dictionaries</li> </ul> <pre><code>from pydapter.protocols import Identifiable\n\nclass User(Identifiable):\n    name: str\n    email: str\n\n# UUID is automatically generated\nuser = User(name=\"John Doe\", email=\"john@example.com\")\nprint(f\"User ID: {user.id}\")  # User ID: 3f7c8e9a-1d2b-4c3d-8e7f-5a6b7c8d9e0f\n</code></pre>"},{"location":"protocols/#temporal","title":"Temporal","text":"<p>The <code>Temporal</code> protocol adds creation and update timestamps to objects.</p> <p>Key features:</p> <ul> <li>Automatic creation timestamp</li> <li>Automatic update timestamp</li> <li>Method to manually update the timestamp</li> <li>ISO-8601 serialization of timestamps</li> </ul> <pre><code>from pydapter.protocols import Identifiable, Temporal\n\nclass User(Identifiable, Temporal):\n    name: str\n    email: str\n\n# Timestamps are automatically set\nuser = User(name=\"John Doe\", email=\"john@example.com\")\nprint(f\"Created at: {user.created_at}\")  # Created at: 2025-05-16T15:30:00+00:00\nprint(f\"Updated at: {user.updated_at}\")  # Updated at: 2025-05-16T15:30:00+00:00\n\n# Update the timestamp manually\nuser.name = \"Jane Doe\"\nuser.update_timestamp()\nprint(f\"Updated at: {user.updated_at}\")  # Updated at: 2025-05-16T15:31:00+00:00\n</code></pre>"},{"location":"protocols/#embedable","title":"Embedable","text":"<p>The <code>Embedable</code> protocol adds support for vector embeddings, which are commonly used in machine learning and natural language processing applications.</p> <p>Key features:</p> <ul> <li>Storage for embedding vectors</li> <li>Content field for the text to be embedded</li> <li>Dimension calculation</li> <li>Support for various embedding formats (list, JSON string)</li> </ul> <pre><code>from pydapter.protocols import Identifiable, Temporal, Embedable\n\nclass Document(Identifiable, Temporal, Embedable):\n    title: str\n\n# Create a document with an embedding\ndocument = Document(\n    title=\"Sample Document\",\n    content=\"This is a sample document for embedding.\",\n    embedding=[0.1, 0.2, 0.3, 0.4]\n)\n\nprint(f\"Embedding dimensions: {document.n_dim}\")  # Embedding dimensions: 4\n\n# Embeddings can also be provided as a JSON string\ndocument2 = Document(\n    title=\"Another Document\",\n    content=\"This is another sample document.\",\n    embedding=\"[0.5, 0.6, 0.7, 0.8]\"\n)\n</code></pre>"},{"location":"protocols/#invokable","title":"Invokable","text":"<p>The <code>Invokable</code> protocol adds function invocation capabilities with execution tracking.</p> <p>Key features:</p> <ul> <li>Execution status tracking</li> <li>Duration measurement</li> <li>Error handling</li> <li>Response storage</li> </ul> <pre><code>import asyncio\nfrom pydapter.protocols import Identifiable, Temporal, Invokable\n\nclass APICall(Identifiable, Temporal, Invokable):\n    endpoint: str\n\n    async def fetch_data(self):\n        # Simulate API call\n        await asyncio.sleep(1)\n        return {\"data\": \"Sample response\"}\n\n# Create an API call\napi_call = APICall(endpoint=\"/api/data\")\napi_call._invoke_function = api_call.fetch_data\n\n# Execute the call\nawait api_call.invoke()\n\nprint(f\"Status: {api_call.execution.status}\")  # Status: completed\nprint(f\"Duration: {api_call.execution.duration:.2f}s\")  # Duration: 1.00s\nprint(f\"Response: {api_call.execution.response}\")  # Response: {'data': 'Sample response'}\n</code></pre>"},{"location":"protocols/#event","title":"Event","text":"<p>The <code>Event</code> protocol combines the capabilities of <code>Identifiable</code>, <code>Temporal</code>, <code>Embedable</code>, and <code>Invokable</code> to provide a comprehensive event tracking interface.</p> <pre><code>from pydapter.protocols import Event\n\nclass LogEvent(Event):\n    event_type: str\n\n    async def process(self):\n        # Process the event\n        return {\"processed\": True}\n\n# Create an event\nlog_event = LogEvent(\n    event_type=\"system_log\",\n    content=\"User logged in\",\n)\nlog_event._invoke_function = log_event.process\n\n# Execute the event\nawait log_event.invoke()\n\nprint(f\"Event ID: {log_event.id}\")\nprint(f\"Created at: {log_event.created_at}\")\nprint(f\"Status: {log_event.execution.status}\")\n</code></pre>"},{"location":"protocols/#auditable","title":"Auditable","text":"<p>The <code>Auditable</code> protocol adds user tracking and versioning capabilities for audit trails.</p> <p>Key features:</p> <ul> <li>Tracks who created the entity (<code>created_by</code>)</li> <li>Tracks who last updated the entity (<code>updated_by</code>)</li> <li>Version number for optimistic locking</li> <li>Integration with temporal updates</li> </ul> <pre><code>from pydapter.protocols import Identifiable, Temporal\nfrom pydapter.protocols.auditable import AuditableMixin\n\nclass AuditedDocument(Identifiable, Temporal, AuditableMixin):\n    title: str\n    content: str\n    created_by: str | None = None\n    updated_by: str | None = None\n    version: int = 1\n\n# Create and audit a document\ndoc = AuditedDocument(\n    title=\"Confidential Report\",\n    content=\"Sensitive information\",\n    created_by=\"admin\"\n)\n\n# Update with audit trail\ndoc.content = \"Updated content\"\ndoc.mark_updated_by(\"editor123\")\nprint(f\"Version: {doc.version}\")      # Version: 2\nprint(f\"Updated by: {doc.updated_by}\") # Updated by: editor123\n</code></pre>"},{"location":"protocols/#softdeletable","title":"SoftDeletable","text":"<p>The <code>SoftDeletable</code> protocol provides soft deletion capabilities, allowing entities to be marked as deleted without permanently removing them.</p> <p>Key features:</p> <ul> <li>Marks deletion with timestamp (<code>deleted_at</code>)</li> <li>Boolean flag for deletion status (<code>is_deleted</code>)</li> <li>Restore capability for recovering deleted entities</li> <li>Preserves data for audit and recovery purposes</li> </ul> <pre><code>from pydapter.protocols import Identifiable\nfrom pydapter.protocols.soft_deletable import SoftDeletableMixin\n\nclass SoftDeletableUser(Identifiable, SoftDeletableMixin):\n    name: str\n    email: str\n    deleted_at: datetime | None = None\n    is_deleted: bool = False\n\n# Create and soft delete a user\nuser = SoftDeletableUser(name=\"John Doe\", email=\"john@example.com\")\n\n# Soft delete the user\nuser.soft_delete()\nprint(f\"Deleted: {user.is_deleted}\")    # Deleted: True\nprint(f\"Deleted at: {user.deleted_at}\") # Deleted at: 2023-10-01T12:00:00+00:00\n\n# Restore the user\nuser.restore()\nprint(f\"Deleted: {user.is_deleted}\")    # Deleted: False\nprint(f\"Deleted at: {user.deleted_at}\") # Deleted at: None\n</code></pre>"},{"location":"protocols/#protocol-registry","title":"Protocol Registry","text":"<p>The protocol registry system allows for dynamic registration and discovery of protocol mixins.</p> <p>Key features:</p> <ul> <li>Dynamic mixin registration</li> <li>Extensible protocol system</li> <li>Runtime protocol discovery</li> <li>Custom protocol support</li> </ul> <pre><code>from pydapter.protocols.registry import register_mixin, get_mixin_registry\n\n# Define a custom protocol mixin\nclass GeospatialMixin:\n    def set_coordinates(self, latitude: float, longitude: float):\n        self.latitude = latitude\n        self.longitude = longitude\n\n# Register the custom mixin\nregister_mixin(\"geospatial\", GeospatialMixin)\n\n# View all registered protocols\nregistry = get_mixin_registry()\nprint(list(registry.keys()))\n# Output: ['identifiable', 'temporal', 'embeddable', 'invokable', 'cryptographical', 'auditable', 'soft_deletable', 'geospatial']\n</code></pre>"},{"location":"protocols/#protocol-inheritance-hierarchy","title":"Protocol Inheritance Hierarchy","text":"<p>The protocols follow a hierarchical structure:</p> <pre><code>Identifiable\n    \u2502\n    \u251c\u2500\u2500 Temporal\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u2500 Embedable\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u2500 Invokable\n    \u2502               \u2502\n    \u2502               \u2514\u2500\u2500 Event\n    \u2502\n    \u2514\u2500\u2500 Other custom protocols...\n</code></pre> <p>This design allows you to compose protocols as needed, inheriting only the capabilities required for your specific use case.</p>"},{"location":"protocols/#best-practices","title":"Best Practices","text":""},{"location":"protocols/#composing-protocols","title":"Composing Protocols","text":"<p>When using multiple protocols, inherit them in the correct order to ensure proper initialization:</p> <pre><code># Correct order\nclass MyModel(Identifiable, Temporal, Embedable):\n    pass\n\n# Incorrect order - may cause initialization issues\nclass MyModel(Embedable, Temporal, Identifiable):\n    pass\n</code></pre>"},{"location":"protocols/#custom-content-creation","title":"Custom Content Creation","text":"<p>The <code>Embedable</code> protocol allows you to customize how content is created by overriding the <code>create_content</code> method:</p> <pre><code>class Document(Identifiable, Temporal, Embedable):\n    title: str\n    body: str\n\n    def create_content(self):\n        return f\"{self.title}\\n\\n{self.body}\"\n</code></pre>"},{"location":"protocols/#custom-invocation-functions","title":"Custom Invocation Functions","text":"<p>When using the <code>Invokable</code> protocol, you need to set the <code>_invoke_function</code> attribute to the function you want to invoke:</p> <pre><code>async def fetch_data(endpoint):\n    # Fetch data from endpoint\n    return {\"data\": \"Sample response\"}\n\napi_call = APICall(endpoint=\"/api/data\")\napi_call._invoke_function = fetch_data\napi_call._invoke_args = [api_call.endpoint]  # Arguments to pass to the function\n</code></pre>"},{"location":"protocols/#type-checking","title":"Type Checking","text":"<p>The protocols module is designed to work well with static type checkers like mypy. The protocols are defined using <code>typing_extensions.Protocol</code> and are marked as <code>runtime_checkable</code>, allowing for both static and runtime type checking.</p> <pre><code>from typing import List\nfrom pydapter.protocols import Identifiable\n\ndef process_identifiables(items: List[Identifiable]):\n    for item in items:\n        print(f\"Processing item {item.id}\")\n\n# This will pass type checking\nprocess_identifiables([User(name=\"John\"), Document(title=\"Sample\")])\n</code></pre>"},{"location":"protocols/#error-handling","title":"Error Handling","text":"<p>If you try to import protocols without the required dependencies, you'll get a clear error message:</p> <pre><code>ImportError: The 'protocols' feature requires the 'typing_extensions' package. Install it with: pip install pydapter[protocols]\n</code></pre> <p>This helps guide users to install the correct dependencies.</p>"},{"location":"protocols/#advanced-usage","title":"Advanced Usage","text":""},{"location":"protocols/#custom-protocol-extensions","title":"Custom Protocol Extensions","text":"<p>You can create your own protocols by extending the existing ones:</p> <pre><code>from pydapter.protocols import Identifiable, Temporal\nfrom pydantic import Field\n\nclass Versionable(Temporal):\n    \"\"\"Protocol for objects that support versioning.\"\"\"\n\n    version: int = Field(default=1)\n\n    def increment_version(self):\n        \"\"\"Increment the version and update the timestamp.\"\"\"\n        self.version += 1\n        self.update_timestamp()\n\nclass Document(Identifiable, Temporal, Versionable):\n    title: str\n    content: str\n</code></pre>"},{"location":"protocols/#integration-with-adapters","title":"Integration with Adapters","text":"<p>The protocols can be used with pydapter adapters to provide standardized interfaces for data access:</p> <pre><code>from pydapter.core import Adapter\nfrom pydapter.protocols import Identifiable, Temporal\n\nclass User(Identifiable, Temporal):\n    name: str\n    email: str\n\n# Create an adapter for a list of users\nusers = [\n    User(name=\"John Doe\", email=\"john@example.com\"),\n    User(name=\"Jane Doe\", email=\"jane@example.com\")\n]\nadapter = Adapter(users)\n\n# Query by ID\nuser = adapter.get(id=\"3f7c8e9a-1d2b-4c3d-8e7f-5a6b7c8d9e0f\")\n</code></pre>"},{"location":"qdrant_adapter/","title":"Vector Database Tutorial with Pydapter's Qdrant Adapters","text":"<p>This tutorial demonstrates how to use pydapter's Qdrant adapters to seamlessly work with vector embeddings for semantic search and similarity-based retrieval. We'll cover both synchronous and asynchronous implementations.</p>"},{"location":"qdrant_adapter/#introduction-to-vector-databases","title":"Introduction to Vector Databases","text":"<p>Vector databases are specialized storage systems designed for high-dimensional vector data (embeddings) that enable efficient similarity search. They're crucial for:</p> <ul> <li>Semantic search</li> <li>Recommendation systems</li> <li>Image similarity</li> <li>Document retrieval</li> <li>Natural language understanding</li> </ul> <p>Qdrant is a powerful vector database with extensive filtering capabilities, making it perfect for applications that need both semantic similarity and metadata filtering.</p>"},{"location":"qdrant_adapter/#prerequisites","title":"Prerequisites","text":""},{"location":"qdrant_adapter/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code># Create a virtual environment if you haven't already\npython -m venv pydapter-demo\nsource pydapter-demo/bin/activate  # On Windows: pydapter-demo\\Scripts\\activate\n\n# Install dependencies\npip install pydantic qdrant-client sentence-transformers numpy\n\n# Install pydapter (if you haven't done so already)\n# Either from PyPI when available:\n# pip install pydapter\n# Or from the repository:\ngit clone https://github.com/ohdearquant/pydapter.git\ncd pydapter\npip install -e .\n</code></pre>"},{"location":"qdrant_adapter/#2-set-up-qdrant","title":"2. Set Up Qdrant","text":"<p>You can run Qdrant locally using Docker:</p> <pre><code>docker run -p 6333:6333 -p 6334:6334 -v $(pwd)/qdrant_data:/qdrant/storage qdrant/qdrant\n</code></pre> <p>For testing, you can also use the in-memory mode without Docker.</p>"},{"location":"qdrant_adapter/#basic-example-synchronous-qdrant-adapter","title":"Basic Example: Synchronous Qdrant Adapter","text":"<p>Let's start by creating a document search system using the synchronous adapter:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.extras.qdrant_ import QdrantAdapter\n\n# Load a sentence transformer model to generate embeddings\nmodel = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dimensional embeddings\n\n# Define our document model with vector embeddings\nclass Document(BaseModel):\n    id: str\n    title: str\n    content: str\n    tags: List[str] = []\n    embedding: List[float] = []  # Vector embedding\n\n    def generate_embedding(self):\n        \"\"\"Generate embedding from the document content\"\"\"\n        self.embedding = model.encode(self.content).tolist()\n        return self\n\n# Create sample documents\nsample_docs = [\n    Document(\n        id=\"doc1\",\n        title=\"Introduction to Machine Learning\",\n        content=\"Machine learning is a field of artificial intelligence that uses \"\n                \"statistical techniques to give computer systems the ability to learn from data.\",\n        tags=[\"ML\", \"AI\", \"Data Science\"]\n    ),\n    Document(\n        id=\"doc2\",\n        title=\"Deep Learning Fundamentals\",\n        content=\"Deep learning is a subset of machine learning that uses neural networks \"\n                \"with many layers to analyze various factors of data.\",\n        tags=[\"Deep Learning\", \"Neural Networks\", \"AI\"]\n    ),\n    Document(\n        id=\"doc3\",\n        title=\"Natural Language Processing\",\n        content=\"NLP combines computational linguistics and AI to enable computers to \"\n                \"understand, interpret, and generate human language.\",\n        tags=[\"NLP\", \"AI\", \"Linguistics\"]\n    ),\n    Document(\n        id=\"doc4\",\n        title=\"Computer Vision\",\n        content=\"Computer vision is a field of AI that trains computers to interpret and \"\n                \"understand visual data from the world around us.\",\n        tags=[\"Computer Vision\", \"AI\", \"Image Processing\"]\n    ),\n]\n\n# Generate embeddings for each document\nfor doc in sample_docs:\n    doc.generate_embedding()\n\n# Store documents in Qdrant\ndef store_documents(documents):\n    print(f\"Storing {len(documents)} documents in Qdrant...\")\n\n    # Store in Qdrant using the QdrantAdapter\n    result = QdrantAdapter.to_obj(\n        documents,\n        collection=\"documents\",  # Collection name\n        url=None,  # Use in-memory storage for this example\n        many=True\n    )\n\n    print(f\"Storage result: {result}\")\n\n# Search for similar documents\ndef search_documents(query_text, top_k=2):\n    print(f\"Searching for documents similar to: '{query_text}'\")\n\n    # Generate embedding for the query\n    query_embedding = model.encode(query_text).tolist()\n\n    # Search in Qdrant using the QdrantAdapter\n    results = QdrantAdapter.from_obj(\n        Document,\n        {\n            \"collection\": \"documents\",\n            \"query_vector\": query_embedding,\n            \"top_k\": top_k,\n            \"url\": None  # Use in-memory storage\n        },\n        many=True\n    )\n\n    print(f\"Found {len(results)} similar documents:\")\n    for i, doc in enumerate(results):\n        print(f\"{i+1}. {doc.title}\")\n        print(f\"   Content: {doc.content}\")\n        print(f\"   Tags: {', '.join(doc.tags)}\")\n        print()\n\n    return results\n\n# Main function to demo the adapter\ndef main():\n    # Store documents\n    store_documents(sample_docs)\n\n    # Perform searches\n    search_documents(\"What is machine learning?\")\n    search_documents(\"How do computers understand language?\")\n    search_documents(\"How do computers process images?\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"qdrant_adapter/#asynchronous-qdrant-adapter","title":"Asynchronous Qdrant Adapter","text":"<p>Now let's implement the same functionality using the asynchronous adapter:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n# Load the sentence transformer model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Define our document model\nclass Document(BaseModel):\n    id: str\n    title: str\n    content: str\n    tags: List[str] = []\n    embedding: List[float] = []\n\n    def generate_embedding(self):\n        self.embedding = model.encode(self.content).tolist()\n        return self\n\n# Create sample documents (same as the synchronous example)\nsample_docs = [\n    Document(\n        id=\"doc1\",\n        title=\"Introduction to Machine Learning\",\n        content=\"Machine learning is a field of artificial intelligence that uses \"\n                \"statistical techniques to give computer systems the ability to learn from data.\",\n        tags=[\"ML\", \"AI\", \"Data Science\"]\n    ),\n    Document(\n        id=\"doc2\",\n        title=\"Deep Learning Fundamentals\",\n        content=\"Deep learning is a subset of machine learning that uses neural networks \"\n                \"with many layers to analyze various factors of data.\",\n        tags=[\"Deep Learning\", \"Neural Networks\", \"AI\"]\n    ),\n    Document(\n        id=\"doc3\",\n        title=\"Natural Language Processing\",\n        content=\"NLP combines computational linguistics and AI to enable computers to \"\n                \"understand, interpret, and generate human language.\",\n        tags=[\"NLP\", \"AI\", \"Linguistics\"]\n    ),\n    Document(\n        id=\"doc4\",\n        title=\"Computer Vision\",\n        content=\"Computer vision is a field of AI that trains computers to interpret and \"\n                \"understand visual data from the world around us.\",\n        tags=[\"Computer Vision\", \"AI\", \"Image Processing\"]\n    ),\n]\n\n# Generate embeddings for each document\nfor doc in sample_docs:\n    doc.generate_embedding()\n\n# Store documents in Qdrant asynchronously\nasync def store_documents(documents):\n    print(f\"Storing {len(documents)} documents in Qdrant...\")\n\n    # Store in Qdrant using the AsyncQdrantAdapter\n    result = await AsyncQdrantAdapter.to_obj(\n        documents,\n        collection=\"documents\",\n        url=None,  # Use in-memory storage\n        many=True\n    )\n\n    print(f\"Storage result: {result}\")\n\n# Search for similar documents asynchronously\nasync def search_documents(query_text, top_k=2):\n    print(f\"Searching for documents similar to: '{query_text}'\")\n\n    # Generate embedding for the query\n    query_embedding = model.encode(query_text).tolist()\n\n    # Search in Qdrant using the AsyncQdrantAdapter\n    results = await AsyncQdrantAdapter.from_obj(\n        Document,\n        {\n            \"collection\": \"documents\",\n            \"query_vector\": query_embedding,\n            \"top_k\": top_k,\n            \"url\": None\n        },\n        many=True\n    )\n\n    print(f\"Found {len(results)} similar documents:\")\n    for i, doc in enumerate(results):\n        print(f\"{i+1}. {doc.title}\")\n        print(f\"   Content: {doc.content}\")\n        print(f\"   Tags: {', '.join(doc.tags)}\")\n        print()\n\n    return results\n\n# Main async function\nasync def main():\n    # Store documents\n    await store_documents(sample_docs)\n\n    # Perform searches\n    await search_documents(\"What is machine learning?\")\n    await search_documents(\"How do computers understand language?\")\n    await search_documents(\"How do computers process images?\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"qdrant_adapter/#advanced-example-semantic-product-search-with-filtering","title":"Advanced Example: Semantic Product Search with Filtering","text":"<p>Let's build a more practical example - a product search system that combines semantic similarity with metadata filtering:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional, Any\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.async_core import AsyncAdaptable\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n# Load the sentence transformer model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Define our product model with the AsyncAdaptable mixin\nclass Product(BaseModel, AsyncAdaptable):\n    id: str\n    name: str\n    description: str\n    price: float\n    category: str\n    brand: str\n    tags: List[str] = []\n    embedding: List[float] = []\n\n    def generate_embedding(self):\n        # Combine name and description for better semantic search\n        text = f\"{self.name}. {self.description}\"\n        self.embedding = model.encode(text).tolist()\n        return self\n\n# Register the async Qdrant adapter\nProduct.register_async_adapter(AsyncQdrantAdapter)\n\n# Sample products\nsample_products = [\n    Product(\n        id=\"p1\",\n        name=\"Premium Wireless Headphones\",\n        description=\"Noise-cancelling wireless headphones with 30-hour battery life and \"\n                   \"premium sound quality.\",\n        price=299.99,\n        category=\"Electronics\",\n        brand=\"SoundMaster\",\n        tags=[\"wireless\", \"noise-cancelling\", \"premium\", \"headphones\"]\n    ),\n    Product(\n        id=\"p2\",\n        name=\"Ultra-Slim Laptop\",\n        description=\"Lightweight laptop with 15-inch display, 16GB RAM, and 512GB SSD. \"\n                   \"Perfect for productivity on the go.\",\n        price=1299.99,\n        category=\"Electronics\",\n        brand=\"TechPro\",\n        tags=[\"laptop\", \"lightweight\", \"powerful\", \"portable\"]\n    ),\n    Product(\n        id=\"p3\",\n        name=\"Smart Fitness Watch\",\n        description=\"Track your fitness goals with this advanced smartwatch featuring \"\n                   \"heart rate monitoring, GPS, and sleep tracking.\",\n        price=199.99,\n        category=\"Wearables\",\n        brand=\"FitTech\",\n        tags=[\"fitness\", \"smartwatch\", \"health\", \"tracking\"]\n    ),\n    Product(\n        id=\"p4\",\n        name=\"Wireless Earbuds\",\n        description=\"Compact wireless earbuds with crystal clear sound, water resistance, \"\n                   \"and 24-hour battery life with the charging case.\",\n        price=129.99,\n        category=\"Electronics\",\n        brand=\"SoundMaster\",\n        tags=[\"wireless\", \"earbuds\", \"compact\", \"waterproof\"]\n    ),\n    Product(\n        id=\"p5\",\n        name=\"Professional DSLR Camera\",\n        description=\"High-end DSLR camera with 24MP sensor, 4K video recording, and \"\n                   \"professional-grade image quality.\",\n        price=1499.99,\n        category=\"Photography\",\n        brand=\"OptixPro\",\n        tags=[\"camera\", \"professional\", \"DSLR\", \"high-quality\"]\n    ),\n]\n\n# Generate embeddings for all products\nfor product in sample_products:\n    product.generate_embedding()\n\n# Product search system\nclass ProductSearchSystem:\n    def __init__(self, collection_name=\"products\", url=None):\n        self.collection_name = collection_name\n        self.url = url\n\n    async def initialize(self, products):\n        \"\"\"Initialize the search system with products\"\"\"\n        print(f\"Initializing product search system with {len(products)} products...\")\n\n        # Store products in Qdrant\n        results = []\n        for product in products:\n            result = await product.adapt_to_async(\n                obj_key=\"async_qdrant\",\n                collection=self.collection_name,\n                url=self.url\n            )\n            results.append(result)\n\n        print(\"Product search system initialized successfully\")\n        return results\n\n    async def search(self, query_text, filters=None, top_k=3):\n        \"\"\"Search for products by semantic similarity with optional filtering\"\"\"\n        print(f\"Searching for products similar to: '{query_text}'\")\n        if filters:\n            filter_desc = \", \".join(f\"{k}={v}\" for k, v in filters.items())\n            print(f\"With filters: {filter_desc}\")\n\n        # Generate embedding for the query\n        query_embedding = model.encode(query_text).tolist()\n\n        # We'll do the filtering in Python since pydapter doesn't directly expose Qdrant's filtering\n        # In a real implementation, you could extend the adapter to support Qdrant's filtering\n\n        # First, search by vector similarity\n        results = await Product.adapt_from_async(\n            {\n                \"collection\": self.collection_name,\n                \"query_vector\": query_embedding,\n                \"top_k\": top_k * 3 if filters else top_k,  # Get more results for filtering\n                \"url\": self.url\n            },\n            obj_key=\"async_qdrant\",\n            many=True\n        )\n\n        # Apply filters if specified\n        if filters:\n            filtered_results = []\n            for product in results:\n                match = True\n                for key, value in filters.items():\n                    if hasattr(product, key):\n                        if isinstance(value, list):\n                            # For list values (e.g., checking if a tag is in tags)\n                            if isinstance(getattr(product, key), list):\n                                if not any(v in getattr(product, key) for v in value):\n                                    match = False\n                                    break\n                        else:\n                            # For exact value matching\n                            if getattr(product, key) != value:\n                                match = False\n                                break\n                if match:\n                    filtered_results.append(product)\n\n            # Limit to top_k after filtering\n            results = filtered_results[:top_k]\n\n        print(f\"Found {len(results)} matching products:\")\n        for i, product in enumerate(results):\n            print(f\"{i+1}. {product.name} - ${product.price}\")\n            print(f\"   Brand: {product.brand}, Category: {product.category}\")\n            print(f\"   Description: {product.description}\")\n            print(f\"   Tags: {', '.join(product.tags)}\")\n            print()\n\n        return results\n\n# Main function to demo the advanced search system\nasync def main():\n    # Create and initialize the search system\n    search_system = ProductSearchSystem()\n    await search_system.initialize(sample_products)\n\n    # Perform various searches\n    print(\"\\n--- Basic Semantic Search ---\")\n    await search_system.search(\"wireless audio devices\")\n\n    print(\"\\n--- Search with Brand Filter ---\")\n    await search_system.search(\"wireless audio\", filters={\"brand\": \"SoundMaster\"})\n\n    print(\"\\n--- Search with Price Filter ---\")\n    await search_system.search(\"portable computing device\", filters={\"category\": \"Electronics\"})\n\n    print(\"\\n--- Search with Tag Filter ---\")\n    await search_system.search(\"advanced technology\", filters={\"tags\": [\"professional\", \"powerful\"]})\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"qdrant_adapter/#working-with-a-persistent-qdrant-instance","title":"Working with a Persistent Qdrant Instance","text":"<p>For production use, you'll want to connect to a persistent Qdrant instance rather than using in-memory storage. Here's how to do it:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom typing import List\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n# Define model and transformer as before\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nclass Document(BaseModel):\n    id: str\n    title: str\n    content: str\n    embedding: List[float] = []\n\n    def generate_embedding(self):\n        self.embedding = model.encode(self.content).tolist()\n        return self\n\nasync def demo_persistent_qdrant():\n    # Connect to Qdrant running in Docker\n    qdrant_url = \"http://localhost:6333\"\n\n    # Create a test document\n    doc = Document(\n        id=\"test1\",\n        title=\"Test Document\",\n        content=\"This is a test document to verify connection to a persistent Qdrant instance.\"\n    ).generate_embedding()\n\n    try:\n        # Store the document\n        print(\"Storing document in persistent Qdrant...\")\n        result = await AsyncQdrantAdapter.to_obj(\n            doc,\n            collection=\"test_collection\",\n            url=qdrant_url\n        )\n        print(f\"Storage result: {result}\")\n\n        # Search for the document\n        print(\"\\nRetrieving document from persistent Qdrant...\")\n        query_embedding = model.encode(\"test document verify\").tolist()\n        results = await AsyncQdrantAdapter.from_obj(\n            Document,\n            {\n                \"collection\": \"test_collection\",\n                \"query_vector\": query_embedding,\n                \"url\": qdrant_url\n            },\n            many=True\n        )\n\n        print(f\"Retrieved {len(results)} documents:\")\n        for doc in results:\n            print(f\"  - {doc.title}: {doc.content}\")\n\n    except Exception as e:\n        print(f\"Error connecting to Qdrant: {e}\")\n        print(\"Make sure Qdrant is running on localhost:6333\")\n\nif __name__ == \"__main__\":\n    asyncio.run(demo_persistent_qdrant())\n</code></pre>"},{"location":"qdrant_adapter/#error-handling-for-vector-operations","title":"Error Handling for Vector Operations","text":"<p>Let's demonstrate proper error handling for common Qdrant operations:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom typing import List\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.exceptions import ConnectionError, QueryError, ResourceError, ValidationError\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n# Define a simple model\nclass ImageVector(BaseModel):\n    id: str\n    embedding: List[float]\n\nasync def handle_vector_errors():\n    print(\"Testing error handling for vector operations...\")\n\n    # 1. Validation error - empty vector\n    try:\n        invalid_vector = ImageVector(id=\"test1\", embedding=[])\n        await AsyncQdrantAdapter.to_obj(\n            invalid_vector,\n            collection=\"test_collection\",\n            url=None\n        )\n    except ValidationError as e:\n        print(f\"Vector validation error handled: {e}\")\n\n    # 2. Validation error - inconsistent vector dimensions\n    try:\n        # First create a collection with 5D vectors\n        valid_vector = ImageVector(id=\"test2\", embedding=[0.1, 0.2, 0.3, 0.4, 0.5])\n        await AsyncQdrantAdapter.to_obj(\n            valid_vector,\n            collection=\"dimension_test\",\n            url=None\n        )\n\n        # Then try to add a 3D vector to the same collection\n        invalid_vector = ImageVector(id=\"test3\", embedding=[0.1, 0.2, 0.3])\n        await AsyncQdrantAdapter.to_obj(\n            invalid_vector,\n            collection=\"dimension_test\",\n            url=None\n        )\n    except ValidationError as e:\n        print(f\"Vector dimension mismatch handled: {e}\")\n\n    # 3. Connection error - wrong URL\n    try:\n        vector = ImageVector(id=\"test4\", embedding=[0.1, 0.2, 0.3, 0.4, 0.5])\n        await AsyncQdrantAdapter.to_obj(\n            vector,\n            collection=\"test_collection\",\n            url=\"http://nonexistent-qdrant-host:6333\"\n        )\n    except ConnectionError as e:\n        print(f\"Connection error handled: {e}\")\n\n    # 4. Resource error - collection doesn't exist\n    try:\n        await AsyncQdrantAdapter.from_obj(\n            ImageVector,\n            {\n                \"collection\": \"nonexistent_collection\",\n                \"query_vector\": [0.1, 0.2, 0.3, 0.4, 0.5],\n                \"url\": None\n            }\n        )\n    except ResourceError as e:\n        print(f\"Resource error handled: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(handle_vector_errors())\n</code></pre>"},{"location":"qdrant_adapter/#advanced-topics-working-with-high-dimensional-vectors","title":"Advanced Topics: Working with High-Dimensional Vectors","text":"<p>For production applications, you'll often work with higher-dimensional vectors. Here's how to use pydapter with larger models:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n# Load a larger model with higher dimensions\n# ada-002 generates 1536-dimensional vectors, better for semantic similarity\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')  # 768 dimensions\n\nclass Document(BaseModel):\n    id: str\n    title: str\n    content: str\n    embedding: List[float] = []\n\n    def generate_embedding(self):\n        self.embedding = model.encode(self.content).tolist()\n        return self\n\nasync def demo_high_dim_vectors():\n    print(\"Demonstrating high-dimensional vector operations...\")\n\n    # Create a document with high-dim embedding\n    doc = Document(\n        id=\"highdim1\",\n        title=\"High-Dimensional Vector Example\",\n        content=\"This document has a higher-dimensional embedding vector for better \"\n               \"semantic search accuracy.\"\n    ).generate_embedding()\n\n    print(f\"Generated embedding with {len(doc.embedding)} dimensions\")\n\n    # Store in Qdrant\n    print(\"Storing document...\")\n    result = await AsyncQdrantAdapter.to_obj(\n        doc,\n        collection=\"highdim_documents\",\n        url=None\n    )\n\n    # Search with a similar query\n    query_text = \"semantic search with high dimensions\"\n    print(f\"Searching with query: '{query_text}'\")\n\n    query_embedding = model.encode(query_text).tolist()\n    results = await AsyncQdrantAdapter.from_obj(\n        Document,\n        {\n            \"collection\": \"highdim_documents\",\n            \"query_vector\": query_embedding,\n            \"url\": None\n        },\n        many=True\n    )\n\n    print(f\"Found {len(results)} results:\")\n    for doc in results:\n        print(f\"  - {doc.title}: {doc.content}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(demo_high_dim_vectors())\n</code></pre>"},{"location":"qdrant_adapter/#real-world-application-document-search-engine","title":"Real-World Application: Document Search Engine","text":"<p>Let's build a more complete document search engine with pydapter and Qdrant:</p> <pre><code>import asyncio\nimport os\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional, Any\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.async_core import AsyncAdaptable\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n# Load the sentence transformer model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nclass Document(BaseModel, AsyncAdaptable):\n    id: str\n    title: str\n    content: str\n    author: Optional[str] = None\n    date: Optional[str] = None\n    source: Optional[str] = None\n    tags: List[str] = []\n    embedding: List[float] = []\n\n    def generate_embedding(self):\n        # Combine title and content for better semantic search\n        text = f\"{self.title}. {self.content}\"\n        self.embedding = model.encode(text).tolist()\n        return self\n\n# Register the async Qdrant adapter\nDocument.register_async_adapter(AsyncQdrantAdapter)\n\nclass DocumentSearchEngine:\n    def __init__(self, collection_name=\"documents\", url=None):\n        self.collection_name = collection_name\n        self.url = url\n\n    async def add_document(self, document):\n        \"\"\"Add a single document to the search engine\"\"\"\n        # Generate embedding if not already present\n        if not document.embedding:\n            document.generate_embedding()\n\n        # Store in Qdrant\n        result = await document.adapt_to_async(\n            obj_key=\"async_qdrant\",\n            collection=self.collection_name,\n            url=self.url\n        )\n\n        return result\n\n    async def add_documents(self, documents):\n        \"\"\"Add multiple documents to the search engine\"\"\"\n        results = []\n        for doc in documents:\n            result = await self.add_document(doc)\n            results.append(result)\n\n        return results\n\n    async def search(self, query_text, filters=None, top_k=5):\n        \"\"\"Search for documents similar to the query text with optional filters\"\"\"\n        # Generate embedding for the query\n        query_embedding = model.encode(query_text).tolist()\n\n        # Get raw results (we'll filter in Python)\n        raw_results = await Document.adapt_from_async(\n            {\n                \"collection\": self.collection_name,\n                \"query_vector\": query_embedding,\n                \"top_k\": top_k * 3 if filters else top_k,  # Get more results for filtering\n                \"url\": self.url\n            },\n            obj_key=\"async_qdrant\",\n            many=True\n        )\n\n        # Apply filters if any\n        if filters:\n            filtered_results = []\n            for doc in raw_results:\n                match = True\n                for key, value in filters.items():\n                    if hasattr(doc, key):\n                        if isinstance(value, list):\n                            # For tags or other list fields\n                            if isinstance(getattr(doc, key), list):\n                                if not any(v in getattr(doc, key) for v in value):\n                                    match = False\n                                    break\n                        elif key == \"date\" and isinstance(value, dict):\n                            # Special handling for date range filters\n                            doc_date = getattr(doc, key)\n                            if not doc_date:\n                                match = False\n                                break\n\n                            if \"from\" in value and doc_date &lt; value[\"from\"]:\n                                match = False\n                                break\n\n                            if \"to\" in value and doc_date &gt; value[\"to\"]:\n                                match = False\n                                break\n                        else:\n                            # For exact matching\n                            if getattr(doc, key) != value:\n                                match = False\n                                break\n\n                if match:\n                    filtered_results.append(doc)\n\n            # Limit to top_k after filtering\n            results = filtered_results[:top_k]\n        else:\n            results = raw_results[:top_k]\n\n        return results\n\n    async def get_document(self, doc_id):\n        \"\"\"Retrieve a specific document by ID\"\"\"\n        # In a real implementation, you would use Qdrant's point_id search\n        # For now, we'll use the vector search and filter in Python\n        try:\n            # Get a document with a similar ID (not ideal, but works for the example)\n            results = await Document.adapt_from_async(\n                {\n                    \"collection\": self.collection_name,\n                    \"query_vector\": model.encode(doc_id).tolist(),\n                    \"top_k\": 10,\n                    \"url\": self.url\n                },\n                obj_key=\"async_qdrant\",\n                many=True\n            )\n\n            # Find the exact ID match\n            for doc in results:\n                if doc.id == doc_id:\n                    return doc\n\n            return None\n        except Exception as e:\n            print(f\"Error retrieving document: {e}\")\n            return None\n\n    async def delete_document(self, doc_id):\n        \"\"\"Delete a document by ID\"\"\"\n        # This functionality would require extending the adapter\n        # to support Qdrant's delete_points method\n        raise NotImplementedError(\"Delete functionality not yet implemented\")\n\nasync def main():\n    # Create sample documents\n    documents = [\n        Document(\n            id=\"doc1\",\n            title=\"Introduction to Vector Databases\",\n            content=\"Vector databases store high-dimensional vectors and enable semantic \"\n                   \"search based on similarity rather than exact matching.\",\n            author=\"Jane Smith\",\n            date=\"2023-01-15\",\n            source=\"TechBlog\",\n            tags=[\"vector-database\", \"semantic-search\", \"embeddings\"]\n        ),\n        Document(\n            id=\"doc2\",\n            title=\"Machine Learning Fundamentals\",\n            content=\"Machine learning algorithms learn patterns from data without being \"\n                   \"explicitly programmed. They improve with experience.\",\n            author=\"John Doe\",\n            date=\"2023-02-20\",\n            source=\"AI Journal\",\n            tags=[\"machine-learning\", \"AI\", \"algorithms\"]\n        ),\n        Document(\n            id=\"doc3\",\n            title=\"Natural Language Processing Techniques\",\n            content=\"NLP enables computers to understand human language by processing, \"\n                   \"analyzing, and generating text data.\",\n            author=\"Jane Smith\",\n            date=\"2023-03-10\",\n            source=\"AI Journal\",\n            tags=[\"NLP\", \"text-processing\", \"AI\"]\n        ),\n        Document(\n            id=\"doc4\",\n            title=\"Semantic Search Implementation\",\n            content=\"Implementing semantic search requires converting text to vector \"\n                   \"embeddings and finding similar vectors efficiently.\",\n            author=\"Alex Johnson\",\n            date=\"2023-04-05\",\n            source=\"TechBlog\",\n            tags=[\"semantic-search\", \"embeddings\", \"implementation\"]\n        ),\n        Document(\n            id=\"doc5\",\n            title=\"Vector Database Comparison\",\n            content=\"Comparing popular vector databases like Qdrant, Pinecone, and Milvus \"\n                   \"for semantic search applications.\",\n            author=\"Chris Williams\",\n            date=\"2023-05-12\",\n            source=\"Database Review\",\n            tags=[\"vector-database\", \"comparison\", \"Qdrant\", \"Pinecone\", \"Milvus\"]\n        ),\n    ]\n\n    # Initialize the search engine\n    search_engine = DocumentSearchEngine()\n\n    # Add sample documents\n    print(\"Adding sample documents to the search engine...\")\n    await search_engine.add_documents(documents)\n\n    # Perform searches\n    print(\"\\n--- Basic Semantic Search ---\")\n    results = await search_engine.search(\"How do vector databases work?\")\n    print(f\"Found {len(results)} documents:\")\n    for i, doc in enumerate(results):\n        print(f\"{i+1}. {doc.title}\")\n        print(f\"   Author: {doc.author}, Date: {doc.date}\")\n        print(f\"   Content: {doc.content}\")\n        print()\n\n    print(\"\\n--- Search with Author Filter ---\")\n    author_results = await search_engine.search(\n        \"AI and machine learning techniques\",\n        filters={\"author\": \"Jane Smith\"}\n    )\n    print(f\"Found {len(author_results)} documents by Jane Smith:\")\n    for i, doc in enumerate(author_results):\n        print(f\"{i+1}. {doc.title}\")\n        print(f\"   Author: {doc.author}, Date: {doc.date}\")\n        print(f\"   Content: {doc.content}\")\n        print()\n\n    print(\"\\n--- Search with Tag Filter ---\")\n    tag_results = await search_engine.search(\n        \"database technology\",\n        filters={\"tags\": [\"vector-database\"]}\n    )\n    print(f\"Found {len(tag_results)} documents with 'vector-database' tag:\")\n    for i, doc in enumerate(tag_results):\n        print(f\"{i+1}. {doc.title}\")\n        print(f\"   Tags: {', '.join(doc.tags)}\")\n        print(f\"   Content: {doc.content}\")\n        print()\n\n    print(\"\\n--- Get Document by ID ---\")\n    doc = await search_engine.get_document(\"doc3\")\n    if doc:\n        print(f\"Retrieved document: {doc.title}\")\n        print(f\"Content: {doc.content}\")\n    else:\n        print(\"Document not found\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"qdrant_adapter/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you've learned how to use pydapter's Qdrant adapters to build semantic search applications with vector embeddings. We've covered:</p> <ol> <li>Setting up Qdrant and generating vector embeddings</li> <li>Using both synchronous and asynchronous adapters</li> <li>Storing Pydantic models with embeddings in Qdrant</li> <li>Performing similarity searches</li> <li>Implementing filtering and advanced search features</li> <li>Building complete search applications</li> </ol> <p>Vector databases like Qdrant are powerful tools for implementing semantic search, recommendation systems, and other AI applications that require similarity matching rather than exact keyword matching.</p> <p>The pydapter adapters make it easy to integrate vector database functionality into your Python applications, with a clean and consistent interface for working with Pydantic models.</p> <p>By combining pydapter's adapters with pre-trained embedding models like sentence-transformers, you can quickly build sophisticated semantic search systems with minimal code.</p>"},{"location":"sql_model_adapter/","title":"pydapter 0.1.4 Tutorial","text":"<p>Bridge Pydantic \u21c6 SQLAlchemy (with optional pgvector)</p>"},{"location":"sql_model_adapter/#1-installation","title":"1 Installation","text":"<pre><code># core features\npip install pydapter&gt;=0.1.4 sqlalchemy&gt;=2.0 alembic\n\n# add pgvector support and drivers\npip install pydapter[pgvector] psycopg[binary] pgvector\n</code></pre>"},{"location":"sql_model_adapter/#2-quick-start-scalar-models","title":"2 Quick-start (scalar models)","text":""},{"location":"sql_model_adapter/#21-define-your-validation-model","title":"2.1 Define your validation model","text":"<pre><code>from pydantic import BaseModel\n\nclass UserSchema(BaseModel):\n    id: int | None = None          # promoted to PK\n    name: str\n    email: str | None = None\n    active: bool = True\n</code></pre>"},{"location":"sql_model_adapter/#22-generate-the-orm-class","title":"2.2 Generate the ORM class","text":"<pre><code>from pydapter.model_adapters import SQLModelAdapter\n\nUserSQL = SQLModelAdapter.pydantic_model_to_sql(UserSchema)\n</code></pre> <p><code>UserSQL</code> is a fully-mapped SQLAlchemy declarative model\u2014Alembic will pick it up automatically.</p>"},{"location":"sql_model_adapter/#23-round-trip-back-to-pydantic-optional","title":"2.3 Round-trip back to Pydantic (optional)","text":"<pre><code>RoundTrip = SQLModelAdapter.sql_model_to_pydantic(UserSQL)\nuser_json = RoundTrip.model_validate(UserSQL(name=\"Ann\")).model_dump()\n</code></pre>"},{"location":"sql_model_adapter/#3-embeddings-with-pgvector","title":"3 Embeddings with <code>pgvector</code>","text":""},{"location":"sql_model_adapter/#31-validation-layer","title":"3.1 Validation layer","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass DocSchema(BaseModel):\n    id: int | None = None\n    text: str\n    embedding: list[float] = Field(..., vector_dim=768)\n</code></pre>"},{"location":"sql_model_adapter/#32-generate-vector-aware-model","title":"3.2 Generate vector-aware model","text":"<pre><code>from pydapter.model_adapters import SQLVectorModelAdapter\n\nDocSQL = SQLVectorModelAdapter.pydantic_model_to_sql(DocSchema)\n</code></pre> <p>Result:</p> <pre><code>Column('embedding', Vector(768), nullable=False)\n</code></pre>"},{"location":"sql_model_adapter/#33-reverse-conversion","title":"3.3 Reverse conversion","text":"<pre><code>DocSchemaRT = SQLVectorModelAdapter.sql_model_to_pydantic(DocSQL)\nassert DocSchemaRT.model_fields[\"embedding\"].json_schema_extra[\"vector_dim\"] == 768\n</code></pre>"},{"location":"sql_model_adapter/#4-alembic-integration","title":"4 Alembic integration","text":"<ol> <li>Add pgvector extension (first migration only)</li> </ol> <pre><code># env.py or an initial upgrade() block\nop.execute(\"CREATE EXTENSION IF NOT EXISTS pgvector\")\n</code></pre> <ol> <li>Autogenerate migrations</li> </ol> <pre><code>alembic revision --autogenerate -m \"init tables\"\n</code></pre> <p>All columns\u2014including <code>Vector(dim)</code>\u2014appear in the diff.</p>"},{"location":"sql_model_adapter/#5-advanced-options","title":"5 Advanced options","text":"Need How Custom table name <code>SQLModelAdapter.pydantic_model_to_sql(UserSchema, table_name=\"users\")</code> Alternate PK field <code>\u2026, pk_field=\"uuid\"</code> Cache generated classes Wrap the call in your own memoization layer; generation runs once per import. Unsupported types Extend <code>_PY_TO_SQL</code> / <code>_SQL_TO_PY</code> dictionaries or subclass the adapter."},{"location":"sql_model_adapter/#6-testing-ci","title":"6 Testing &amp; CI","text":"<p>Unit tests rely only on SQLAlchemy inspection\u2014no database spin-up.</p> <pre><code>pytest -q\n</code></pre> <p>To include vector tests:</p> <pre><code>pytest -q -m \"not pgvector\"          # skip\npytest -q                            # run all (pgvector installed)\n</code></pre>"},{"location":"sql_model_adapter/#7-troubleshooting","title":"7 Troubleshooting","text":"Symptom Fix <code>TypeError: Unsupported type \u2026</code> Add a mapping in the adapter or exclude the field. Alembic shows no changes Ensure generated classes share <code>metadata</code> or are imported in <code>env.py</code>. Vector dim missing Provide <code>vector_dim</code> in <code>json_schema_extra</code>, or accept flexible dimension."},{"location":"sql_model_adapter/#8-wrap-up","title":"8 Wrap-up","text":"<p>pydapter 0.1.4 lets you:</p> <ul> <li>Keep one source of truth\u2014your Pydantic models.</li> <li>Ship migrations without hand-writing ORM classes.</li> <li>Store embeddings directly in Postgres with pgvector.</li> </ul> <p>Update, generate, migrate\u2014done. Happy coding! \ud83d\ude80</p>"},{"location":"testing/","title":"Testing in pydapter","text":"<p>pydapter uses a comprehensive testing strategy to ensure reliability and correctness of all adapters. This document explains the testing approach and how to run tests.</p>"},{"location":"testing/#testing-strategy","title":"Testing Strategy","text":"<p>pydapter employs two main types of tests:</p> <ol> <li>Unit Tests - Test adapter functionality in isolation using mocks</li> <li>Integration Tests - Test adapters with real database systems using    TestContainers</li> </ol>"},{"location":"testing/#unit-tests","title":"Unit Tests","text":"<p>Unit tests are designed to test adapter functionality without requiring external dependencies. These tests use mocks to simulate database connections and responses, making them fast and reliable.</p> <p>Example unit test for a database adapter:</p> <pre><code>def test_postgres_adapter_to_obj(mocker):\n    # Mock SQLAlchemy engine and connection\n    mock_engine = mocker.patch(\"sqlalchemy.create_engine\")\n    mock_conn = mock_engine.return_value.begin.return_value.__enter__.return_value\n\n    # Create test model\n    test_model = TestModel(id=1, name=\"test\", value=42.0)\n\n    # Test adapter\n    PostgresAdapter.to_obj(test_model, engine_url=\"postgresql://test\", table=\"test_table\")\n\n    # Verify SQL execution was called with correct parameters\n    mock_conn.execute.assert_called_once()\n</code></pre>"},{"location":"testing/#integration-tests","title":"Integration Tests","text":"<p>Integration tests verify that adapters work correctly with actual database systems. These tests use TestContainers to spin up isolated database instances in Docker containers during test execution.</p>"},{"location":"testing/#supported-databases","title":"Supported Databases","text":"<p>pydapter includes integration tests for:</p> <ul> <li>PostgreSQL - SQL database adapter tests</li> <li>MongoDB - Document database adapter tests</li> <li>Neo4j - Graph database adapter tests</li> <li>Qdrant - Vector database adapter tests</li> </ul>"},{"location":"testing/#testcontainers-setup","title":"TestContainers Setup","text":"<p>Integration tests use pytest fixtures to create and manage database containers:</p> <pre><code>@pytest.fixture(scope=\"session\")\ndef pg_url():\n    \"\"\"PostgreSQL container fixture for tests.\"\"\"\n    from testcontainers.postgres import PostgresContainer\n\n    with PostgresContainer(\"postgres:16-alpine\") as pg:\n        url = pg.get_connection_url()\n        yield url\n</code></pre> <p>These fixtures handle container lifecycle, ensuring proper cleanup after tests complete.</p>"},{"location":"testing/#example-integration-test","title":"Example Integration Test","text":"<pre><code>def test_postgres_single_record(pg_url, sync_model_factory, postgres_table):\n    \"\"\"Test PostgreSQL adapter with a single record.\"\"\"\n    # Create test instance\n    test_model = sync_model_factory(id=42, name=\"test_postgres\", value=12.34)\n\n    # Register adapter\n    test_model.__class__.register_adapter(PostgresAdapter)\n\n    # Store in database\n    test_model.adapt_to(obj_key=\"postgres\", engine_url=pg_url, table=\"test_table\")\n\n    # Retrieve from database\n    retrieved = test_model.__class__.adapt_from(\n        {\"engine_url\": pg_url, \"table\": \"test_table\", \"selectors\": {\"id\": 42}},\n        obj_key=\"postgres\",\n        many=False,\n    )\n\n    # Verify data integrity\n    assert retrieved.id == test_model.id\n    assert retrieved.name == test_model.name\n    assert retrieved.value == test_model.value\n</code></pre>"},{"location":"testing/#running-tests","title":"Running Tests","text":""},{"location":"testing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Docker (for integration tests)</li> </ul>"},{"location":"testing/#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/ohdearquant/pydapter.git\ncd pydapter\n\n# Install development dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"testing/#running-all-tests","title":"Running All Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"testing/#running-specific-tests","title":"Running Specific Tests","text":"<pre><code># Run only unit tests\npytest tests/test_*.py -k \"not test_integration\"\n\n# Run only integration tests\npytest tests/test_integration_*.py\n\n# Run tests for a specific adapter\npytest tests/test_*postgres*.py\n</code></pre>"},{"location":"testing/#docker-availability-check","title":"Docker Availability Check","text":"<p>Integration tests automatically check for Docker availability and are skipped if Docker is not running:</p> <pre><code>def is_docker_available():\n    \"\"\"Check if Docker is available.\"\"\"\n    import subprocess\n    try:\n        subprocess.run([\"docker\", \"info\"], check=True, capture_output=True)\n        return True\n    except (subprocess.SubprocessError, FileNotFoundError):\n        return False\n\n# Skip tests if Docker is not available\npytestmark = pytest.mark.skipif(\n    not is_docker_available(), reason=\"Docker is not available\"\n)\n</code></pre>"},{"location":"testing/#writing-new-tests","title":"Writing New Tests","text":"<p>When contributing new adapters or features to pydapter, please include both unit tests and integration tests:</p> <ol> <li>Unit tests should test the adapter's functionality in isolation using    mocks</li> <li>Integration tests should verify the adapter works with a real database    instance</li> </ol>"},{"location":"testing/#integration-test-template","title":"Integration Test Template","text":"<pre><code>def test_new_adapter_integration(container_url, model_factory, cleanup_fixture):\n    \"\"\"Test new adapter with a real database.\"\"\"\n    # Create test instance\n    test_model = model_factory(id=1, name=\"test\", value=42.0)\n\n    # Register adapter\n    test_model.__class__.register_adapter(NewAdapter)\n\n    # Store in database\n    test_model.adapt_to(obj_key=\"new_adapter\", url=container_url, ...)\n\n    # Retrieve from database\n    retrieved = test_model.__class__.adapt_from(\n        {\"url\": container_url, ...},\n        obj_key=\"new_adapter\",\n        many=False,\n    )\n\n    # Verify data integrity\n    assert retrieved.id == test_model.id\n    assert retrieved.name == test_model.name\n    assert retrieved.value == test_model.value\n</code></pre>"},{"location":"testing/#test-coverage","title":"Test Coverage","text":"<p>pydapter aims to maintain high test coverage. You can generate a coverage report with:</p> <pre><code>pytest --cov=pydapter\n</code></pre> <p>For a detailed HTML report:</p> <pre><code>pytest --cov=pydapter --cov-report=html\n</code></pre> <p>This will create a <code>htmlcov</code> directory with the coverage report.</p>"},{"location":"api/adapters/","title":"Adapters API","text":"<p>This page documents the built-in adapters provided by pydapter.</p>"},{"location":"api/adapters/#csv-adapter","title":"CSV Adapter","text":""},{"location":"api/adapters/#pydapter.adapters.csv_","title":"<code>pydapter.adapters.csv_</code>","text":"<p>CSV Adapter for Pydantic Models.</p> <p>This module provides the CsvAdapter class for converting between Pydantic models and CSV data formats. It supports reading from CSV files or strings and writing Pydantic models to CSV format.</p>"},{"location":"api/adapters/#pydapter.adapters.csv_-classes","title":"Classes","text":""},{"location":"api/adapters/#pydapter.adapters.csv_.CsvAdapter","title":"<code>CsvAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Adapter for converting between Pydantic models and CSV data.</p> <p>This adapter handles CSV files and strings, providing methods to: - Parse CSV data into Pydantic model instances - Convert Pydantic models to CSV format - Handle various CSV dialects and formatting options</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"csv\")</p> <code>DEFAULT_CSV_KWARGS</code> <p>Default CSV parsing parameters</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.adapters.csv_ import CsvAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Parse CSV data\ncsv_data = \"name,age\\nJohn,30\\nJane,25\"\npeople = CsvAdapter.from_obj(Person, csv_data, many=True)\n\n# Convert to CSV\ncsv_output = CsvAdapter.to_obj(people, many=True)\n</code></pre> Source code in <code>src/pydapter/adapters/csv_.py</code> <pre><code>class CsvAdapter(Adapter[T]):\n    \"\"\"\n    Adapter for converting between Pydantic models and CSV data.\n\n    This adapter handles CSV files and strings, providing methods to:\n    - Parse CSV data into Pydantic model instances\n    - Convert Pydantic models to CSV format\n    - Handle various CSV dialects and formatting options\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"csv\")\n        DEFAULT_CSV_KWARGS: Default CSV parsing parameters\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.adapters.csv_ import CsvAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Parse CSV data\n        csv_data = \"name,age\\\\nJohn,30\\\\nJane,25\"\n        people = CsvAdapter.from_obj(Person, csv_data, many=True)\n\n        # Convert to CSV\n        csv_output = CsvAdapter.to_obj(people, many=True)\n        ```\n    \"\"\"\n\n    obj_key = \"csv\"\n\n    # Default CSV dialect settings\n    DEFAULT_CSV_KWARGS = {\n        \"escapechar\": \"\\\\\",\n        \"quotechar\": '\"',\n        \"delimiter\": \",\",\n        \"quoting\": csv.QUOTE_MINIMAL,\n    }\n\n    # ---------------- incoming\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: str | Path,\n        /,\n        *,\n        many: bool = True,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Handle file path or string content\n            if isinstance(obj, Path):\n                try:\n                    text = Path(obj).read_text()\n                except Exception as e:\n                    raise ParseError(f\"Failed to read CSV file: {e}\", source=str(obj))\n            else:\n                text = obj\n\n            # Sanitize text to remove NULL bytes\n            text = text.replace(\"\\0\", \"\")\n\n            if not text.strip():\n                raise ParseError(\n                    \"Empty CSV content\",\n                    source=str(obj)[:100] if isinstance(obj, str) else str(obj),\n                )\n\n            # Merge default CSV kwargs with user-provided kwargs\n            csv_kwargs = cls.DEFAULT_CSV_KWARGS.copy()\n            csv_kwargs.update(kw)  # User-provided kwargs override defaults\n\n            # Parse CSV\n            try:\n                # Extract specific parameters from csv_kwargs\n                delimiter = \",\"\n                quotechar = '\"'\n                escapechar = \"\\\\\"\n                quoting = csv.QUOTE_MINIMAL\n\n                if \"delimiter\" in csv_kwargs:\n                    delimiter = str(csv_kwargs.pop(\"delimiter\"))\n                if \"quotechar\" in csv_kwargs:\n                    quotechar = str(csv_kwargs.pop(\"quotechar\"))\n                if \"escapechar\" in csv_kwargs:\n                    escapechar = str(csv_kwargs.pop(\"escapechar\"))\n                if \"quoting\" in csv_kwargs:\n                    quoting_value = csv_kwargs.pop(\"quoting\")\n                    if isinstance(quoting_value, int):\n                        quoting = quoting_value\n                    else:\n                        quoting = csv.QUOTE_MINIMAL\n\n                reader = csv.DictReader(\n                    io.StringIO(text),\n                    delimiter=delimiter,\n                    quotechar=quotechar,\n                    escapechar=escapechar,\n                    quoting=quoting,\n                )\n                rows = list(reader)\n\n                if not rows:\n                    return [] if many else None\n\n                # Check for missing fieldnames\n                if not reader.fieldnames:\n                    raise ParseError(\"CSV has no headers\", source=text[:100])\n\n                # Check for missing required fields in the model\n                model_fields = subj_cls.model_fields\n                required_fields = [\n                    field for field, info in model_fields.items() if info.is_required()\n                ]\n\n                missing_fields = [\n                    field for field in required_fields if field not in reader.fieldnames\n                ]\n\n                if missing_fields:\n                    raise ParseError(\n                        f\"CSV missing required fields: {', '.join(missing_fields)}\",\n                        source=text[:100],\n                        fields=missing_fields,\n                    )\n\n                # Convert rows to model instances\n                result = []\n                for i, row in enumerate(rows):\n                    try:\n                        result.append(\n                            getattr(subj_cls, adapt_meth)(row, **(adapt_kw or {}))\n                        )\n                    except ValidationError as e:\n                        raise AdapterValidationError(\n                            f\"Validation error in row {i + 1}: {e}\",\n                            data=row,\n                            row=i + 1,\n                            errors=e.errors(),\n                        )\n\n                # If there's only one row and many=False, return a single object\n                if len(result) == 1 and not many:\n                    return result[0]\n                # Otherwise, return a list of objects\n                return result\n\n            except csv.Error as e:\n                raise ParseError(f\"CSV parsing error: {e}\", source=text[:100])\n\n        except (ParseError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ParseError(\n                f\"Unexpected error parsing CSV: {e}\",\n                source=str(obj)[:100] if isinstance(obj, str) else str(obj),\n            )\n\n    # ---------------- outgoing\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | list[T],\n        /,\n        *,\n        many: bool = False,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ) -&gt; str:\n        try:\n            items = subj if isinstance(subj, list) else [subj]\n\n            if not items:\n                return \"\"\n\n            buf = io.StringIO()\n\n            # Sanitize any string values to remove NULL bytes\n            sanitized_items = []\n            for item in items:\n                item_dict = getattr(item, adapt_meth)(**(adapt_kw or {}))\n                for key, value in item_dict.items():\n                    if isinstance(value, str):\n                        item_dict[key] = value.replace(\"\\0\", \"\")\n                sanitized_items.append(item_dict)\n\n            # Merge default CSV kwargs with user-provided kwargs\n            csv_kwargs = cls.DEFAULT_CSV_KWARGS.copy()\n            csv_kwargs.update(kw)  # User-provided kwargs override defaults\n\n            # Get fieldnames from the first item\n            fieldnames = list(getattr(items[0], adapt_meth)(**(adapt_kw or {})).keys())\n\n            # Extract specific parameters from csv_kwargs\n            delimiter = \",\"\n            quotechar = '\"'\n            escapechar = \"\\\\\"\n            quoting = csv.QUOTE_MINIMAL\n\n            if \"delimiter\" in csv_kwargs:\n                delimiter = str(csv_kwargs.pop(\"delimiter\"))\n            if \"quotechar\" in csv_kwargs:\n                quotechar = str(csv_kwargs.pop(\"quotechar\"))\n            if \"escapechar\" in csv_kwargs:\n                escapechar = str(csv_kwargs.pop(\"escapechar\"))\n            if \"quoting\" in csv_kwargs:\n                quoting_value = csv_kwargs.pop(\"quoting\")\n                if isinstance(quoting_value, int):\n                    quoting = quoting_value\n                else:\n                    quoting = csv.QUOTE_MINIMAL\n\n            writer = csv.DictWriter(\n                buf,\n                fieldnames=fieldnames,\n                delimiter=delimiter,\n                quotechar=quotechar,\n                escapechar=escapechar,\n                quoting=quoting,\n            )\n            writer.writeheader()\n            writer.writerows(\n                [getattr(i, adapt_meth)(**(adapt_kw or {})) for i in items]\n            )\n            return buf.getvalue()\n\n        except Exception as e:\n            # Wrap exceptions\n            raise ParseError(f\"Error generating CSV: {e}\")\n</code></pre>"},{"location":"api/adapters/#json-adapter","title":"JSON Adapter","text":""},{"location":"api/adapters/#pydapter.adapters.json_","title":"<code>pydapter.adapters.json_</code>","text":"<p>JSON Adapter for Pydantic Models.</p> <p>This module provides the JsonAdapter class for converting between Pydantic models and JSON data formats. It supports reading from JSON files, strings, or bytes and writing Pydantic models to JSON format.</p>"},{"location":"api/adapters/#pydapter.adapters.json_-classes","title":"Classes","text":""},{"location":"api/adapters/#pydapter.adapters.json_.JsonAdapter","title":"<code>JsonAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Adapter for converting between Pydantic models and JSON data.</p> <p>This adapter handles JSON files, strings, and byte data, providing methods to: - Parse JSON data into Pydantic model instances - Convert Pydantic models to JSON format - Handle both single objects and arrays of objects</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"json\")</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.adapters.json_ import JsonAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Parse JSON data\njson_data = '{\"name\": \"John\", \"age\": 30}'\nperson = JsonAdapter.from_obj(Person, json_data)\n\n# Parse JSON array\njson_array = '[{\"name\": \"John\", \"age\": 30}, {\"name\": \"Jane\", \"age\": 25}]'\npeople = JsonAdapter.from_obj(Person, json_array, many=True)\n\n# Convert to JSON\njson_output = JsonAdapter.to_obj(person)\n</code></pre> Source code in <code>src/pydapter/adapters/json_.py</code> <pre><code>class JsonAdapter(Adapter[T]):\n    \"\"\"\n    Adapter for converting between Pydantic models and JSON data.\n\n    This adapter handles JSON files, strings, and byte data, providing methods to:\n    - Parse JSON data into Pydantic model instances\n    - Convert Pydantic models to JSON format\n    - Handle both single objects and arrays of objects\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"json\")\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.adapters.json_ import JsonAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Parse JSON data\n        json_data = '{\"name\": \"John\", \"age\": 30}'\n        person = JsonAdapter.from_obj(Person, json_data)\n\n        # Parse JSON array\n        json_array = '[{\"name\": \"John\", \"age\": 30}, {\"name\": \"Jane\", \"age\": 25}]'\n        people = JsonAdapter.from_obj(Person, json_array, many=True)\n\n        # Convert to JSON\n        json_output = JsonAdapter.to_obj(person)\n        ```\n    \"\"\"\n\n    obj_key = \"json\"\n\n    # ---------------- incoming\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: str | bytes | Path,\n        /,\n        *,\n        many=False,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Handle file path\n            if isinstance(obj, Path):\n                try:\n                    text = Path(obj).read_text()\n                except Exception as e:\n                    raise ParseError(f\"Failed to read JSON file: {e}\", source=str(obj))\n            else:\n                text = obj.decode(\"utf-8\") if isinstance(obj, bytes) else obj\n            # Check for empty input\n            if not text or (isinstance(text, str) and not text.strip()):\n                raise ParseError(\n                    \"Empty JSON content\",\n                    source=str(obj)[:100] if isinstance(obj, str) else str(obj),\n                )\n\n            # Parse JSON\n            try:\n                data = json.loads(text)\n            except json.JSONDecodeError as e:\n                raise ParseError(\n                    f\"Invalid JSON: {e}\",\n                    source=str(text)[:100] if isinstance(text, str) else str(text),\n                    position=e.pos,\n                    line=e.lineno,\n                    column=e.colno,\n                )\n\n            # Validate against model\n            try:\n                if many:\n                    if not isinstance(data, list):\n                        raise AdapterValidationError(\n                            \"Expected JSON array for many=True\", data=data\n                        )\n                    return [\n                        getattr(subj_cls, adapt_meth)(i, **(adapt_kw or {}))\n                        for i in data\n                    ]\n                return getattr(subj_cls, adapt_meth)(data, **(adapt_kw or {}))\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=data,\n                    errors=e.errors(),\n                )\n\n        except (ParseError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ParseError(\n                f\"Unexpected error parsing JSON: {e}\",\n                source=str(obj)[:100] if isinstance(obj, str) else str(obj),\n            )\n\n    # ---------------- outgoing\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | list[T],\n        /,\n        *,\n        many=False,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ) -&gt; str:\n        try:\n            items = subj if isinstance(subj, list) else [subj]\n\n            if not items:\n                return \"[]\" if many else \"{}\"\n\n            # Extract JSON serialization options from kwargs\n            json_kwargs = {\n                \"indent\": kw.pop(\"indent\", 2),\n                \"sort_keys\": kw.pop(\"sort_keys\", True),\n                \"ensure_ascii\": kw.pop(\"ensure_ascii\", False),\n            }\n\n            payload = (\n                [getattr(i, adapt_meth)(**(adapt_kw or {})) for i in items]\n                if many\n                else getattr(items[0], adapt_meth)(**(adapt_kw or {}))\n            )\n            return json.dumps(payload, **json_kwargs)\n\n        except Exception as e:\n            # Wrap exceptions\n            raise ParseError(f\"Error generating JSON: {e}\")\n</code></pre>"},{"location":"api/adapters/#toml-adapter","title":"TOML Adapter","text":""},{"location":"api/adapters/#pydapter.adapters.toml_","title":"<code>pydapter.adapters.toml_</code>","text":"<p>TOML Adapter for Pydantic Models.</p> <p>This module provides the TomlAdapter class for converting between Pydantic models and TOML data formats. It supports reading from TOML files or strings and writing Pydantic models to TOML format.</p>"},{"location":"api/adapters/#pydapter.adapters.toml_-classes","title":"Classes","text":""},{"location":"api/adapters/#pydapter.adapters.toml_.TomlAdapter","title":"<code>TomlAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Adapter for converting between Pydantic models and TOML data.</p> <p>This adapter handles TOML files and strings, providing methods to: - Parse TOML data into Pydantic model instances - Convert Pydantic models to TOML format - Handle both single objects and arrays of objects</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"toml\")</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.adapters.toml_ import TomlAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Parse TOML data\ntoml_data = '''\nname = \"John\"\nage = 30\n'''\nperson = TomlAdapter.from_obj(Person, toml_data)\n\n# Parse TOML array\ntoml_array = '''\n[[people]]\nname = \"John\"\nage = 30\n\n[[people]]\nname = \"Jane\"\nage = 25\n'''\npeople = TomlAdapter.from_obj(Person, toml_array, many=True)\n\n# Convert to TOML\ntoml_output = TomlAdapter.to_obj(person)\n</code></pre> Source code in <code>src/pydapter/adapters/toml_.py</code> <pre><code>class TomlAdapter(Adapter[T]):\n    \"\"\"\n    Adapter for converting between Pydantic models and TOML data.\n\n    This adapter handles TOML files and strings, providing methods to:\n    - Parse TOML data into Pydantic model instances\n    - Convert Pydantic models to TOML format\n    - Handle both single objects and arrays of objects\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"toml\")\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.adapters.toml_ import TomlAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Parse TOML data\n        toml_data = '''\n        name = \"John\"\n        age = 30\n        '''\n        person = TomlAdapter.from_obj(Person, toml_data)\n\n        # Parse TOML array\n        toml_array = '''\n        [[people]]\n        name = \"John\"\n        age = 30\n\n        [[people]]\n        name = \"Jane\"\n        age = 25\n        '''\n        people = TomlAdapter.from_obj(Person, toml_array, many=True)\n\n        # Convert to TOML\n        toml_output = TomlAdapter.to_obj(person)\n        ```\n    \"\"\"\n\n    obj_key = \"toml\"\n\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: str | Path,\n        /,\n        *,\n        many=False,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Handle file path\n            if isinstance(obj, Path):\n                try:\n                    text = Path(obj).read_text()\n                except Exception as e:\n                    raise ParseError(f\"Failed to read TOML file: {e}\", source=str(obj))\n            else:\n                text = obj\n\n            # Check for empty input\n            if not text or (isinstance(text, str) and not text.strip()):\n                raise ParseError(\n                    \"Empty TOML content\",\n                    source=str(obj)[:100] if isinstance(obj, str) else str(obj),\n                )\n\n            # Parse TOML\n            try:\n                parsed = toml.loads(text, **kw)\n            except toml.TomlDecodeError as e:\n                raise ParseError(\n                    f\"Invalid TOML: {e}\",\n                    source=str(text)[:100] if isinstance(text, str) else str(text),\n                )\n\n            # Validate against model\n            try:\n                if many:\n                    return [\n                        getattr(subj_cls, adapt_meth)(x, **(adapt_kw or {}))\n                        for x in _ensure_list(parsed)\n                    ]\n                return getattr(subj_cls, adapt_meth)(parsed, **(adapt_kw or {}))\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=parsed,\n                    errors=e.errors(),\n                )\n\n        except (ParseError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ParseError(\n                f\"Unexpected error parsing TOML: {e}\",\n                source=str(obj)[:100] if isinstance(obj, str) else str(obj),\n            )\n\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | list[T],\n        /,\n        *,\n        many=False,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ) -&gt; str:\n        try:\n            items = subj if isinstance(subj, list) else [subj]\n\n            if not items:\n                return \"\"\n\n            payload = (\n                {\"items\": [getattr(i, adapt_meth)(**(adapt_kw or {})) for i in items]}\n                if many\n                else getattr(items[0], adapt_meth)(**(adapt_kw or {}))\n            )\n            return toml.dumps(payload, **kw)\n\n        except Exception as e:\n            # Wrap exceptions\n            raise ParseError(f\"Error generating TOML: {e}\")\n</code></pre>"},{"location":"api/core/","title":"Core API Reference","text":"<p>The <code>pydapter.core</code> module provides the foundational adapter system for converting between Pydantic models and various data formats. It implements a registry-based pattern that enables stateless, bidirectional data transformations.</p>"},{"location":"api/core/#installation","title":"Installation","text":"<pre><code>pip install pydapter\n</code></pre>"},{"location":"api/core/#overview","title":"Overview","text":"<p>The core module establishes the fundamental concepts of pydapter:</p> <ul> <li>Adapter Protocol: Defines the interface for data conversion</li> <li>Registry System: Manages and discovers adapters</li> <li>Adaptable Mixin: Provides convenient model integration</li> <li>Error Handling: Comprehensive exception hierarchy for debugging</li> </ul> <pre><code>Core Architecture:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Adapter     \u2502  \u2502 AdapterRegistry \u2502  \u2502    Adaptable    \u2502\n\u2502   (Protocol)    \u2502  \u2502   (Manager)     \u2502  \u2502    (Mixin)      \u2502\n\u2502                 \u2502  \u2502                 \u2502  \u2502                 \u2502\n\u2502 from_obj()      \u2502  \u2502 register()      \u2502  \u2502 adapt_from()    \u2502\n\u2502 to_obj()        \u2502  \u2502 get()           \u2502  \u2502 adapt_to()      \u2502\n\u2502 obj_key         \u2502  \u2502 adapt_from()    \u2502  \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 adapt_to()      \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The system supports both synchronous and asynchronous operations through parallel implementations in <code>pydapter.core</code> and <code>pydapter.async_core</code>.</p>"},{"location":"api/core/#core-protocols","title":"Core Protocols","text":""},{"location":"api/core/#adapter","title":"Adapter","text":"<p>Module: <code>pydapter.core</code></p> <p>Defines the interface for stateless data conversion between Pydantic models and external formats.</p> <p>Protocol Interface:</p> <pre><code>@runtime_checkable\nclass Adapter(Protocol[T]):\n    \"\"\"Stateless conversion helper.\"\"\"\n\n    obj_key: ClassVar[str]  # Unique identifier for the adapter\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: Any, /, *, many: bool = False, **kw): ...\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw): ...\n</code></pre> <p>Key Concepts:</p> <ul> <li>Stateless: Adapters should not maintain internal state</li> <li>Bidirectional: Support both <code>from_obj</code> (import) and <code>to_obj</code> (export)</li> <li>Type-safe: Use generic typing for type safety</li> <li>Batch Support: Handle single items or collections via <code>many</code> parameter</li> </ul> <p>Implementation Example:</p> <pre><code>from pydapter.core import Adapter\nfrom pydantic import BaseModel\nimport json\n\nclass JSONAdapter(Adapter):\n    obj_key = \"json\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: str, /, *, many: bool = False, **kw):\n        \"\"\"Convert JSON string to Pydantic model(s).\"\"\"\n        data = json.loads(obj)\n        if many:\n            return [subj_cls.model_validate(item) for item in data]\n        return subj_cls.model_validate(data)\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw):\n        \"\"\"Convert Pydantic model(s) to JSON string.\"\"\"\n        if many or isinstance(subj, list):\n            data = [item.model_dump() for item in subj]\n        else:\n            data = subj.model_dump()\n        return json.dumps(data, **kw)\n\n# Usage\nclass User(BaseModel):\n    name: str\n    email: str\n\njson_data = '{\"name\": \"John\", \"email\": \"john@example.com\"}'\nuser = JSONAdapter.from_obj(User, json_data)\nback_to_json = JSONAdapter.to_obj(user)\n</code></pre>"},{"location":"api/core/#asyncadapter","title":"AsyncAdapter","text":"<p>Module: <code>pydapter.async_core</code></p> <p>Asynchronous counterpart to the <code>Adapter</code> protocol for operations requiring async/await.</p> <p>Protocol Interface:</p> <pre><code>@runtime_checkable\nclass AsyncAdapter(Protocol[T]):\n    \"\"\"Stateless, **async** conversion helper.\"\"\"\n\n    obj_key: ClassVar[str]\n\n    @classmethod\n    async def from_obj(\n        cls, subj_cls: type[T], obj: Any, /, *, many: bool = False, **kw\n    ) -&gt; T | list[T]: ...\n\n    @classmethod\n    async def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw) -&gt; Any: ...\n</code></pre> <p>Implementation Example:</p> <pre><code>from pydapter.async_core import AsyncAdapter\nimport aiohttp\nimport json\n\nclass HTTPAPIAdapter(AsyncAdapter):\n    obj_key = \"http_api\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: str, /, *, many: bool = False, **kw):\n        \"\"\"Fetch data from HTTP API and convert to model(s).\"\"\"\n        async with aiohttp.ClientSession() as session:\n            async with session.get(obj) as response:\n                data = await response.json()\n                if many:\n                    return [subj_cls.model_validate(item) for item in data]\n                return subj_cls.model_validate(data)\n\n    @classmethod\n    async def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw):\n        \"\"\"Convert model(s) to API payload.\"\"\"\n        if many or isinstance(subj, list):\n            return [item.model_dump() for item in subj]\n        return subj.model_dump()\n\n# Usage\nusers = await HTTPAPIAdapter.from_obj(User, \"https://api.example.com/users\", many=True)\n</code></pre>"},{"location":"api/core/#registry-system","title":"Registry System","text":""},{"location":"api/core/#adapterregistry","title":"AdapterRegistry","text":"<p>Module: <code>pydapter.core</code></p> <p>Manages adapter registration and provides convenient access methods.</p> <p>Class Interface:</p> <pre><code>class AdapterRegistry:\n    def __init__(self) -&gt; None: ...\n\n    def register(self, adapter_cls: type[Adapter]) -&gt; None: ...\n    def get(self, obj_key: str) -&gt; type[Adapter]: ...\n    def adapt_from(self, subj_cls: type[T], obj, *, obj_key: str, **kw): ...\n    def adapt_to(self, subj, *, obj_key: str, **kw): ...\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.core import AdapterRegistry\n\n# Create registry\nregistry = AdapterRegistry()\n\n# Register adapters\nregistry.register(JSONAdapter)\nregistry.register(CSVAdapter)\n\n# Use via registry\nuser = registry.adapt_from(User, json_data, obj_key=\"json\")\ncsv_data = registry.adapt_to(user, obj_key=\"csv\")\n\n# Direct adapter access\nadapter_cls = registry.get(\"json\")\nuser = adapter_cls.from_obj(User, json_data)\n</code></pre> <p>Error Handling:</p> <p>The registry provides comprehensive error handling:</p> <pre><code>from pydapter.exceptions import AdapterNotFoundError, AdapterError\n\ntry:\n    user = registry.adapt_from(User, data, obj_key=\"unknown\")\nexcept AdapterNotFoundError as e:\n    print(f\"No adapter found: {e}\")\nexcept AdapterError as e:\n    print(f\"Adaptation failed: {e}\")\n</code></pre>"},{"location":"api/core/#asyncadapterregistry","title":"AsyncAdapterRegistry","text":"<p>Module: <code>pydapter.async_core</code></p> <p>Asynchronous version of <code>AdapterRegistry</code> for async adapters.</p> <p>Usage:</p> <pre><code>from pydapter.async_core import AsyncAdapterRegistry\n\n# Create async registry\nasync_registry = AsyncAdapterRegistry()\nasync_registry.register(HTTPAPIAdapter)\n\n# Use with async/await\nusers = await async_registry.adapt_from(User, api_url, obj_key=\"http_api\", many=True)\n</code></pre>"},{"location":"api/core/#adaptable-mixin","title":"Adaptable Mixin","text":""},{"location":"api/core/#adaptable","title":"Adaptable","text":"<p>Module: <code>pydapter.core</code></p> <p>Mixin class that integrates adapter functionality directly into Pydantic models.</p> <p>Class Interface:</p> <pre><code>class Adaptable:\n    \"\"\"Mixin that endows any Pydantic model with adapt-from / adapt-to.\"\"\"\n\n    _adapter_registry: ClassVar[AdapterRegistry | None] = None\n\n    @classmethod\n    def _registry(cls) -&gt; AdapterRegistry: ...\n\n    @classmethod\n    def register_adapter(cls, adapter_cls: type[Adapter]) -&gt; None: ...\n\n    @classmethod\n    def adapt_from(cls, obj, *, obj_key: str, **kw): ...\n\n    def adapt_to(self, *, obj_key: str, **kw): ...\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.core import Adaptable\nfrom pydantic import BaseModel\n\nclass User(BaseModel, Adaptable):\n    name: str\n    email: str\n    age: int\n\n# Register adapters for this model\nUser.register_adapter(JSONAdapter)\nUser.register_adapter(CSVAdapter)\n\n# Use adapter methods directly on the model\nuser = User.adapt_from(json_data, obj_key=\"json\")\ncsv_output = user.adapt_to(obj_key=\"csv\")\n\n# Class method for creating from external data\nusers = User.adapt_from(csv_file_content, obj_key=\"csv\", many=True)\n</code></pre> <p>Advanced Usage:</p> <pre><code># Custom model with multiple adapters\nclass Product(BaseModel, Adaptable):\n    id: str\n    name: str\n    price: float\n    category: str\n\n# Register multiple adapters\nProduct.register_adapter(JSONAdapter)\nProduct.register_adapter(XMLAdapter)\nProduct.register_adapter(DatabaseAdapter)\n\n# Chain conversions\nproduct = Product.adapt_from(xml_data, obj_key=\"xml\")\njson_data = product.adapt_to(obj_key=\"json\")\ndatabase_record = product.adapt_to(obj_key=\"database\")\n</code></pre>"},{"location":"api/core/#exception-hierarchy","title":"Exception Hierarchy","text":""},{"location":"api/core/#core-exceptions","title":"Core Exceptions","text":"<p>Module: <code>pydapter.exceptions</code></p> <p>Comprehensive exception system for error handling and debugging.</p>"},{"location":"api/core/#adaptererror","title":"AdapterError","text":"<p>Base exception for all pydapter errors.</p> <pre><code>class AdapterError(Exception):\n    \"\"\"Base exception for all pydapter errors.\"\"\"\n\n    def __init__(self, message: str, **context: Any):\n        super().__init__(message)\n        self.message = message\n        self.context = context  # Additional error context\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.exceptions import AdapterError\n\ntry:\n    result = adapter.from_obj(Model, invalid_data)\nexcept AdapterError as e:\n    print(f\"Error: {e.message}\")\n    print(f\"Context: {e.context}\")\n</code></pre>"},{"location":"api/core/#validationerror","title":"ValidationError","text":"<p>Exception for data validation failures.</p> <pre><code>class ValidationError(AdapterError):\n    \"\"\"Exception raised when data validation fails.\"\"\"\n\n    def __init__(self, message: str, data: Optional[Any] = None, **context: Any):\n        super().__init__(message, **context)\n        self.data = data  # The data that failed validation\n</code></pre>"},{"location":"api/core/#typeconversionerror","title":"TypeConversionError","text":"<p>Exception for type conversion failures.</p> <pre><code>class TypeConversionError(ValidationError):\n    \"\"\"Exception raised when type conversion fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        source_type: Optional[type] = None,\n        target_type: Optional[type] = None,\n        field_name: Optional[str] = None,\n        model_name: Optional[str] = None,\n        **context: Any,\n    ): ...\n</code></pre>"},{"location":"api/core/#adapternotfounderror","title":"AdapterNotFoundError","text":"<p>Exception when no adapter is registered for a given key.</p> <pre><code>from pydapter.exceptions import AdapterNotFoundError\n\ntry:\n    adapter = registry.get(\"nonexistent\")\nexcept AdapterNotFoundError as e:\n    print(f\"Adapter not found: {e}\")\n</code></pre>"},{"location":"api/core/#configurationerror","title":"ConfigurationError","text":"<p>Exception for adapter configuration issues.</p> <pre><code>from pydapter.exceptions import ConfigurationError\n\nclass BadAdapter:\n    # Missing obj_key will raise ConfigurationError\n    pass\n\ntry:\n    registry.register(BadAdapter)\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"api/core/#advanced-usage-patterns","title":"Advanced Usage Patterns","text":""},{"location":"api/core/#custom-adapter-development","title":"Custom Adapter Development","text":"<p>Create specialized adapters for specific use cases:</p> <pre><code>from pydapter.core import Adapter\nfrom typing import Any, TypeVar\nimport yaml\n\nT = TypeVar(\"T\")\n\nclass YAMLAdapter(Adapter[T]):\n    obj_key = \"yaml\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: str, /, *, many: bool = False, **kw):\n        \"\"\"Convert YAML string to Pydantic model(s).\"\"\"\n        data = yaml.safe_load(obj)\n        if many:\n            return [subj_cls.model_validate(item) for item in data]\n        return subj_cls.model_validate(data)\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw):\n        \"\"\"Convert Pydantic model(s) to YAML string.\"\"\"\n        if many or isinstance(subj, list):\n            data = [item.model_dump() for item in subj]\n        else:\n            data = subj.model_dump()\n        return yaml.dump(data, **kw)\n</code></pre>"},{"location":"api/core/#adapter-composition","title":"Adapter Composition","text":"<p>Combine multiple adapters for complex workflows:</p> <pre><code>class DataPipeline:\n    def __init__(self, model_cls, registry: AdapterRegistry):\n        self.model_cls = model_cls\n        self.registry = registry\n\n    def transform(self, data, from_format: str, to_format: str, **kw):\n        \"\"\"Transform data from one format to another via Pydantic model.\"\"\"\n        # Parse input format to model\n        model_instance = self.registry.adapt_from(\n            self.model_cls, data, obj_key=from_format, **kw\n        )\n\n        # Convert model to output format\n        return self.registry.adapt_to(\n            model_instance, obj_key=to_format, **kw\n        )\n\n# Usage\npipeline = DataPipeline(User, registry)\njson_data = pipeline.transform(csv_data, \"csv\", \"json\")\n</code></pre>"},{"location":"api/core/#error-recovery","title":"Error Recovery","text":"<p>Implement robust error handling with fallbacks:</p> <pre><code>def safe_adapt_from(model_cls, data, primary_key: str, fallback_key: str, registry):\n    \"\"\"Attempt adaptation with fallback on failure.\"\"\"\n    try:\n        return registry.adapt_from(model_cls, data, obj_key=primary_key)\n    except AdapterError as e:\n        print(f\"Primary adapter {primary_key} failed: {e}\")\n        try:\n            return registry.adapt_from(model_cls, data, obj_key=fallback_key)\n        except AdapterError as fallback_error:\n            print(f\"Fallback adapter {fallback_key} also failed: {fallback_error}\")\n            raise AdapterError(\n                f\"Both {primary_key} and {fallback_key} adapters failed\",\n                primary_error=str(e),\n                fallback_error=str(fallback_error)\n            )\n\n# Usage\nuser = safe_adapt_from(User, data, \"json\", \"yaml\", registry)\n</code></pre>"},{"location":"api/core/#best-practices","title":"Best Practices","text":""},{"location":"api/core/#adapter-design","title":"Adapter Design","text":"<ol> <li>Stateless Design: Keep adapters stateless for thread safety</li> <li>Clear obj_key: Use descriptive, unique keys for adapter identification</li> <li>Error Handling: Provide meaningful error messages with context</li> <li>Type Safety: Use proper type hints and validation</li> <li>Documentation: Document expected input/output formats</li> </ol>"},{"location":"api/core/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Lazy Loading: Register adapters only when needed</li> <li>Batch Processing: Use <code>many=True</code> for collections</li> <li>Caching: Cache registry lookups for frequently used adapters</li> <li>Memory Management: Be mindful of memory usage with large datasets</li> </ol>"},{"location":"api/core/#registry-management","title":"Registry Management","text":"<ol> <li>Global Registry: Use a single global registry for consistency</li> <li>Namespace Keys: Use namespaced keys to avoid conflicts (e.g., \"db.postgres\")</li> <li>Validation: Validate adapter implementations before registration</li> <li>Testing: Test all registered adapters thoroughly</li> </ol>"},{"location":"api/core/#error-handling","title":"Error Handling","text":"<ol> <li>Specific Exceptions: Use specific exception types for different error conditions</li> <li>Context Information: Include relevant context in exception details</li> <li>Logging: Log adapter errors for debugging</li> <li>Recovery Strategies: Implement fallback mechanisms where appropriate</li> </ol>"},{"location":"api/core/#integration-examples","title":"Integration Examples","text":""},{"location":"api/core/#database-integration","title":"Database Integration","text":"<pre><code>from pydapter.core import Adapter\nimport sqlite3\n\nclass SQLiteAdapter(Adapter):\n    obj_key = \"sqlite\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: str, /, *, many: bool = False, **kw):\n        \"\"\"Load from SQLite database.\"\"\"\n        conn = sqlite3.connect(kw.get('database', ':memory:'))\n        conn.row_factory = sqlite3.Row\n        cursor = conn.cursor()\n\n        if many:\n            cursor.execute(f\"SELECT * FROM {kw.get('table', subj_cls.__name__.lower())}\")\n            rows = cursor.fetchall()\n            return [subj_cls.model_validate(dict(row)) for row in rows]\n        else:\n            cursor.execute(\n                f\"SELECT * FROM {kw.get('table', subj_cls.__name__.lower())} WHERE id = ?\",\n                (kw.get('id'),)\n            )\n            row = cursor.fetchone()\n            return subj_cls.model_validate(dict(row)) if row else None\n</code></pre>"},{"location":"api/core/#web-api-integration","title":"Web API Integration","text":"<pre><code>from pydapter.async_core import AsyncAdapter\nimport httpx\n\nclass RESTAPIAdapter(AsyncAdapter):\n    obj_key = \"rest_api\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: str, /, *, many: bool = False, **kw):\n        \"\"\"Fetch from REST API.\"\"\"\n        async with httpx.AsyncClient() as client:\n            response = await client.get(obj, params=kw.get('params', {}))\n            response.raise_for_status()\n            data = response.json()\n\n            if many:\n                return [subj_cls.model_validate(item) for item in data]\n            return subj_cls.model_validate(data)\n\n    @classmethod\n    async def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw):\n        \"\"\"Post to REST API.\"\"\"\n        url = kw.get('url')\n        if not url:\n            raise ValueError(\"URL required for REST API adapter\")\n\n        async with httpx.AsyncClient() as client:\n            if many or isinstance(subj, list):\n                data = [item.model_dump() for item in subj]\n            else:\n                data = subj.model_dump()\n\n            response = await client.post(url, json=data)\n            response.raise_for_status()\n            return response.json()\n</code></pre>"},{"location":"api/core/#migration-guide","title":"Migration Guide","text":"<p>When upgrading from previous versions:</p> <ol> <li>Adapter Interface: Update custom adapters to use new protocol interface</li> <li>Error Handling: Migrate to new exception hierarchy</li> <li>Registry Usage: Use <code>AdapterRegistry</code> for better organization</li> <li>Async Support: Consider migrating to async adapters for I/O operations</li> <li>Type Safety: Add proper type hints to existing adapters</li> </ol> <p>For detailed migration instructions, see the Migration Guide.</p>"},{"location":"api/core/#auto-generated-api-reference","title":"Auto-generated API Reference","text":"<p>The following sections contain auto-generated API documentation:</p>"},{"location":"api/core/#core-module","title":"Core Module","text":""},{"location":"api/core/#pydapter.core","title":"<code>pydapter.core</code>","text":"<p>pydapter.core - Adapter protocol, registry, Adaptable mix-in.</p>"},{"location":"api/core/#pydapter.core-classes","title":"Classes","text":""},{"location":"api/core/#pydapter.core.Adaptable","title":"<code>Adaptable</code>","text":"<p>Mixin class that adds adapter functionality to Pydantic models.</p> <p>This mixin provides convenient methods for converting to/from various data formats by maintaining a registry of adapters and providing high-level convenience methods.</p> <p>When mixed into a Pydantic model, it adds: - Class methods for registering adapters - Class methods for creating instances from external formats - Instance methods for converting to external formats</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.core import Adaptable\nfrom pydapter.adapters.json_ import JsonAdapter\n\nclass Person(BaseModel, Adaptable):\n    name: str\n    age: int\n\n# Register an adapter\nPerson.register_adapter(JsonAdapter)\n\n# Create from JSON\njson_data = '{\"name\": \"John\", \"age\": 30}'\nperson = Person.adapt_from(json_data, obj_key=\"json\")\n\n# Convert to JSON\njson_output = person.adapt_to(obj_key=\"json\")\n</code></pre> Source code in <code>src/pydapter/core.py</code> <pre><code>class Adaptable:\n    \"\"\"\n    Mixin class that adds adapter functionality to Pydantic models.\n\n    This mixin provides convenient methods for converting to/from various data formats\n    by maintaining a registry of adapters and providing high-level convenience methods.\n\n    When mixed into a Pydantic model, it adds:\n    - Class methods for registering adapters\n    - Class methods for creating instances from external formats\n    - Instance methods for converting to external formats\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.core import Adaptable\n        from pydapter.adapters.json_ import JsonAdapter\n\n        class Person(BaseModel, Adaptable):\n            name: str\n            age: int\n\n        # Register an adapter\n        Person.register_adapter(JsonAdapter)\n\n        # Create from JSON\n        json_data = '{\"name\": \"John\", \"age\": 30}'\n        person = Person.adapt_from(json_data, obj_key=\"json\")\n\n        # Convert to JSON\n        json_output = person.adapt_to(obj_key=\"json\")\n        ```\n    \"\"\"\n\n    _adapter_registry: ClassVar[AdapterRegistry | None] = None\n\n    @classmethod\n    def _registry(cls) -&gt; AdapterRegistry:\n        \"\"\"Get or create the adapter registry for this class.\"\"\"\n        if cls._adapter_registry is None:\n            cls._adapter_registry = AdapterRegistry()\n        return cls._adapter_registry\n\n    @classmethod\n    def register_adapter(cls, adapter_cls: type[Adapter]) -&gt; None:\n        \"\"\"\n        Register an adapter class with this model.\n\n        Args:\n            adapter_cls: The adapter class to register\n        \"\"\"\n        cls._registry().register(adapter_cls)\n\n    @classmethod\n    def adapt_from(\n        cls, obj: Any, *, obj_key: str, adapt_meth: str = \"model_validate\", **kw: Any\n    ) -&gt; Any:\n        \"\"\"\n        Create model instance(s) from external data format.\n\n        Args:\n            obj: The source data in the specified format\n            obj_key: The key identifying which adapter to use\n            adapt_meth: Method name to use for model validation (default: \"model_validate\")\n            **kw: Additional adapter-specific arguments\n\n        Returns:\n            Model instance(s) created from the source data\n        \"\"\"\n        return cls._registry().adapt_from(\n            cls, obj, obj_key=obj_key, adapt_meth=adapt_meth, **kw\n        )\n\n    def adapt_to(\n        self, *, obj_key: str, adapt_meth: str = \"model_dump\", **kw: Any\n    ) -&gt; Any:\n        \"\"\"\n        Convert this model instance to external data format.\n\n        Args:\n            obj_key: The key identifying which adapter to use\n            adapt_meth: Method name to use for model dumping (default: \"model_dump\")\n            **kw: Additional adapter-specific arguments\n\n        Returns:\n            Data in the specified external format\n        \"\"\"\n        return self._registry().adapt_to(\n            self, obj_key=obj_key, adapt_meth=adapt_meth, **kw\n        )\n</code></pre>"},{"location":"api/core/#pydapter.core.Adaptable-functions","title":"Functions","text":""},{"location":"api/core/#pydapter.core.Adaptable.adapt_from","title":"<code>adapt_from(obj, *, obj_key, adapt_meth='model_validate', **kw)</code>  <code>classmethod</code>","text":"<p>Create model instance(s) from external data format.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The source data in the specified format</p> required <code>obj_key</code> <code>str</code> <p>The key identifying which adapter to use</p> required <code>adapt_meth</code> <code>str</code> <p>Method name to use for model validation (default: \"model_validate\")</p> <code>'model_validate'</code> <code>**kw</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Model instance(s) created from the source data</p> Source code in <code>src/pydapter/core.py</code> <pre><code>@classmethod\ndef adapt_from(\n    cls, obj: Any, *, obj_key: str, adapt_meth: str = \"model_validate\", **kw: Any\n) -&gt; Any:\n    \"\"\"\n    Create model instance(s) from external data format.\n\n    Args:\n        obj: The source data in the specified format\n        obj_key: The key identifying which adapter to use\n        adapt_meth: Method name to use for model validation (default: \"model_validate\")\n        **kw: Additional adapter-specific arguments\n\n    Returns:\n        Model instance(s) created from the source data\n    \"\"\"\n    return cls._registry().adapt_from(\n        cls, obj, obj_key=obj_key, adapt_meth=adapt_meth, **kw\n    )\n</code></pre>"},{"location":"api/core/#pydapter.core.Adaptable.adapt_to","title":"<code>adapt_to(*, obj_key, adapt_meth='model_dump', **kw)</code>","text":"<p>Convert this model instance to external data format.</p> <p>Parameters:</p> Name Type Description Default <code>obj_key</code> <code>str</code> <p>The key identifying which adapter to use</p> required <code>adapt_meth</code> <code>str</code> <p>Method name to use for model dumping (default: \"model_dump\")</p> <code>'model_dump'</code> <code>**kw</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Data in the specified external format</p> Source code in <code>src/pydapter/core.py</code> <pre><code>def adapt_to(\n    self, *, obj_key: str, adapt_meth: str = \"model_dump\", **kw: Any\n) -&gt; Any:\n    \"\"\"\n    Convert this model instance to external data format.\n\n    Args:\n        obj_key: The key identifying which adapter to use\n        adapt_meth: Method name to use for model dumping (default: \"model_dump\")\n        **kw: Additional adapter-specific arguments\n\n    Returns:\n        Data in the specified external format\n    \"\"\"\n    return self._registry().adapt_to(\n        self, obj_key=obj_key, adapt_meth=adapt_meth, **kw\n    )\n</code></pre>"},{"location":"api/core/#pydapter.core.Adaptable.register_adapter","title":"<code>register_adapter(adapter_cls)</code>  <code>classmethod</code>","text":"<p>Register an adapter class with this model.</p> <p>Parameters:</p> Name Type Description Default <code>adapter_cls</code> <code>type[Adapter]</code> <p>The adapter class to register</p> required Source code in <code>src/pydapter/core.py</code> <pre><code>@classmethod\ndef register_adapter(cls, adapter_cls: type[Adapter]) -&gt; None:\n    \"\"\"\n    Register an adapter class with this model.\n\n    Args:\n        adapter_cls: The adapter class to register\n    \"\"\"\n    cls._registry().register(adapter_cls)\n</code></pre>"},{"location":"api/core/#pydapter.core.Adapter","title":"<code>Adapter</code>","text":"<p>               Bases: <code>Protocol[T]</code></p> <p>Protocol defining the interface for data format adapters.</p> <p>Adapters are stateless conversion helpers that transform data between Pydantic models and various formats (CSV, JSON, TOML, etc.).</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <code>str</code> <p>Unique identifier for the adapter type (e.g., \"csv\", \"json\")</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.adapters.json_ import JsonAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Convert from JSON to Pydantic model\njson_data = '{\"name\": \"John\", \"age\": 30}'\nperson = JsonAdapter.from_obj(Person, json_data)\n\n# Convert from Pydantic model to JSON\njson_output = JsonAdapter.to_obj(person)\n</code></pre> Source code in <code>src/pydapter/core.py</code> <pre><code>@runtime_checkable\nclass Adapter(Protocol[T]):\n    \"\"\"\n    Protocol defining the interface for data format adapters.\n\n    Adapters are stateless conversion helpers that transform data between\n    Pydantic models and various formats (CSV, JSON, TOML, etc.).\n\n    Attributes:\n        obj_key: Unique identifier for the adapter type (e.g., \"csv\", \"json\")\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.adapters.json_ import JsonAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Convert from JSON to Pydantic model\n        json_data = '{\"name\": \"John\", \"age\": 30}'\n        person = JsonAdapter.from_obj(Person, json_data)\n\n        # Convert from Pydantic model to JSON\n        json_output = JsonAdapter.to_obj(person)\n        ```\n    \"\"\"\n\n    obj_key: ClassVar[str]\n\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: Any,\n        /,\n        *,\n        many: bool = False,\n        adapt_meth: str = \"model_validate\",\n        **kw: Any,\n    ) -&gt; T | list[T]:\n        \"\"\"\n        Convert from external format to Pydantic model instances.\n\n        Args:\n            subj_cls: The Pydantic model class to instantiate\n            obj: The source data in the adapter's format\n            many: If True, expect/return a list of instances\n            adapt_meth: Method name to use for model validation (default: \"model_validate\")\n            **kw: Additional adapter-specific arguments\n\n        Returns:\n            Single model instance or list of instances based on 'many' parameter\n        \"\"\"\n        ...\n\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | list[T],\n        /,\n        *,\n        many: bool = False,\n        adapt_meth: str = \"model_dump\",\n        **kw: Any,\n    ) -&gt; Any:\n        \"\"\"\n        Convert from Pydantic model instances to external format.\n\n        Args:\n            subj: Single model instance or list of instances\n            many: If True, handle as list of instances\n            adapt_meth: Method name to use for model dumping (default: \"model_dump\")\n            **kw: Additional adapter-specific arguments\n\n        Returns:\n            Data in the adapter's external format\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/core/#pydapter.core.Adapter-functions","title":"Functions","text":""},{"location":"api/core/#pydapter.core.Adapter.from_obj","title":"<code>from_obj(subj_cls, obj, /, *, many=False, adapt_meth='model_validate', **kw)</code>  <code>classmethod</code>","text":"<p>Convert from external format to Pydantic model instances.</p> <p>Parameters:</p> Name Type Description Default <code>subj_cls</code> <code>type[T]</code> <p>The Pydantic model class to instantiate</p> required <code>obj</code> <code>Any</code> <p>The source data in the adapter's format</p> required <code>many</code> <code>bool</code> <p>If True, expect/return a list of instances</p> <code>False</code> <code>adapt_meth</code> <code>str</code> <p>Method name to use for model validation (default: \"model_validate\")</p> <code>'model_validate'</code> <code>**kw</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>T | list[T]</code> <p>Single model instance or list of instances based on 'many' parameter</p> Source code in <code>src/pydapter/core.py</code> <pre><code>@classmethod\ndef from_obj(\n    cls,\n    subj_cls: type[T],\n    obj: Any,\n    /,\n    *,\n    many: bool = False,\n    adapt_meth: str = \"model_validate\",\n    **kw: Any,\n) -&gt; T | list[T]:\n    \"\"\"\n    Convert from external format to Pydantic model instances.\n\n    Args:\n        subj_cls: The Pydantic model class to instantiate\n        obj: The source data in the adapter's format\n        many: If True, expect/return a list of instances\n        adapt_meth: Method name to use for model validation (default: \"model_validate\")\n        **kw: Additional adapter-specific arguments\n\n    Returns:\n        Single model instance or list of instances based on 'many' parameter\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/#pydapter.core.Adapter.to_obj","title":"<code>to_obj(subj, /, *, many=False, adapt_meth='model_dump', **kw)</code>  <code>classmethod</code>","text":"<p>Convert from Pydantic model instances to external format.</p> <p>Parameters:</p> Name Type Description Default <code>subj</code> <code>T | list[T]</code> <p>Single model instance or list of instances</p> required <code>many</code> <code>bool</code> <p>If True, handle as list of instances</p> <code>False</code> <code>adapt_meth</code> <code>str</code> <p>Method name to use for model dumping (default: \"model_dump\")</p> <code>'model_dump'</code> <code>**kw</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Data in the adapter's external format</p> Source code in <code>src/pydapter/core.py</code> <pre><code>@classmethod\ndef to_obj(\n    cls,\n    subj: T | list[T],\n    /,\n    *,\n    many: bool = False,\n    adapt_meth: str = \"model_dump\",\n    **kw: Any,\n) -&gt; Any:\n    \"\"\"\n    Convert from Pydantic model instances to external format.\n\n    Args:\n        subj: Single model instance or list of instances\n        many: If True, handle as list of instances\n        adapt_meth: Method name to use for model dumping (default: \"model_dump\")\n        **kw: Additional adapter-specific arguments\n\n    Returns:\n        Data in the adapter's external format\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/#pydapter.core.AdapterRegistry","title":"<code>AdapterRegistry</code>","text":"<p>Registry for managing and accessing data format adapters.</p> <p>The registry maintains a mapping of adapter keys to adapter classes, providing a centralized way to register and retrieve adapters for different data formats.</p> Example <pre><code>from pydapter.core import AdapterRegistry\nfrom pydapter.adapters.json_ import JsonAdapter\n\nregistry = AdapterRegistry()\nregistry.register(JsonAdapter)\n\n# Use the registry to adapt data\njson_data = '{\"name\": \"John\", \"age\": 30}'\nperson = registry.adapt_from(Person, json_data, obj_key=\"json\")\n</code></pre> Source code in <code>src/pydapter/core.py</code> <pre><code>class AdapterRegistry:\n    \"\"\"\n    Registry for managing and accessing data format adapters.\n\n    The registry maintains a mapping of adapter keys to adapter classes,\n    providing a centralized way to register and retrieve adapters for\n    different data formats.\n\n    Example:\n        ```python\n        from pydapter.core import AdapterRegistry\n        from pydapter.adapters.json_ import JsonAdapter\n\n        registry = AdapterRegistry()\n        registry.register(JsonAdapter)\n\n        # Use the registry to adapt data\n        json_data = '{\"name\": \"John\", \"age\": 30}'\n        person = registry.adapt_from(Person, json_data, obj_key=\"json\")\n        ```\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize an empty adapter registry.\"\"\"\n        self._reg: dict[str, type[Adapter]] = {}\n\n    def register(self, adapter_cls: type[Adapter]) -&gt; None:\n        \"\"\"\n        Register an adapter class with the registry.\n\n        Args:\n            adapter_cls: The adapter class to register. Must have an 'obj_key' attribute.\n\n        Raises:\n            ConfigurationError: If the adapter class doesn't define 'obj_key'\n        \"\"\"\n        key = getattr(adapter_cls, \"obj_key\", None)\n        if not key:\n            raise ConfigurationError(\n                \"Adapter must define 'obj_key'\", adapter_cls=adapter_cls.__name__\n            )\n        self._reg[key] = adapter_cls\n\n    def get(self, obj_key: str) -&gt; type[Adapter]:\n        \"\"\"\n        Retrieve an adapter class by its key.\n\n        Args:\n            obj_key: The key identifier for the adapter\n\n        Returns:\n            The adapter class associated with the key\n\n        Raises:\n            AdapterNotFoundError: If no adapter is registered for the given key\n        \"\"\"\n        try:\n            return self._reg[obj_key]\n        except KeyError as exc:\n            raise AdapterNotFoundError(\n                f\"No adapter registered for '{obj_key}'\", obj_key=obj_key\n            ) from exc\n\n    def adapt_from(\n        self,\n        subj_cls: type[T],\n        obj: Any,\n        *,\n        obj_key: str,\n        adapt_meth: str = \"model_validate\",\n        **kw: Any,\n    ) -&gt; T | list[T]:\n        \"\"\"\n        Convenience method to convert from external format to Pydantic model.\n\n        Args:\n            subj_cls: The Pydantic model class to instantiate\n            obj: The source data in the specified format\n            obj_key: The key identifying which adapter to use\n            adapt_meth: Method name to use for model validation (default: \"model_validate\")\n            **kw: Additional adapter-specific arguments\n\n        Returns:\n            Model instance(s) created from the source data\n\n        Raises:\n            AdapterNotFoundError: If no adapter is registered for obj_key\n            AdapterError: If the adaptation process fails\n        \"\"\"\n        try:\n            result = self.get(obj_key).from_obj(\n                subj_cls, obj, adapt_meth=adapt_meth, **kw\n            )\n            if result is None:\n                raise AdapterError(f\"Adapter {obj_key} returned None\", adapter=obj_key)\n            return result\n\n        except Exception as exc:\n            if isinstance(exc, AdapterError) or isinstance(exc, PYDAPTER_PYTHON_ERRORS):\n                raise\n\n            raise AdapterError(\n                f\"Error adapting from {obj_key}\", original_error=str(exc)\n            ) from exc\n\n    def adapt_to(\n        self, subj: Any, *, obj_key: str, adapt_meth: str = \"model_dump\", **kw: Any\n    ) -&gt; Any:\n        \"\"\"\n        Convenience method to convert from Pydantic model to external format.\n\n        Args:\n            subj: The model instance(s) to convert\n            obj_key: The key identifying which adapter to use\n            adapt_meth: Method name to use for model dumping (default: \"model_dump\")\n            **kw: Additional adapter-specific arguments\n\n        Returns:\n            Data in the specified external format\n\n        Raises:\n            AdapterNotFoundError: If no adapter is registered for obj_key\n            AdapterError: If the adaptation process fails\n        \"\"\"\n        try:\n            result = self.get(obj_key).to_obj(subj, adapt_meth=adapt_meth, **kw)\n            if result is None:\n                raise AdapterError(f\"Adapter {obj_key} returned None\", adapter=obj_key)\n            return result\n\n        except Exception as exc:\n            if isinstance(exc, AdapterError) or isinstance(exc, PYDAPTER_PYTHON_ERRORS):\n                raise\n\n            raise AdapterError(\n                f\"Error adapting to {obj_key}\", original_error=str(exc)\n            ) from exc\n</code></pre>"},{"location":"api/core/#pydapter.core.AdapterRegistry-functions","title":"Functions","text":""},{"location":"api/core/#pydapter.core.AdapterRegistry.__init__","title":"<code>__init__()</code>","text":"<p>Initialize an empty adapter registry.</p> Source code in <code>src/pydapter/core.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize an empty adapter registry.\"\"\"\n    self._reg: dict[str, type[Adapter]] = {}\n</code></pre>"},{"location":"api/core/#pydapter.core.AdapterRegistry.adapt_from","title":"<code>adapt_from(subj_cls, obj, *, obj_key, adapt_meth='model_validate', **kw)</code>","text":"<p>Convenience method to convert from external format to Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>subj_cls</code> <code>type[T]</code> <p>The Pydantic model class to instantiate</p> required <code>obj</code> <code>Any</code> <p>The source data in the specified format</p> required <code>obj_key</code> <code>str</code> <p>The key identifying which adapter to use</p> required <code>adapt_meth</code> <code>str</code> <p>Method name to use for model validation (default: \"model_validate\")</p> <code>'model_validate'</code> <code>**kw</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>T | list[T]</code> <p>Model instance(s) created from the source data</p> <p>Raises:</p> Type Description <code>AdapterNotFoundError</code> <p>If no adapter is registered for obj_key</p> <code>AdapterError</code> <p>If the adaptation process fails</p> Source code in <code>src/pydapter/core.py</code> <pre><code>def adapt_from(\n    self,\n    subj_cls: type[T],\n    obj: Any,\n    *,\n    obj_key: str,\n    adapt_meth: str = \"model_validate\",\n    **kw: Any,\n) -&gt; T | list[T]:\n    \"\"\"\n    Convenience method to convert from external format to Pydantic model.\n\n    Args:\n        subj_cls: The Pydantic model class to instantiate\n        obj: The source data in the specified format\n        obj_key: The key identifying which adapter to use\n        adapt_meth: Method name to use for model validation (default: \"model_validate\")\n        **kw: Additional adapter-specific arguments\n\n    Returns:\n        Model instance(s) created from the source data\n\n    Raises:\n        AdapterNotFoundError: If no adapter is registered for obj_key\n        AdapterError: If the adaptation process fails\n    \"\"\"\n    try:\n        result = self.get(obj_key).from_obj(\n            subj_cls, obj, adapt_meth=adapt_meth, **kw\n        )\n        if result is None:\n            raise AdapterError(f\"Adapter {obj_key} returned None\", adapter=obj_key)\n        return result\n\n    except Exception as exc:\n        if isinstance(exc, AdapterError) or isinstance(exc, PYDAPTER_PYTHON_ERRORS):\n            raise\n\n        raise AdapterError(\n            f\"Error adapting from {obj_key}\", original_error=str(exc)\n        ) from exc\n</code></pre>"},{"location":"api/core/#pydapter.core.AdapterRegistry.adapt_to","title":"<code>adapt_to(subj, *, obj_key, adapt_meth='model_dump', **kw)</code>","text":"<p>Convenience method to convert from Pydantic model to external format.</p> <p>Parameters:</p> Name Type Description Default <code>subj</code> <code>Any</code> <p>The model instance(s) to convert</p> required <code>obj_key</code> <code>str</code> <p>The key identifying which adapter to use</p> required <code>adapt_meth</code> <code>str</code> <p>Method name to use for model dumping (default: \"model_dump\")</p> <code>'model_dump'</code> <code>**kw</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Data in the specified external format</p> <p>Raises:</p> Type Description <code>AdapterNotFoundError</code> <p>If no adapter is registered for obj_key</p> <code>AdapterError</code> <p>If the adaptation process fails</p> Source code in <code>src/pydapter/core.py</code> <pre><code>def adapt_to(\n    self, subj: Any, *, obj_key: str, adapt_meth: str = \"model_dump\", **kw: Any\n) -&gt; Any:\n    \"\"\"\n    Convenience method to convert from Pydantic model to external format.\n\n    Args:\n        subj: The model instance(s) to convert\n        obj_key: The key identifying which adapter to use\n        adapt_meth: Method name to use for model dumping (default: \"model_dump\")\n        **kw: Additional adapter-specific arguments\n\n    Returns:\n        Data in the specified external format\n\n    Raises:\n        AdapterNotFoundError: If no adapter is registered for obj_key\n        AdapterError: If the adaptation process fails\n    \"\"\"\n    try:\n        result = self.get(obj_key).to_obj(subj, adapt_meth=adapt_meth, **kw)\n        if result is None:\n            raise AdapterError(f\"Adapter {obj_key} returned None\", adapter=obj_key)\n        return result\n\n    except Exception as exc:\n        if isinstance(exc, AdapterError) or isinstance(exc, PYDAPTER_PYTHON_ERRORS):\n            raise\n\n        raise AdapterError(\n            f\"Error adapting to {obj_key}\", original_error=str(exc)\n        ) from exc\n</code></pre>"},{"location":"api/core/#pydapter.core.AdapterRegistry.get","title":"<code>get(obj_key)</code>","text":"<p>Retrieve an adapter class by its key.</p> <p>Parameters:</p> Name Type Description Default <code>obj_key</code> <code>str</code> <p>The key identifier for the adapter</p> required <p>Returns:</p> Type Description <code>type[Adapter]</code> <p>The adapter class associated with the key</p> <p>Raises:</p> Type Description <code>AdapterNotFoundError</code> <p>If no adapter is registered for the given key</p> Source code in <code>src/pydapter/core.py</code> <pre><code>def get(self, obj_key: str) -&gt; type[Adapter]:\n    \"\"\"\n    Retrieve an adapter class by its key.\n\n    Args:\n        obj_key: The key identifier for the adapter\n\n    Returns:\n        The adapter class associated with the key\n\n    Raises:\n        AdapterNotFoundError: If no adapter is registered for the given key\n    \"\"\"\n    try:\n        return self._reg[obj_key]\n    except KeyError as exc:\n        raise AdapterNotFoundError(\n            f\"No adapter registered for '{obj_key}'\", obj_key=obj_key\n        ) from exc\n</code></pre>"},{"location":"api/core/#pydapter.core.AdapterRegistry.register","title":"<code>register(adapter_cls)</code>","text":"<p>Register an adapter class with the registry.</p> <p>Parameters:</p> Name Type Description Default <code>adapter_cls</code> <code>type[Adapter]</code> <p>The adapter class to register. Must have an 'obj_key' attribute.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If the adapter class doesn't define 'obj_key'</p> Source code in <code>src/pydapter/core.py</code> <pre><code>def register(self, adapter_cls: type[Adapter]) -&gt; None:\n    \"\"\"\n    Register an adapter class with the registry.\n\n    Args:\n        adapter_cls: The adapter class to register. Must have an 'obj_key' attribute.\n\n    Raises:\n        ConfigurationError: If the adapter class doesn't define 'obj_key'\n    \"\"\"\n    key = getattr(adapter_cls, \"obj_key\", None)\n    if not key:\n        raise ConfigurationError(\n            \"Adapter must define 'obj_key'\", adapter_cls=adapter_cls.__name__\n        )\n    self._reg[key] = adapter_cls\n</code></pre>"},{"location":"api/core/#async-core-module","title":"Async Core Module","text":""},{"location":"api/core/#pydapter.async_core","title":"<code>pydapter.async_core</code>","text":"<p>pydapter.async_core - async counterparts to the sync Adapter stack</p>"},{"location":"api/core/#pydapter.async_core-classes","title":"Classes","text":""},{"location":"api/core/#pydapter.async_core.AsyncAdaptable","title":"<code>AsyncAdaptable</code>","text":"<p>Mixin that endows any Pydantic model with async adapt-from / adapt-to.</p> Source code in <code>src/pydapter/async_core.py</code> <pre><code>class AsyncAdaptable:\n    \"\"\"\n    Mixin that endows any Pydantic model with async adapt-from / adapt-to.\n    \"\"\"\n\n    _async_registry: ClassVar[AsyncAdapterRegistry | None] = None\n\n    # registry access\n    @classmethod\n    def _areg(cls) -&gt; AsyncAdapterRegistry:\n        if cls._async_registry is None:\n            cls._async_registry = AsyncAdapterRegistry()\n        return cls._async_registry\n\n    @classmethod\n    def register_async_adapter(cls, adapter_cls: type[AsyncAdapter]) -&gt; None:\n        cls._areg().register(adapter_cls)\n\n    # helpers\n    @classmethod\n    async def adapt_from_async(\n        cls, obj, *, obj_key: str, adapt_meth: str = \"model_validate\", **kw\n    ):\n        return await cls._areg().adapt_from(\n            cls, obj, obj_key=obj_key, adapt_meth=adapt_meth, **kw\n        )\n\n    async def adapt_to_async(\n        self, *, obj_key: str, adapt_meth: str = \"model_dump\", **kw\n    ):\n        return await self._areg().adapt_to(\n            self, obj_key=obj_key, adapt_meth=adapt_meth, **kw\n        )\n</code></pre>"},{"location":"api/core/#pydapter.async_core.AsyncAdapter","title":"<code>AsyncAdapter</code>","text":"<p>               Bases: <code>Protocol[T]</code></p> <p>Stateless, async conversion helper.</p> Source code in <code>src/pydapter/async_core.py</code> <pre><code>@runtime_checkable\nclass AsyncAdapter(Protocol[T]):\n    \"\"\"Stateless, **async** conversion helper.\"\"\"\n\n    obj_key: ClassVar[str]\n\n    @classmethod\n    async def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: Any,\n        /,\n        *,\n        many: bool = False,\n        adapt_meth: str = \"model_validate\",\n        **kw,\n    ) -&gt; T | list[T]: ...\n\n    @classmethod\n    async def to_obj(\n        cls,\n        subj: T | list[T],\n        /,\n        *,\n        many: bool = False,\n        adapt_meth: str = \"model_dump\",\n        **kw,\n    ) -&gt; Any: ...\n</code></pre>"},{"location":"api/core/#exceptions","title":"Exceptions","text":""},{"location":"api/core/#pydapter.exceptions","title":"<code>pydapter.exceptions</code>","text":"<p>pydapter.exceptions - Custom exception hierarchy for pydapter.</p>"},{"location":"api/core/#pydapter.exceptions-classes","title":"Classes","text":""},{"location":"api/core/#pydapter.exceptions.AdapterError","title":"<code>AdapterError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all pydapter errors.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class AdapterError(Exception):\n    \"\"\"Base exception for all pydapter errors.\"\"\"\n\n    def __init__(self, message: str, **context: Any):\n        super().__init__(message)\n        self.message = message\n        self.context = context\n\n    def __str__(self) -&gt; str:\n        context_str = \", \".join(f\"{k}={v!r}\" for k, v in self.context.items())\n        if context_str:\n            return f\"{self.message} ({context_str})\"\n        return self.message\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.AdapterNotFoundError","title":"<code>AdapterNotFoundError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when an adapter is not found.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class AdapterNotFoundError(AdapterError):\n    \"\"\"Exception raised when an adapter is not found.\"\"\"\n\n    def __init__(self, message: str, obj_key: str | None = None, **context: Any):\n        super().__init__(message, **context)\n        self.obj_key = obj_key\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.ConfigurationError","title":"<code>ConfigurationError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when adapter configuration is invalid.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class ConfigurationError(AdapterError):\n    \"\"\"Exception raised when adapter configuration is invalid.\"\"\"\n\n    def __init__(\n        self, message: str, config: dict[str, Any] | None = None, **context: Any\n    ):\n        super().__init__(message, **context)\n        self.config = config\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.ConnectionError","title":"<code>ConnectionError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when a connection to a data source fails.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class ConnectionError(AdapterError):\n    \"\"\"Exception raised when a connection to a data source fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        adapter: str | None = None,\n        url: str | None = None,\n        **context: Any,\n    ):\n        super().__init__(message, **context)\n        self.adapter = adapter\n        self.url = url\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.ParseError","title":"<code>ParseError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when data parsing fails.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class ParseError(AdapterError):\n    \"\"\"Exception raised when data parsing fails.\"\"\"\n\n    def __init__(self, message: str, source: str | None = None, **context: Any):\n        super().__init__(message, **context)\n        self.source = source\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.QueryError","title":"<code>QueryError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when a query to a data source fails.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class QueryError(AdapterError):\n    \"\"\"Exception raised when a query to a data source fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        query: Any | None = None,\n        adapter: str | None = None,\n        **context: Any,\n    ):\n        super().__init__(message, **context)\n        self.query = query\n        self.adapter = adapter\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.ResourceError","title":"<code>ResourceError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when a resource (file, database, etc.) cannot be accessed.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class ResourceError(AdapterError):\n    \"\"\"Exception raised when a resource (file, database, etc.) cannot be accessed.\"\"\"\n\n    def __init__(self, message: str, resource: str | None = None, **context: Any):\n        super().__init__(message, **context)\n        self.resource = resource\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.TypeConversionError","title":"<code>TypeConversionError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Exception raised when type conversion fails.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class TypeConversionError(ValidationError):\n    \"\"\"Exception raised when type conversion fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        source_type: type | None = None,\n        target_type: type | None = None,\n        field_name: str | None = None,\n        model_name: str | None = None,\n        **context: Any,\n    ):\n        super().__init__(message, **context)\n        self.source_type = source_type\n        self.target_type = target_type\n        self.field_name = field_name\n        self.model_name = model_name\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when data validation fails.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class ValidationError(AdapterError):\n    \"\"\"Exception raised when data validation fails.\"\"\"\n\n    def __init__(self, message: str, data: Any | None = None, **context: Any):\n        super().__init__(message, **context)\n        self.data = data\n</code></pre>"},{"location":"api/extras/","title":"Extras API","text":"<p>This page documents the extra adapters provided by pydapter.</p>"},{"location":"api/extras/#excel-adapter","title":"Excel Adapter","text":""},{"location":"api/extras/#pydapter.extras.excel_","title":"<code>pydapter.extras.excel_</code>","text":"<p>Excel adapter (requires pandas + xlsxwriter engine).</p>"},{"location":"api/extras/#pydapter.extras.excel_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.excel_.ExcelAdapter","title":"<code>ExcelAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Adapter for converting between Pydantic models and Excel files.</p> <p>This adapter handles Excel (.xlsx) files, providing methods to: - Read Excel files into Pydantic model instances - Write Pydantic models to Excel files - Support for different sheets and pandas read_excel options</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"xlsx\")</p> Example <pre><code>from pathlib import Path\nfrom pydantic import BaseModel\nfrom pydapter.extras.excel_ import ExcelAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Read from Excel file\nexcel_file = Path(\"people.xlsx\")\npeople = ExcelAdapter.from_obj(Person, excel_file, many=True)\n\n# Write to Excel file\noutput_bytes = ExcelAdapter.to_obj(people, many=True)\nwith open(\"output.xlsx\", \"wb\") as f:\n    f.write(output_bytes)\n</code></pre> Source code in <code>src/pydapter/extras/excel_.py</code> <pre><code>class ExcelAdapter(Adapter[T]):\n    \"\"\"\n    Adapter for converting between Pydantic models and Excel files.\n\n    This adapter handles Excel (.xlsx) files, providing methods to:\n    - Read Excel files into Pydantic model instances\n    - Write Pydantic models to Excel files\n    - Support for different sheets and pandas read_excel options\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"xlsx\")\n\n    Example:\n        ```python\n        from pathlib import Path\n        from pydantic import BaseModel\n        from pydapter.extras.excel_ import ExcelAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Read from Excel file\n        excel_file = Path(\"people.xlsx\")\n        people = ExcelAdapter.from_obj(Person, excel_file, many=True)\n\n        # Write to Excel file\n        output_bytes = ExcelAdapter.to_obj(people, many=True)\n        with open(\"output.xlsx\", \"wb\") as f:\n            f.write(output_bytes)\n        ```\n    \"\"\"\n\n    obj_key = \"xlsx\"\n\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: str | Path | bytes,\n        /,\n        *,\n        many: bool = True,\n        adapt_meth: str = \"model_validate\",\n        sheet_name: str | int = 0,\n        adapt_kw: dict | None = None,\n        **kw: Any,\n    ) -&gt; T | list[T]:\n        \"\"\"\n        Convert Excel data to Pydantic model instances.\n\n        Args:\n            subj_cls: The Pydantic model class to instantiate\n            obj: Excel file path, file-like object, or bytes\n            many: If True, convert all rows; if False, convert only first row\n            adapt_meth: Method name to use for model validation (default: \"model_validate\")\n            sheet_name: Sheet name or index to read (default: 0)\n            **kw: Additional arguments passed to pandas.read_excel\n\n        Returns:\n            List of model instances if many=True, single instance if many=False\n\n        Raises:\n            ResourceError: If the Excel file cannot be read\n            AdapterError: If the data cannot be converted to models\n        \"\"\"\n        try:\n            if isinstance(obj, bytes):\n                df = pd.read_excel(io.BytesIO(obj), sheet_name=sheet_name, **kw)\n            else:\n                df = pd.read_excel(obj, sheet_name=sheet_name, **kw)\n            return DataFrameAdapter.from_obj(\n                subj_cls, df, many=many, adapt_meth=adapt_meth, adapt_kw=adapt_kw\n            )\n        except FileNotFoundError as e:\n            raise ResourceError(f\"File not found: {e}\", resource=str(obj)) from e\n        except ValueError as e:\n            raise AdapterError(\n                f\"Error adapting from xlsx (original_error='{e}')\", adapter=\"xlsx\"\n            ) from e\n        except Exception as e:\n            raise AdapterError(\n                f\"Unexpected error in Excel adapter: {e}\", adapter=\"xlsx\"\n            ) from e\n\n    # outgoing\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | list[T],\n        /,\n        *,\n        many: bool = True,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        sheet_name: str = \"Sheet1\",\n        **kw: Any,\n    ) -&gt; bytes:\n        \"\"\"\n        Convert Pydantic model instances to Excel bytes.\n\n        Args:\n            subj: Single model instance or list of instances\n            many: If True, handle as multiple instances\n            adapt_meth: Method name to use for model dumping (default: \"model_dump\")\n            sheet_name: Name of the Excel sheet (default: \"Sheet1\")\n            **kw: Additional arguments passed to DataFrame constructor\n\n        Returns:\n            Excel file content as bytes\n        \"\"\"\n        df = DataFrameAdapter.to_obj(\n            subj, many=many, adapt_meth=adapt_meth, adapt_kw=adapt_kw, **kw\n        )\n        buf = io.BytesIO()\n        with pd.ExcelWriter(buf, engine=\"xlsxwriter\") as wr:\n            df.to_excel(wr, sheet_name=sheet_name, index=False)\n        return buf.getvalue()\n</code></pre>"},{"location":"api/extras/#pydapter.extras.excel_.ExcelAdapter-functions","title":"Functions","text":""},{"location":"api/extras/#pydapter.extras.excel_.ExcelAdapter.from_obj","title":"<code>from_obj(subj_cls, obj, /, *, many=True, adapt_meth='model_validate', sheet_name=0, adapt_kw=None, **kw)</code>  <code>classmethod</code>","text":"<p>Convert Excel data to Pydantic model instances.</p> <p>Parameters:</p> Name Type Description Default <code>subj_cls</code> <code>type[T]</code> <p>The Pydantic model class to instantiate</p> required <code>obj</code> <code>str | Path | bytes</code> <p>Excel file path, file-like object, or bytes</p> required <code>many</code> <code>bool</code> <p>If True, convert all rows; if False, convert only first row</p> <code>True</code> <code>adapt_meth</code> <code>str</code> <p>Method name to use for model validation (default: \"model_validate\")</p> <code>'model_validate'</code> <code>sheet_name</code> <code>str | int</code> <p>Sheet name or index to read (default: 0)</p> <code>0</code> <code>**kw</code> <code>Any</code> <p>Additional arguments passed to pandas.read_excel</p> <code>{}</code> <p>Returns:</p> Type Description <code>T | list[T]</code> <p>List of model instances if many=True, single instance if many=False</p> <p>Raises:</p> Type Description <code>ResourceError</code> <p>If the Excel file cannot be read</p> <code>AdapterError</code> <p>If the data cannot be converted to models</p> Source code in <code>src/pydapter/extras/excel_.py</code> <pre><code>@classmethod\ndef from_obj(\n    cls,\n    subj_cls: type[T],\n    obj: str | Path | bytes,\n    /,\n    *,\n    many: bool = True,\n    adapt_meth: str = \"model_validate\",\n    sheet_name: str | int = 0,\n    adapt_kw: dict | None = None,\n    **kw: Any,\n) -&gt; T | list[T]:\n    \"\"\"\n    Convert Excel data to Pydantic model instances.\n\n    Args:\n        subj_cls: The Pydantic model class to instantiate\n        obj: Excel file path, file-like object, or bytes\n        many: If True, convert all rows; if False, convert only first row\n        adapt_meth: Method name to use for model validation (default: \"model_validate\")\n        sheet_name: Sheet name or index to read (default: 0)\n        **kw: Additional arguments passed to pandas.read_excel\n\n    Returns:\n        List of model instances if many=True, single instance if many=False\n\n    Raises:\n        ResourceError: If the Excel file cannot be read\n        AdapterError: If the data cannot be converted to models\n    \"\"\"\n    try:\n        if isinstance(obj, bytes):\n            df = pd.read_excel(io.BytesIO(obj), sheet_name=sheet_name, **kw)\n        else:\n            df = pd.read_excel(obj, sheet_name=sheet_name, **kw)\n        return DataFrameAdapter.from_obj(\n            subj_cls, df, many=many, adapt_meth=adapt_meth, adapt_kw=adapt_kw\n        )\n    except FileNotFoundError as e:\n        raise ResourceError(f\"File not found: {e}\", resource=str(obj)) from e\n    except ValueError as e:\n        raise AdapterError(\n            f\"Error adapting from xlsx (original_error='{e}')\", adapter=\"xlsx\"\n        ) from e\n    except Exception as e:\n        raise AdapterError(\n            f\"Unexpected error in Excel adapter: {e}\", adapter=\"xlsx\"\n        ) from e\n</code></pre>"},{"location":"api/extras/#pydapter.extras.excel_.ExcelAdapter.to_obj","title":"<code>to_obj(subj, /, *, many=True, adapt_meth='model_dump', adapt_kw=None, sheet_name='Sheet1', **kw)</code>  <code>classmethod</code>","text":"<p>Convert Pydantic model instances to Excel bytes.</p> <p>Parameters:</p> Name Type Description Default <code>subj</code> <code>T | list[T]</code> <p>Single model instance or list of instances</p> required <code>many</code> <code>bool</code> <p>If True, handle as multiple instances</p> <code>True</code> <code>adapt_meth</code> <code>str</code> <p>Method name to use for model dumping (default: \"model_dump\")</p> <code>'model_dump'</code> <code>sheet_name</code> <code>str</code> <p>Name of the Excel sheet (default: \"Sheet1\")</p> <code>'Sheet1'</code> <code>**kw</code> <code>Any</code> <p>Additional arguments passed to DataFrame constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>bytes</code> <p>Excel file content as bytes</p> Source code in <code>src/pydapter/extras/excel_.py</code> <pre><code>@classmethod\ndef to_obj(\n    cls,\n    subj: T | list[T],\n    /,\n    *,\n    many: bool = True,\n    adapt_meth: str = \"model_dump\",\n    adapt_kw: dict | None = None,\n    sheet_name: str = \"Sheet1\",\n    **kw: Any,\n) -&gt; bytes:\n    \"\"\"\n    Convert Pydantic model instances to Excel bytes.\n\n    Args:\n        subj: Single model instance or list of instances\n        many: If True, handle as multiple instances\n        adapt_meth: Method name to use for model dumping (default: \"model_dump\")\n        sheet_name: Name of the Excel sheet (default: \"Sheet1\")\n        **kw: Additional arguments passed to DataFrame constructor\n\n    Returns:\n        Excel file content as bytes\n    \"\"\"\n    df = DataFrameAdapter.to_obj(\n        subj, many=many, adapt_meth=adapt_meth, adapt_kw=adapt_kw, **kw\n    )\n    buf = io.BytesIO()\n    with pd.ExcelWriter(buf, engine=\"xlsxwriter\") as wr:\n        df.to_excel(wr, sheet_name=sheet_name, index=False)\n    return buf.getvalue()\n</code></pre>"},{"location":"api/extras/#pandas-adapter","title":"Pandas Adapter","text":""},{"location":"api/extras/#pydapter.extras.pandas_","title":"<code>pydapter.extras.pandas_</code>","text":"<p>DataFrame &amp; Series adapters (require <code>pandas</code>).</p>"},{"location":"api/extras/#pydapter.extras.pandas_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.pandas_.DataFrameAdapter","title":"<code>DataFrameAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Adapter for converting between Pydantic models and pandas DataFrames.</p> <p>This adapter handles pandas DataFrame objects, providing methods to: - Convert DataFrame rows to Pydantic model instances - Convert Pydantic models to DataFrame rows - Handle both single records and multiple records</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"pd.DataFrame\")</p> Example <pre><code>import pandas as pd\nfrom pydantic import BaseModel\nfrom pydapter.extras.pandas_ import DataFrameAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Create DataFrame\ndf = pd.DataFrame([\n    {\"name\": \"John\", \"age\": 30},\n    {\"name\": \"Jane\", \"age\": 25}\n])\n\n# Convert to Pydantic models\npeople = DataFrameAdapter.from_obj(Person, df, many=True)\n\n# Convert back to DataFrame\ndf_output = DataFrameAdapter.to_obj(people, many=True)\n</code></pre> Source code in <code>src/pydapter/extras/pandas_.py</code> <pre><code>class DataFrameAdapter(Adapter[T]):\n    \"\"\"\n    Adapter for converting between Pydantic models and pandas DataFrames.\n\n    This adapter handles pandas DataFrame objects, providing methods to:\n    - Convert DataFrame rows to Pydantic model instances\n    - Convert Pydantic models to DataFrame rows\n    - Handle both single records and multiple records\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"pd.DataFrame\")\n\n    Example:\n        ```python\n        import pandas as pd\n        from pydantic import BaseModel\n        from pydapter.extras.pandas_ import DataFrameAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Create DataFrame\n        df = pd.DataFrame([\n            {\"name\": \"John\", \"age\": 30},\n            {\"name\": \"Jane\", \"age\": 25}\n        ])\n\n        # Convert to Pydantic models\n        people = DataFrameAdapter.from_obj(Person, df, many=True)\n\n        # Convert back to DataFrame\n        df_output = DataFrameAdapter.to_obj(people, many=True)\n        ```\n    \"\"\"\n\n    obj_key = \"pd.DataFrame\"\n\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: pd.DataFrame,\n        /,\n        *,\n        many: bool = True,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw: Any,\n    ) -&gt; T | list[T]:\n        \"\"\"\n        Convert DataFrame to Pydantic model instances.\n\n        Args:\n            subj_cls: The Pydantic model class to instantiate\n            obj: The pandas DataFrame to convert\n            many: If True, convert all rows; if False, convert only first row\n            adapt_meth: Method name to call on subj_cls (default: \"model_validate\")\n            **kw: Additional arguments passed to the adaptation method\n\n        Returns:\n            List of model instances if many=True, single instance if many=False\n        \"\"\"\n        if many:\n            return [\n                getattr(subj_cls, adapt_meth)(r, **(adapt_kw or {}))\n                for r in obj.to_dict(orient=\"records\")\n            ]\n        return getattr(subj_cls, adapt_meth)(\n            obj.iloc[0].to_dict(**kw), **(adapt_kw or {})\n        )\n\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | list[T],\n        /,\n        *,\n        many: bool = True,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw: Any,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert Pydantic model instances to pandas DataFrame.\n\n        Args:\n            subj: Single model instance or list of instances\n            many: If True, handle as multiple instances\n            adapt_meth: Method name to call on model instances (default: \"model_dump\")\n            **kw: Additional arguments passed to DataFrame constructor\n\n        Returns:\n            pandas DataFrame with model data\n        \"\"\"\n        items = subj if isinstance(subj, list) else [subj]\n        return pd.DataFrame(\n            [getattr(i, adapt_meth)(**(adapt_kw or {})) for i in items], **kw\n        )\n</code></pre>"},{"location":"api/extras/#pydapter.extras.pandas_.DataFrameAdapter-functions","title":"Functions","text":""},{"location":"api/extras/#pydapter.extras.pandas_.DataFrameAdapter.from_obj","title":"<code>from_obj(subj_cls, obj, /, *, many=True, adapt_meth='model_validate', adapt_kw=None, **kw)</code>  <code>classmethod</code>","text":"<p>Convert DataFrame to Pydantic model instances.</p> <p>Parameters:</p> Name Type Description Default <code>subj_cls</code> <code>type[T]</code> <p>The Pydantic model class to instantiate</p> required <code>obj</code> <code>DataFrame</code> <p>The pandas DataFrame to convert</p> required <code>many</code> <code>bool</code> <p>If True, convert all rows; if False, convert only first row</p> <code>True</code> <code>adapt_meth</code> <code>str</code> <p>Method name to call on subj_cls (default: \"model_validate\")</p> <code>'model_validate'</code> <code>**kw</code> <code>Any</code> <p>Additional arguments passed to the adaptation method</p> <code>{}</code> <p>Returns:</p> Type Description <code>T | list[T]</code> <p>List of model instances if many=True, single instance if many=False</p> Source code in <code>src/pydapter/extras/pandas_.py</code> <pre><code>@classmethod\ndef from_obj(\n    cls,\n    subj_cls: type[T],\n    obj: pd.DataFrame,\n    /,\n    *,\n    many: bool = True,\n    adapt_meth: str = \"model_validate\",\n    adapt_kw: dict | None = None,\n    **kw: Any,\n) -&gt; T | list[T]:\n    \"\"\"\n    Convert DataFrame to Pydantic model instances.\n\n    Args:\n        subj_cls: The Pydantic model class to instantiate\n        obj: The pandas DataFrame to convert\n        many: If True, convert all rows; if False, convert only first row\n        adapt_meth: Method name to call on subj_cls (default: \"model_validate\")\n        **kw: Additional arguments passed to the adaptation method\n\n    Returns:\n        List of model instances if many=True, single instance if many=False\n    \"\"\"\n    if many:\n        return [\n            getattr(subj_cls, adapt_meth)(r, **(adapt_kw or {}))\n            for r in obj.to_dict(orient=\"records\")\n        ]\n    return getattr(subj_cls, adapt_meth)(\n        obj.iloc[0].to_dict(**kw), **(adapt_kw or {})\n    )\n</code></pre>"},{"location":"api/extras/#pydapter.extras.pandas_.DataFrameAdapter.to_obj","title":"<code>to_obj(subj, /, *, many=True, adapt_meth='model_dump', adapt_kw=None, **kw)</code>  <code>classmethod</code>","text":"<p>Convert Pydantic model instances to pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>subj</code> <code>T | list[T]</code> <p>Single model instance or list of instances</p> required <code>many</code> <code>bool</code> <p>If True, handle as multiple instances</p> <code>True</code> <code>adapt_meth</code> <code>str</code> <p>Method name to call on model instances (default: \"model_dump\")</p> <code>'model_dump'</code> <code>**kw</code> <code>Any</code> <p>Additional arguments passed to DataFrame constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas DataFrame with model data</p> Source code in <code>src/pydapter/extras/pandas_.py</code> <pre><code>@classmethod\ndef to_obj(\n    cls,\n    subj: T | list[T],\n    /,\n    *,\n    many: bool = True,\n    adapt_meth: str = \"model_dump\",\n    adapt_kw: dict | None = None,\n    **kw: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert Pydantic model instances to pandas DataFrame.\n\n    Args:\n        subj: Single model instance or list of instances\n        many: If True, handle as multiple instances\n        adapt_meth: Method name to call on model instances (default: \"model_dump\")\n        **kw: Additional arguments passed to DataFrame constructor\n\n    Returns:\n        pandas DataFrame with model data\n    \"\"\"\n    items = subj if isinstance(subj, list) else [subj]\n    return pd.DataFrame(\n        [getattr(i, adapt_meth)(**(adapt_kw or {})) for i in items], **kw\n    )\n</code></pre>"},{"location":"api/extras/#pydapter.extras.pandas_.SeriesAdapter","title":"<code>SeriesAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Adapter for converting between Pydantic models and pandas Series.</p> <p>This adapter handles pandas Series objects, providing methods to: - Convert Series to a single Pydantic model instance - Convert Pydantic model to Series - Only supports single records (many=False)</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"pd.Series\")</p> Example <pre><code>import pandas as pd\nfrom pydantic import BaseModel\nfrom pydapter.extras.pandas_ import SeriesAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Create Series\nseries = pd.Series({\"name\": \"John\", \"age\": 30})\n\n# Convert to Pydantic model\nperson = SeriesAdapter.from_obj(Person, series)\n\n# Convert back to Series\nseries_output = SeriesAdapter.to_obj(person)\n</code></pre> Source code in <code>src/pydapter/extras/pandas_.py</code> <pre><code>class SeriesAdapter(Adapter[T]):\n    \"\"\"\n    Adapter for converting between Pydantic models and pandas Series.\n\n    This adapter handles pandas Series objects, providing methods to:\n    - Convert Series to a single Pydantic model instance\n    - Convert Pydantic model to Series\n    - Only supports single records (many=False)\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"pd.Series\")\n\n    Example:\n        ```python\n        import pandas as pd\n        from pydantic import BaseModel\n        from pydapter.extras.pandas_ import SeriesAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Create Series\n        series = pd.Series({\"name\": \"John\", \"age\": 30})\n\n        # Convert to Pydantic model\n        person = SeriesAdapter.from_obj(Person, series)\n\n        # Convert back to Series\n        series_output = SeriesAdapter.to_obj(person)\n        ```\n    \"\"\"\n\n    obj_key = \"pd.Series\"\n\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: pd.Series,\n        /,\n        *,\n        many: bool = False,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw: Any,\n    ) -&gt; T:\n        \"\"\"\n        Convert pandas Series to Pydantic model instance.\n\n        Args:\n            subj_cls: The Pydantic model class to instantiate\n            obj: The pandas Series to convert\n            many: Must be False (Series only supports single records)\n            adapt_meth: Method name to call on subj_cls (default: \"model_validate\")\n            **kw: Additional arguments passed to the adaptation method\n\n        Returns:\n            Single model instance\n\n        Raises:\n            ValueError: If many=True is specified\n        \"\"\"\n        if many:\n            raise ValueError(\"SeriesAdapter supports single records only.\")\n        return getattr(subj_cls, adapt_meth)(obj.to_dict(**kw), **(adapt_kw or {}))\n\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | list[T],\n        /,\n        *,\n        many: bool = False,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw: Any,\n    ) -&gt; pd.Series:\n        if many or isinstance(subj, list):\n            raise ValueError(\"SeriesAdapter supports single records only.\")\n        return pd.Series(getattr(subj, adapt_meth)(**(adapt_kw or {})), **kw)\n</code></pre>"},{"location":"api/extras/#pydapter.extras.pandas_.SeriesAdapter-functions","title":"Functions","text":""},{"location":"api/extras/#pydapter.extras.pandas_.SeriesAdapter.from_obj","title":"<code>from_obj(subj_cls, obj, /, *, many=False, adapt_meth='model_validate', adapt_kw=None, **kw)</code>  <code>classmethod</code>","text":"<p>Convert pandas Series to Pydantic model instance.</p> <p>Parameters:</p> Name Type Description Default <code>subj_cls</code> <code>type[T]</code> <p>The Pydantic model class to instantiate</p> required <code>obj</code> <code>Series</code> <p>The pandas Series to convert</p> required <code>many</code> <code>bool</code> <p>Must be False (Series only supports single records)</p> <code>False</code> <code>adapt_meth</code> <code>str</code> <p>Method name to call on subj_cls (default: \"model_validate\")</p> <code>'model_validate'</code> <code>**kw</code> <code>Any</code> <p>Additional arguments passed to the adaptation method</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>Single model instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If many=True is specified</p> Source code in <code>src/pydapter/extras/pandas_.py</code> <pre><code>@classmethod\ndef from_obj(\n    cls,\n    subj_cls: type[T],\n    obj: pd.Series,\n    /,\n    *,\n    many: bool = False,\n    adapt_meth: str = \"model_validate\",\n    adapt_kw: dict | None = None,\n    **kw: Any,\n) -&gt; T:\n    \"\"\"\n    Convert pandas Series to Pydantic model instance.\n\n    Args:\n        subj_cls: The Pydantic model class to instantiate\n        obj: The pandas Series to convert\n        many: Must be False (Series only supports single records)\n        adapt_meth: Method name to call on subj_cls (default: \"model_validate\")\n        **kw: Additional arguments passed to the adaptation method\n\n    Returns:\n        Single model instance\n\n    Raises:\n        ValueError: If many=True is specified\n    \"\"\"\n    if many:\n        raise ValueError(\"SeriesAdapter supports single records only.\")\n    return getattr(subj_cls, adapt_meth)(obj.to_dict(**kw), **(adapt_kw or {}))\n</code></pre>"},{"location":"api/extras/#sql-adapter","title":"SQL Adapter","text":""},{"location":"api/extras/#pydapter.extras.sql_","title":"<code>pydapter.extras.sql_</code>","text":"<p>Generic SQL adapter using SQLAlchemy Core (requires <code>sqlalchemy&gt;=2.0</code>).</p>"},{"location":"api/extras/#pydapter.extras.sql_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.sql_.SQLAdapter","title":"<code>SQLAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Generic SQL adapter using SQLAlchemy Core for database operations.</p> <p>This adapter provides methods to: - Execute SQL queries and convert results to Pydantic models - Insert Pydantic models as rows into database tables - Support for various SQL databases through SQLAlchemy - Handle both raw SQL and table-based operations</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"sql\")</p> Example <pre><code>import sqlalchemy as sa\nfrom pydantic import BaseModel\nfrom pydapter.extras.sql_ import SQLAdapter\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\n# Setup database connection\nengine = sa.create_engine(\"sqlite:///example.db\")\nmetadata = sa.MetaData()\n\n# Query from database\nquery = \"SELECT id, name, email FROM users WHERE active = true\"\nusers = SQLAdapter.from_obj(\n    User,\n    query,\n    many=True,\n    engine=engine\n)\n\n# Insert to database\nnew_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\nSQLAdapter.to_obj(\n    new_users,\n    many=True,\n    table=\"users\",\n    metadata=metadata\n)\n</code></pre> Source code in <code>src/pydapter/extras/sql_.py</code> <pre><code>class SQLAdapter(Adapter[T]):\n    \"\"\"\n    Generic SQL adapter using SQLAlchemy Core for database operations.\n\n    This adapter provides methods to:\n    - Execute SQL queries and convert results to Pydantic models\n    - Insert Pydantic models as rows into database tables\n    - Support for various SQL databases through SQLAlchemy\n    - Handle both raw SQL and table-based operations\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"sql\")\n\n    Example:\n        ```python\n        import sqlalchemy as sa\n        from pydantic import BaseModel\n        from pydapter.extras.sql_ import SQLAdapter\n\n        class User(BaseModel):\n            id: int\n            name: str\n            email: str\n\n        # Setup database connection\n        engine = sa.create_engine(\"sqlite:///example.db\")\n        metadata = sa.MetaData()\n\n        # Query from database\n        query = \"SELECT id, name, email FROM users WHERE active = true\"\n        users = SQLAdapter.from_obj(\n            User,\n            query,\n            many=True,\n            engine=engine\n        )\n\n        # Insert to database\n        new_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\n        SQLAdapter.to_obj(\n            new_users,\n            many=True,\n            table=\"users\",\n            metadata=metadata\n        )\n        ```\n    \"\"\"\n\n    obj_key = \"sql\"\n\n    @staticmethod\n    def _table(metadata: sa.MetaData, table: str, engine=None) -&gt; sa.Table:\n        \"\"\"\n        Helper method to get a SQLAlchemy Table object with autoloading.\n\n        Args:\n            metadata: SQLAlchemy MetaData instance\n            table: Name of the table to load\n            engine: Optional SQLAlchemy engine for autoloading\n\n        Returns:\n            SQLAlchemy Table object\n\n        Raises:\n            ResourceError: If table is not found or cannot be accessed\n        \"\"\"\n        try:\n            # Use engine if provided, otherwise use metadata.bind\n            autoload_with = engine if engine is not None else metadata.bind  # type: ignore\n            return sa.Table(table, metadata, autoload_with=autoload_with)\n        except sq_exc.NoSuchTableError as e:\n            raise ResourceError(f\"Table '{table}' not found\", resource=table) from e\n        except Exception as e:\n            raise ResourceError(\n                f\"Error accessing table '{table}': {e}\", resource=table\n            ) from e\n\n    # ---- incoming\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: dict,\n        /,\n        *,\n        many: bool = True,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if \"engine_url\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'engine_url'\", data=obj\n                )\n            if \"table\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'table'\", data=obj\n                )\n\n            # Create engine and connect to database\n            try:\n                eng = sa.create_engine(obj[\"engine_url\"], future=True)\n            except Exception as e:\n                raise ConnectionError(\n                    f\"Failed to create database engine: {e}\",\n                    adapter=\"sql\",\n                    url=obj[\"engine_url\"],\n                ) from e\n\n            # Create metadata and get table\n            try:\n                md = sa.MetaData()\n                md.reflect(bind=eng)\n                tbl = cls._table(md, obj[\"table\"], engine=eng)\n            except ResourceError:\n                # Re-raise ResourceError from _table\n                raise\n            except Exception as e:\n                raise ResourceError(\n                    f\"Error accessing table metadata: {e}\",\n                    resource=obj[\"table\"],\n                ) from e\n\n            # Build query\n            stmt = sa.select(tbl).filter_by(**obj.get(\"selectors\", {}))\n\n            # Execute query\n            try:\n                with eng.begin() as conn:\n                    rows = conn.execute(stmt).fetchall()\n            except Exception as e:\n                raise QueryError(\n                    f\"Error executing query: {e}\",\n                    query=str(stmt),\n                    adapter=\"sql\",\n                ) from e\n\n            # Handle empty result set\n            if not rows:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No rows found matching the query\",\n                    resource=obj[\"table\"],\n                    selectors=obj.get(\"selectors\", {}),\n                )\n\n            # Convert rows to model instances\n            try:\n                validate_func = getattr(subj_cls, adapt_meth)\n                if many:\n                    return [validate_func(r._mapping, **(adapt_kw or {})) for r in rows]\n                return validate_func(rows[0]._mapping, **(adapt_kw or {}))\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=rows[0]._mapping if not many else [r._mapping for r in rows],\n                    errors=e.errors(),\n                ) from e\n\n        except AdapterError:\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(f\"Unexpected error in SQL adapter: {e}\", adapter=\"sql\")\n\n    # ---- outgoing\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | Sequence[T],\n        /,\n        *,\n        engine_url: str,\n        table: str,\n        many: bool = True,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ) -&gt; dict[str, Any]:\n        try:\n            # Validate required parameters\n            if not engine_url:\n                raise AdapterValidationError(\"Missing required parameter 'engine_url'\")\n            if not table:\n                raise AdapterValidationError(\"Missing required parameter 'table'\")\n\n            # Create engine and connect to database\n            try:\n                eng = sa.create_engine(engine_url, future=True)\n            except Exception as e:\n                raise ConnectionError(\n                    f\"Failed to create database engine: {e}\",\n                    adapter=\"sql\",\n                    url=engine_url,\n                ) from e\n\n            # Create metadata and get table\n            try:\n                md = sa.MetaData()\n                md.reflect(bind=eng)\n                tbl = cls._table(md, table, engine=eng)\n            except ResourceError:\n                # Re-raise ResourceError from _table\n                raise\n            except Exception as e:\n                raise ResourceError(\n                    f\"Error accessing table metadata: {e}\",\n                    resource=table,\n                ) from e\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return {\"success\": True, \"count\": 0}  # Nothing to insert\n\n            rows = [getattr(i, adapt_meth)(**(adapt_kw or {})) for i in items]\n\n            # Execute insert or update (upsert)\n            try:\n                with eng.begin() as conn:\n                    # Get primary key columns\n                    pk_columns = [c.name for c in tbl.primary_key.columns]\n\n                    if not pk_columns:\n                        # If no primary key, just insert\n                        conn.execute(sa.insert(tbl), rows)\n                    else:\n                        # For PostgreSQL, use ON CONFLICT DO UPDATE\n                        for row in rows:\n                            # Build the values to update (excluding primary key columns)\n                            update_values = {\n                                k: v for k, v in row.items() if k not in pk_columns\n                            }\n                            if not update_values:\n                                # If only primary key columns, just try to insert\n                                stmt = sa.insert(tbl).values(**row)\n                            else:\n                                # Otherwise, do an upsert\n                                stmt = postgresql.insert(tbl).values(**row)\n                                stmt = stmt.on_conflict_do_update(\n                                    index_elements=pk_columns, set_=update_values\n                                )\n                            conn.execute(stmt)\n            except Exception as e:\n                raise QueryError(\n                    f\"Error executing insert/update: {e}\",\n                    query=f\"UPSERT INTO {table}\",\n                    adapter=\"sql\",\n                ) from e\n\n            # Return a success indicator instead of None\n            return {\"success\": True, \"count\": len(rows)}\n\n        except AdapterError:\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(f\"Unexpected error in SQL adapter: {e}\", adapter=\"sql\")\n</code></pre>"},{"location":"api/extras/#postgresql-adapter","title":"PostgreSQL Adapter","text":""},{"location":"api/extras/#pydapter.extras.postgres_","title":"<code>pydapter.extras.postgres_</code>","text":"<p>PostgresAdapter - thin preset over SQLAdapter (pgvector-ready if you add vec column).</p>"},{"location":"api/extras/#pydapter.extras.postgres_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.postgres_.PostgresAdapter","title":"<code>PostgresAdapter</code>","text":"<p>               Bases: <code>SQLAdapter[T]</code></p> <p>PostgreSQL-specific adapter extending SQLAdapter with PostgreSQL optimizations.</p> <p>This adapter provides: - PostgreSQL-specific connection handling and error messages - Default PostgreSQL connection string - Enhanced error handling for common PostgreSQL issues - Support for pgvector when vector columns are present</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"postgres\")</p> <code>DEFAULT</code> <p>Default PostgreSQL connection string</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.extras.postgres_ import PostgresAdapter\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\n# Query with custom connection\nquery_config = {\n    \"query\": \"SELECT id, name, email FROM users WHERE active = true\",\n    \"engine_url\": \"postgresql+psycopg://user:pass@localhost/mydb\"\n}\nusers = PostgresAdapter.from_obj(User, query_config, many=True)\n\n# Insert with default connection\ninsert_config = {\n    \"table\": \"users\",\n    \"engine_url\": \"postgresql+psycopg://user:pass@localhost/mydb\"\n}\nnew_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\nPostgresAdapter.to_obj(new_users, insert_config, many=True)\n</code></pre> Source code in <code>src/pydapter/extras/postgres_.py</code> <pre><code>class PostgresAdapter(SQLAdapter[T]):\n    \"\"\"\n    PostgreSQL-specific adapter extending SQLAdapter with PostgreSQL optimizations.\n\n    This adapter provides:\n    - PostgreSQL-specific connection handling and error messages\n    - Default PostgreSQL connection string\n    - Enhanced error handling for common PostgreSQL issues\n    - Support for pgvector when vector columns are present\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"postgres\")\n        DEFAULT: Default PostgreSQL connection string\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.extras.postgres_ import PostgresAdapter\n\n        class User(BaseModel):\n            id: int\n            name: str\n            email: str\n\n        # Query with custom connection\n        query_config = {\n            \"query\": \"SELECT id, name, email FROM users WHERE active = true\",\n            \"engine_url\": \"postgresql+psycopg://user:pass@localhost/mydb\"\n        }\n        users = PostgresAdapter.from_obj(User, query_config, many=True)\n\n        # Insert with default connection\n        insert_config = {\n            \"table\": \"users\",\n            \"engine_url\": \"postgresql+psycopg://user:pass@localhost/mydb\"\n        }\n        new_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\n        PostgresAdapter.to_obj(new_users, insert_config, many=True)\n        ```\n    \"\"\"\n\n    obj_key = \"postgres\"\n    DEFAULT = \"postgresql+psycopg://user:pass@localhost/db\"\n\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls,\n        obj: dict,\n        /,\n        *,\n        many: bool = True,\n        adapt_meth: str = \"model_validate\",\n        **kw,\n    ):\n        try:\n            # Set default connection string if not provided\n            obj.setdefault(\"engine_url\", cls.DEFAULT)\n\n            # Add PostgreSQL-specific error handling\n            try:\n                return super().from_obj(\n                    subj_cls, obj, many=many, adapt_meth=adapt_meth, **kw\n                )\n            except Exception as e:\n                # Check for common PostgreSQL-specific errors\n                error_str = str(e).lower()\n                if \"authentication\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL authentication failed: {e}\",\n                        adapter=\"postgres\",\n                        url=obj[\"engine_url\"],\n                    ) from e\n                elif \"connection\" in error_str and \"refused\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL connection refused: {e}\",\n                        adapter=\"postgres\",\n                        url=obj[\"engine_url\"],\n                    ) from e\n                elif \"does not exist\" in error_str and \"database\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL database does not exist: {e}\",\n                        adapter=\"postgres\",\n                        url=obj[\"engine_url\"],\n                    ) from e\n                # Re-raise the original exception\n                raise\n\n        except ConnectionError:\n            # Re-raise ConnectionError\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ConnectionError(\n                f\"Unexpected error in PostgreSQL adapter: {e}\",\n                adapter=\"postgres\",\n                url=obj.get(\"engine_url\", cls.DEFAULT),\n            ) from e\n\n    @classmethod\n    def to_obj(\n        cls, subj, /, *, many: bool = True, adapt_meth: str = \"model_dump\", **kw\n    ):\n        try:\n            # Set default connection string if not provided\n            kw.setdefault(\"engine_url\", cls.DEFAULT)\n\n            # Add PostgreSQL-specific error handling\n            try:\n                return super().to_obj(subj, many=many, adapt_meth=adapt_meth, **kw)\n            except Exception as e:\n                # Check for common PostgreSQL-specific errors\n                error_str = str(e).lower()\n                if \"authentication\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL authentication failed: {e}\",\n                        adapter=\"postgres\",\n                        url=kw[\"engine_url\"],\n                    ) from e\n                elif \"connection\" in error_str and \"refused\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL connection refused: {e}\",\n                        adapter=\"postgres\",\n                        url=kw[\"engine_url\"],\n                    ) from e\n                elif \"does not exist\" in error_str and \"database\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL database does not exist: {e}\",\n                        adapter=\"postgres\",\n                        url=kw[\"engine_url\"],\n                    ) from e\n                # Re-raise the original exception\n                raise\n\n        except ConnectionError:\n            # Re-raise ConnectionError\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ConnectionError(\n                f\"Unexpected error in PostgreSQL adapter: {e}\",\n                adapter=\"postgres\",\n                url=kw.get(\"engine_url\", cls.DEFAULT),\n            ) from e\n</code></pre>"},{"location":"api/extras/#mongodb-adapter","title":"MongoDB Adapter","text":""},{"location":"api/extras/#pydapter.extras.mongo_","title":"<code>pydapter.extras.mongo_</code>","text":"<p>MongoDB adapter (requires <code>pymongo</code>).</p>"},{"location":"api/extras/#pydapter.extras.mongo_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.mongo_.MongoAdapter","title":"<code>MongoAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>MongoDB adapter for converting between Pydantic models and MongoDB documents.</p> <p>This adapter provides methods to: - Query MongoDB collections and convert documents to Pydantic models - Insert Pydantic models as documents into MongoDB collections - Handle MongoDB connection management and error handling - Support for various MongoDB operations (find, insert, update, delete)</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"mongo\")</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.extras.mongo_ import MongoAdapter\n\nclass User(BaseModel):\n    name: str\n    email: str\n    age: int\n\n# Query from MongoDB\nquery_config = {\n    \"url\": \"mongodb://localhost:27017\",\n    \"database\": \"myapp\",\n    \"collection\": \"users\",\n    \"filter\": {\"age\": {\"$gte\": 18}}\n}\nusers = MongoAdapter.from_obj(User, query_config, many=True)\n\n# Insert to MongoDB\ninsert_config = {\n    \"url\": \"mongodb://localhost:27017\",\n    \"database\": \"myapp\",\n    \"collection\": \"users\"\n}\nnew_users = [User(name=\"John\", email=\"john@example.com\", age=30)]\nMongoAdapter.to_obj(new_users, insert_config, many=True)\n</code></pre> Source code in <code>src/pydapter/extras/mongo_.py</code> <pre><code>class MongoAdapter(Adapter[T]):\n    \"\"\"\n    MongoDB adapter for converting between Pydantic models and MongoDB documents.\n\n    This adapter provides methods to:\n    - Query MongoDB collections and convert documents to Pydantic models\n    - Insert Pydantic models as documents into MongoDB collections\n    - Handle MongoDB connection management and error handling\n    - Support for various MongoDB operations (find, insert, update, delete)\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"mongo\")\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.extras.mongo_ import MongoAdapter\n\n        class User(BaseModel):\n            name: str\n            email: str\n            age: int\n\n        # Query from MongoDB\n        query_config = {\n            \"url\": \"mongodb://localhost:27017\",\n            \"database\": \"myapp\",\n            \"collection\": \"users\",\n            \"filter\": {\"age\": {\"$gte\": 18}}\n        }\n        users = MongoAdapter.from_obj(User, query_config, many=True)\n\n        # Insert to MongoDB\n        insert_config = {\n            \"url\": \"mongodb://localhost:27017\",\n            \"database\": \"myapp\",\n            \"collection\": \"users\"\n        }\n        new_users = [User(name=\"John\", email=\"john@example.com\", age=30)]\n        MongoAdapter.to_obj(new_users, insert_config, many=True)\n        ```\n    \"\"\"\n\n    obj_key = \"mongo\"\n\n    @classmethod\n    def _client(cls, url: str) -&gt; pymongo.MongoClient:\n        \"\"\"\n        Create a MongoDB client with proper error handling.\n\n        Args:\n            url: MongoDB connection string\n\n        Returns:\n            pymongo.MongoClient instance\n\n        Raises:\n            ConnectionError: If connection cannot be established\n        \"\"\"\n        try:\n            return pymongo.MongoClient(url, serverSelectionTimeoutMS=5000)\n        except pymongo.errors.ConfigurationError as e:\n            raise ConnectionError(\n                f\"Invalid MongoDB connection string: {e}\", adapter=\"mongo\", url=url\n            ) from e\n        except Exception as e:\n            raise ConnectionError(\n                f\"Failed to create MongoDB client: {e}\", adapter=\"mongo\", url=url\n            ) from e\n\n    @classmethod\n    def _validate_connection(cls, client: pymongo.MongoClient) -&gt; None:\n        \"\"\"Validate that the MongoDB connection is working.\"\"\"\n        try:\n            # This will raise an exception if the connection fails\n            client.admin.command(\"ping\")\n        except pymongo.errors.ServerSelectionTimeoutError as e:\n            raise ConnectionError(\n                f\"MongoDB server selection timeout: {e}\", adapter=\"mongo\"\n            ) from e\n        except pymongo.errors.OperationFailure as e:\n            if \"auth failed\" in str(e).lower():\n                raise ConnectionError(\n                    f\"MongoDB authentication failed: {e}\", adapter=\"mongo\"\n                ) from e\n            raise QueryError(f\"MongoDB operation failure: {e}\", adapter=\"mongo\") from e\n        except Exception as e:\n            raise ConnectionError(\n                f\"Failed to connect to MongoDB: {e}\", adapter=\"mongo\"\n            ) from e\n\n    # incoming\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: dict,\n        /,\n        *,\n        many=True,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if \"url\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'url'\", data=obj\n                )\n            if \"db\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'db'\", data=obj\n                )\n            if \"collection\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'collection'\", data=obj\n                )\n\n            # Create client and validate connection\n            client = cls._client(obj[\"url\"])\n            cls._validate_connection(client)\n\n            # Get collection and execute query\n            try:\n                coll = client[obj[\"db\"]][obj[\"collection\"]]\n                filter_query = obj.get(\"filter\") or {}\n\n                # Validate filter query if provided\n                if filter_query and not isinstance(filter_query, dict):\n                    raise AdapterValidationError(\n                        \"Filter must be a dictionary\",\n                        data=filter_query,\n                    )\n\n                docs = list(coll.find(filter_query))\n            except pymongo.errors.OperationFailure as e:\n                if \"not authorized\" in str(e).lower():\n                    raise ConnectionError(\n                        f\"Not authorized to access {obj['db']}.{obj['collection']}: {e}\",\n                        adapter=\"mongo\",\n                        url=obj[\"url\"],\n                    ) from e\n                raise QueryError(\n                    f\"MongoDB query error: {e}\",\n                    query=filter_query,\n                    adapter=\"mongo\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Error executing MongoDB query: {e}\",\n                    query=filter_query,\n                    adapter=\"mongo\",\n                ) from e\n\n            # Handle empty result set\n            if not docs:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No documents found matching the query\",\n                    resource=f\"{obj['db']}.{obj['collection']}\",\n                    filter=filter_query,\n                )\n\n            # Convert documents to model instances\n            try:\n                if many:\n                    return [\n                        getattr(subj_cls, adapt_meth)(d, **(adapt_kw or {}))\n                        for d in docs\n                    ]\n                return getattr(subj_cls, adapt_meth)(docs[0], **(adapt_kw or {}))\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=docs[0] if not many else docs,\n                    errors=e.errors(),\n                ) from e\n\n        except AdapterError:\n            raise\n\n        except Exception as e:\n            raise QueryError(\n                f\"Unexpected error in MongoDB adapter: {e}\", adapter=\"mongo\"\n            )\n\n    # outgoing\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | Sequence[T],\n        /,\n        *,\n        url,\n        db,\n        collection,\n        many=True,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if not url:\n                raise AdapterValidationError(\"Missing required parameter 'url'\")\n            if not db:\n                raise AdapterValidationError(\"Missing required parameter 'db'\")\n            if not collection:\n                raise AdapterValidationError(\"Missing required parameter 'collection'\")\n\n            # Create client and validate connection\n            client = cls._client(url)\n            cls._validate_connection(client)\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return None  # Nothing to insert\n\n            payload = [getattr(i, adapt_meth)(**(adapt_kw or {})) for i in items]\n\n            # Execute insert\n            try:\n                result = client[db][collection].insert_many(payload)\n                return {\"inserted_count\": result.inserted_ids}\n            except pymongo.errors.BulkWriteError as e:\n                raise QueryError(\n                    f\"MongoDB bulk write error: {e}\",\n                    adapter=\"mongo\",\n                ) from e\n            except pymongo.errors.OperationFailure as e:\n                if \"not authorized\" in str(e).lower():\n                    raise ConnectionError(\n                        f\"Not authorized to write to {db}.{collection}: {e}\",\n                        adapter=\"mongo\",\n                        url=url,\n                    ) from e\n                raise QueryError(\n                    f\"MongoDB operation failure: {e}\",\n                    adapter=\"mongo\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Error inserting documents into MongoDB: {e}\",\n                    adapter=\"mongo\",\n                ) from e\n\n        except AdapterError:\n            raise\n\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(\n                f\"Unexpected error in MongoDB adapter: {e}\", adapter=\"mongo\"\n            )\n</code></pre>"},{"location":"api/extras/#neo4j-adapter","title":"Neo4j Adapter","text":""},{"location":"api/extras/#pydapter.extras.neo4j_","title":"<code>pydapter.extras.neo4j_</code>","text":"<p>Neo4j adapter (requires <code>neo4j</code>).</p>"},{"location":"api/extras/#pydapter.extras.neo4j_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.neo4j_.Neo4jAdapter","title":"<code>Neo4jAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Neo4j graph database adapter for converting between Pydantic models and Neo4j nodes/relationships.</p> <p>This adapter provides methods to: - Execute Cypher queries and convert results to Pydantic models - Create nodes and relationships from Pydantic models - Handle Neo4j connection management and error handling - Support for complex graph operations and traversals</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"neo4j\")</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\nfrom neo4j import basic_auth\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    city: str\n\n# Query from Neo4j\nquery_config = {\n    \"url\": \"bolt://localhost:7687\",\n    \"auth\": basic_auth(\"neo4j\", \"password\"),\n    \"query\": \"MATCH (p:Person) WHERE p.age &gt;= 18 RETURN p.name, p.age, p.city\"\n}\npeople = Neo4jAdapter.from_obj(Person, query_config, many=True)\n\n# Create nodes in Neo4j\ncreate_config = {\n    \"url\": \"bolt://localhost:7687\",\n    \"auth\": basic_auth(\"neo4j\", \"password\"),\n    \"query\": \"CREATE (p:Person {name: $name, age: $age, city: $city})\"\n}\nnew_people = [Person(name=\"John\", age=30, city=\"NYC\")]\nNeo4jAdapter.to_obj(new_people, create_config, many=True)\n</code></pre> Source code in <code>src/pydapter/extras/neo4j_.py</code> <pre><code>class Neo4jAdapter(Adapter[T]):\n    \"\"\"\n    Neo4j graph database adapter for converting between Pydantic models and Neo4j nodes/relationships.\n\n    This adapter provides methods to:\n    - Execute Cypher queries and convert results to Pydantic models\n    - Create nodes and relationships from Pydantic models\n    - Handle Neo4j connection management and error handling\n    - Support for complex graph operations and traversals\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"neo4j\")\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.extras.neo4j_ import Neo4jAdapter\n        from neo4j import basic_auth\n\n        class Person(BaseModel):\n            name: str\n            age: int\n            city: str\n\n        # Query from Neo4j\n        query_config = {\n            \"url\": \"bolt://localhost:7687\",\n            \"auth\": basic_auth(\"neo4j\", \"password\"),\n            \"query\": \"MATCH (p:Person) WHERE p.age &gt;= 18 RETURN p.name, p.age, p.city\"\n        }\n        people = Neo4jAdapter.from_obj(Person, query_config, many=True)\n\n        # Create nodes in Neo4j\n        create_config = {\n            \"url\": \"bolt://localhost:7687\",\n            \"auth\": basic_auth(\"neo4j\", \"password\"),\n            \"query\": \"CREATE (p:Person {name: $name, age: $age, city: $city})\"\n        }\n        new_people = [Person(name=\"John\", age=30, city=\"NYC\")]\n        Neo4jAdapter.to_obj(new_people, create_config, many=True)\n        ```\n    \"\"\"\n\n    obj_key = \"neo4j\"\n\n    @classmethod\n    def _create_driver(cls, url: str, auth=None) -&gt; neo4j.Driver:\n        \"\"\"\n        Create a Neo4j driver with proper error handling.\n\n        Args:\n            url: Neo4j connection URL (e.g., \"bolt://localhost:7687\")\n            auth: Authentication tuple or None for no auth\n\n        Returns:\n            neo4j.Driver instance\n\n        Raises:\n            ConnectionError: If connection cannot be established or auth fails\n        \"\"\"\n        try:\n            if auth:\n                return GraphDatabase.driver(url, auth=auth)\n            else:\n                return GraphDatabase.driver(url)\n        except neo4j.exceptions.ServiceUnavailable as e:\n            raise ConnectionError(\n                f\"Neo4j service unavailable: {e}\", adapter=\"neo4j\", url=url\n            ) from e\n        except neo4j.exceptions.AuthError as e:\n            raise ConnectionError(\n                f\"Neo4j authentication failed: {e}\", adapter=\"neo4j\", url=url\n            ) from e\n        except Exception as e:\n            raise ConnectionError(\n                f\"Failed to create Neo4j driver: {e}\", adapter=\"neo4j\", url=url\n            ) from e\n\n    @classmethod\n    def _validate_cypher(cls, cypher: str) -&gt; None:\n        \"\"\"Basic validation for Cypher queries to prevent injection.\"\"\"\n        # Check for unescaped backticks in label names\n        if re.search(r\"`[^`]*`[^`]*`\", cypher):\n            raise QueryError(\n                \"Invalid Cypher query: Possible injection in label name\",\n                query=cypher,\n                adapter=\"neo4j\",\n            )\n\n    # incoming\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: dict,\n        /,\n        *,\n        many=True,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if \"url\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'url'\", data=obj\n                )\n\n            # Create driver\n            auth = obj.get(\"auth\")\n            driver = cls._create_driver(obj[\"url\"], auth=auth)\n\n            # Prepare Cypher query\n            label = obj.get(\"label\", subj_cls.__name__)\n            where = f\"WHERE {obj['where']}\" if \"where\" in obj else \"\"\n            cypher = f\"MATCH (n:`{label}`) {where} RETURN n\"\n\n            # Validate Cypher query\n            cls._validate_cypher(cypher)\n\n            # Execute query\n            try:\n                with driver.session() as s:\n                    result = s.run(cypher)\n                    rows = [r[\"n\"]._properties for r in result]\n            except neo4j.exceptions.CypherSyntaxError as e:\n                raise QueryError(\n                    f\"Neo4j Cypher syntax error: {e}\",\n                    query=cypher,\n                    adapter=\"neo4j\",\n                ) from e\n            except neo4j.exceptions.ClientError as e:\n                if \"not found\" in str(e).lower():\n                    raise ResourceError(\n                        f\"Neo4j resource not found: {e}\",\n                        resource=label,\n                    ) from e\n                raise QueryError(\n                    f\"Neo4j client error: {e}\",\n                    query=cypher,\n                    adapter=\"neo4j\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Error executing Neo4j query: {e}\",\n                    query=cypher,\n                    adapter=\"neo4j\",\n                ) from e\n            finally:\n                driver.close()\n\n            # Handle empty result set\n            if not rows:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No nodes found matching the query\",\n                    resource=label,\n                    where=obj.get(\"where\", \"\"),\n                )\n\n            # Convert rows to model instances\n            try:\n                if many:\n                    return [\n                        getattr(subj_cls, adapt_meth)(r, **(adapt_kw or {}))\n                        for r in rows\n                    ]\n                return getattr(subj_cls, adapt_meth)(rows[0], **(adapt_kw or {}))\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=rows[0] if not many else rows,\n                    errors=e.errors(),\n                ) from e\n\n        except (ConnectionError, QueryError, ResourceError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(f\"Unexpected error in Neo4j adapter: {e}\", adapter=\"neo4j\")\n\n    # outgoing\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | Sequence[T],\n        /,\n        *,\n        url,\n        auth=None,\n        label=None,\n        merge_on=\"id\",\n        many: bool = True,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if not url:\n                raise AdapterValidationError(\"Missing required parameter 'url'\")\n            if not merge_on:\n                raise AdapterValidationError(\"Missing required parameter 'merge_on'\")\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return None  # Nothing to insert\n\n            # Get label from first item if not provided\n            label = label or items[0].__class__.__name__\n\n            # Create driver\n            driver = cls._create_driver(url, auth=auth)\n\n            try:\n                with driver.session() as s:\n                    results = []\n                    for it in items:\n                        props = getattr(it, adapt_meth)(**(adapt_kw or {}))\n\n                        # Check if merge_on property exists\n                        if merge_on not in props:\n                            raise AdapterValidationError(\n                                f\"Merge property '{merge_on}' not found in model\",\n                                data=props,\n                            )\n\n                        # Prepare and validate Cypher query\n                        cypher = (\n                            f\"MERGE (n:`{label}` {{{merge_on}: $val}}) SET n += $props\"\n                        )\n                        cls._validate_cypher(cypher)\n\n                        # Execute query\n                        try:\n                            result = s.run(cypher, val=props[merge_on], props=props)\n                            results.append(result)\n                        except neo4j.exceptions.CypherSyntaxError as e:\n                            raise QueryError(\n                                f\"Neo4j Cypher syntax error: {e}\",\n                                query=cypher,\n                                adapter=\"neo4j\",\n                            ) from e\n                        except neo4j.exceptions.ConstraintError as e:\n                            raise QueryError(\n                                f\"Neo4j constraint violation: {e}\",\n                                query=cypher,\n                                adapter=\"neo4j\",\n                            ) from e\n                        except Exception as e:\n                            raise QueryError(\n                                f\"Error executing Neo4j query: {e}\",\n                                query=cypher,\n                                adapter=\"neo4j\",\n                            ) from e\n\n                    return {\"merged_count\": len(results)}\n            finally:\n                driver.close()\n\n        except (ConnectionError, QueryError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(f\"Unexpected error in Neo4j adapter: {e}\", adapter=\"neo4j\")\n</code></pre>"},{"location":"api/extras/#qdrant-adapter","title":"Qdrant Adapter","text":""},{"location":"api/extras/#pydapter.extras.qdrant_","title":"<code>pydapter.extras.qdrant_</code>","text":"<p>Qdrant vector-store adapter (requires <code>qdrant-client</code>).</p>"},{"location":"api/extras/#pydapter.extras.qdrant_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.qdrant_.QdrantAdapter","title":"<code>QdrantAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Qdrant vector database adapter for converting between Pydantic models and vector embeddings.</p> <p>This adapter provides methods to: - Search for similar vectors and convert results to Pydantic models - Insert Pydantic models as vector points into Qdrant collections - Handle vector similarity operations and metadata filtering - Support for both cloud and self-hosted Qdrant instances</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"qdrant\")</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.extras.qdrant_ import QdrantAdapter\n\nclass Document(BaseModel):\n    id: str\n    text: str\n    embedding: list[float]\n    category: str\n\n# Search for similar vectors\nsearch_config = {\n    \"url\": \"http://localhost:6333\",\n    \"collection_name\": \"documents\",\n    \"query_vector\": [0.1, 0.2, 0.3, ...],  # 768-dim vector\n    \"limit\": 10,\n    \"score_threshold\": 0.8\n}\nsimilar_docs = QdrantAdapter.from_obj(Document, search_config, many=True)\n\n# Insert documents with vectors\ninsert_config = {\n    \"url\": \"http://localhost:6333\",\n    \"collection_name\": \"documents\"\n}\nnew_docs = [Document(\n    id=\"doc1\",\n    text=\"Sample text\",\n    embedding=[0.1, 0.2, 0.3, ...],\n    category=\"tech\"\n)]\nQdrantAdapter.to_obj(new_docs, insert_config, many=True)\n</code></pre> Source code in <code>src/pydapter/extras/qdrant_.py</code> <pre><code>class QdrantAdapter(Adapter[T]):\n    \"\"\"\n    Qdrant vector database adapter for converting between Pydantic models and vector embeddings.\n\n    This adapter provides methods to:\n    - Search for similar vectors and convert results to Pydantic models\n    - Insert Pydantic models as vector points into Qdrant collections\n    - Handle vector similarity operations and metadata filtering\n    - Support for both cloud and self-hosted Qdrant instances\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"qdrant\")\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.extras.qdrant_ import QdrantAdapter\n\n        class Document(BaseModel):\n            id: str\n            text: str\n            embedding: list[float]\n            category: str\n\n        # Search for similar vectors\n        search_config = {\n            \"url\": \"http://localhost:6333\",\n            \"collection_name\": \"documents\",\n            \"query_vector\": [0.1, 0.2, 0.3, ...],  # 768-dim vector\n            \"limit\": 10,\n            \"score_threshold\": 0.8\n        }\n        similar_docs = QdrantAdapter.from_obj(Document, search_config, many=True)\n\n        # Insert documents with vectors\n        insert_config = {\n            \"url\": \"http://localhost:6333\",\n            \"collection_name\": \"documents\"\n        }\n        new_docs = [Document(\n            id=\"doc1\",\n            text=\"Sample text\",\n            embedding=[0.1, 0.2, 0.3, ...],\n            category=\"tech\"\n        )]\n        QdrantAdapter.to_obj(new_docs, insert_config, many=True)\n        ```\n    \"\"\"\n\n    obj_key = \"qdrant\"\n\n    @staticmethod\n    def _client(url: str | None):\n        \"\"\"\n        Create a Qdrant client with proper error handling.\n\n        Args:\n            url: Qdrant server URL or None for in-memory instance\n\n        Returns:\n            QdrantClient instance\n\n        Raises:\n            ConnectionError: If connection cannot be established\n        \"\"\"\n        try:\n            return QdrantClient(url=url) if url else QdrantClient(\":memory:\")\n        except UnexpectedResponse as e:\n            raise ConnectionError(\n                f\"Failed to connect to Qdrant: {e}\", adapter=\"qdrant\", url=url\n            ) from e\n        except (ConnectionRefusedError, OSError, grpc.RpcError) as e:\n            # Catch specific network-related errors like DNS resolution failures\n            # Include grpc.RpcError to handle gRPC-specific connection issues\n            raise ConnectionError(\n                f\"Failed to connect to Qdrant: {e}\", adapter=\"qdrant\", url=url\n            ) from e\n        except Exception as e:\n            # Check for DNS resolution errors in the exception message\n            if (\n                \"nodename nor servname provided\" in str(e)\n                or \"Name or service not known\" in str(e)\n                or \"getaddrinfo failed\" in str(e)\n            ):\n                raise ConnectionError(\n                    f\"DNS resolution failed for Qdrant: {e}\", adapter=\"qdrant\", url=url\n                ) from e\n            raise ConnectionError(\n                f\"Unexpected error connecting to Qdrant: {e}\", adapter=\"qdrant\", url=url\n            ) from e\n\n    def _validate_vector_dimensions(vector, expected_dim=None):\n        \"\"\"Validate that the vector has the correct dimensions.\"\"\"\n        if not isinstance(vector, (list, tuple)) or not all(\n            isinstance(x, (int, float)) for x in vector\n        ):\n            raise AdapterValidationError(\n                \"Vector must be a list or tuple of numbers\",\n                data=vector,\n            )\n\n        if expected_dim is not None and len(vector) != expected_dim:\n            raise AdapterValidationError(\n                f\"Vector dimension mismatch: expected {expected_dim}, got {len(vector)}\",\n                data=vector,\n            )\n\n    # outgoing\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | Sequence[T],\n        /,\n        *,\n        collection,\n        vector_field=\"embedding\",\n        id_field=\"id\",\n        url=None,\n        many: bool = True,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ) -&gt; None:\n        try:\n            # Validate required parameters\n            if not collection:\n                raise AdapterValidationError(\"Missing required parameter 'collection'\")\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return None  # Nothing to insert\n\n            # Validate vector field exists\n            if not hasattr(items[0], vector_field):\n                raise AdapterValidationError(\n                    f\"Vector field '{vector_field}' not found in model\",\n                    data=getattr(items[0], adapt_meth)(**(adapt_kw or {})),\n                )\n\n            # Validate ID field exists\n            if not hasattr(items[0], id_field):\n                raise AdapterValidationError(\n                    f\"ID field '{id_field}' not found in model\",\n                    data=getattr(items[0], adapt_meth)(**(adapt_kw or {})),\n                )\n\n            # Create client\n            client = cls._client(url)\n\n            # Get vector dimension\n            vector = getattr(items[0], vector_field)\n            cls._validate_vector_dimensions(vector)\n            dim = len(vector)\n\n            # Create or recreate collection\n            try:\n                client.recreate_collection(\n                    collection,\n                    vectors_config=qd.VectorParams(size=dim, distance=\"Cosine\"),\n                )\n            except UnexpectedResponse as e:\n                raise QueryError(\n                    f\"Failed to create Qdrant collection: {e}\",\n                    adapter=\"qdrant\",\n                ) from e\n            except Exception as e:\n                # Check for various DNS and connection-related error messages\n                if (\n                    \"nodename nor servname provided\" in str(e)\n                    or \"connection\" in str(e).lower()\n                    or \"Name or service not known\" in str(e)\n                    or \"getaddrinfo failed\" in str(e)\n                ):\n                    raise ConnectionError(\n                        f\"Failed to connect to Qdrant: {e}\",\n                        adapter=\"qdrant\",\n                        url=url,\n                    ) from e\n                else:\n                    raise QueryError(\n                        f\"Unexpected error creating Qdrant collection: {e}\",\n                        adapter=\"qdrant\",\n                    ) from e\n\n            # Create points\n            try:\n                points = []\n                for i, item in enumerate(items):\n                    vector = getattr(item, vector_field)\n                    cls._validate_vector_dimensions(vector, dim)\n\n                    # Create payload with all fields\n                    # The test_qdrant_to_obj_with_custom_vector_field test expects\n                    # the embedding field to be excluded, but other integration tests\n                    # expect it to be included. We'll include it for now and handle\n                    # the test case separately.\n                    payload = getattr(item, adapt_meth)(**(adapt_kw or {}))\n\n                    points.append(\n                        qd.PointStruct(\n                            id=getattr(item, id_field),\n                            vector=vector,\n                            payload=payload,\n                        )\n                    )\n            except AdapterValidationError:\n                # Re-raise validation errors\n                raise\n            except Exception as e:\n                raise AdapterValidationError(\n                    f\"Error creating Qdrant points: {e}\",\n                    data=items,\n                ) from e\n\n            # Upsert points\n            try:\n                client.upsert(collection, points)\n                return {\"upserted_count\": len(points)}\n            except UnexpectedResponse as e:\n                raise QueryError(\n                    f\"Failed to upsert points to Qdrant: {e}\",\n                    adapter=\"qdrant\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Unexpected error upserting points to Qdrant: {e}\",\n                    adapter=\"qdrant\",\n                ) from e\n\n        except (ConnectionError, QueryError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(\n                f\"Unexpected error in Qdrant adapter: {e}\", adapter=\"qdrant\"\n            )\n\n    # incoming\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: dict,\n        /,\n        *,\n        many=True,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if \"collection\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'collection'\", data=obj\n                )\n            if \"query_vector\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'query_vector'\", data=obj\n                )\n\n            # Validate query vector\n            cls._validate_vector_dimensions(obj[\"query_vector\"])\n\n            # Create client\n            client = cls._client(obj.get(\"url\"))\n\n            # Execute search\n            try:\n                # Set a high score threshold to ensure we get enough results\n                res = client.search(\n                    obj[\"collection\"],\n                    obj[\"query_vector\"],\n                    limit=obj.get(\"top_k\", 5),\n                    with_payload=True,\n                    score_threshold=0.0,  # Return all results regardless of similarity\n                )\n            except UnexpectedResponse as e:\n                if \"not found\" in str(e).lower():\n                    raise ResourceError(\n                        f\"Qdrant collection not found: {e}\",\n                        resource=obj[\"collection\"],\n                    ) from e\n                raise QueryError(\n                    f\"Failed to search Qdrant: {e}\",\n                    adapter=\"qdrant\",\n                ) from e\n            except grpc.RpcError as e:\n                raise ConnectionError(\n                    f\"Qdrant RPC error: {e}\",\n                    adapter=\"qdrant\",\n                    url=obj.get(\"url\"),\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Unexpected error searching Qdrant: {e}\",\n                    adapter=\"qdrant\",\n                ) from e\n\n            # Extract payloads\n            docs = [r.payload for r in res]\n\n            # Handle empty result set\n            if not docs:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No points found matching the query vector\",\n                    resource=obj[\"collection\"],\n                )\n\n            # Convert documents to model instances\n            try:\n                if many:\n                    return [\n                        getattr(subj_cls, adapt_meth)(d, **(adapt_kw or {}))\n                        for d in docs\n                    ]\n                return getattr(subj_cls, adapt_meth)(docs[0], **(adapt_kw or {}))\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=docs[0] if not many else docs,\n                    errors=e.errors(),\n                ) from e\n\n        except (ConnectionError, QueryError, ResourceError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(\n                f\"Unexpected error in Qdrant adapter: {e}\", adapter=\"qdrant\"\n            )\n</code></pre>"},{"location":"api/extras/#async-sql-adapter","title":"Async SQL Adapter","text":""},{"location":"api/extras/#pydapter.extras.async_sql_","title":"<code>pydapter.extras.async_sql_</code>","text":"<p>Generic async SQL adapter - SQLAlchemy 2.x asyncio + asyncpg driver.</p>"},{"location":"api/extras/#pydapter.extras.async_sql_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.async_sql_.AsyncSQLAdapter","title":"<code>AsyncSQLAdapter</code>","text":"<p>               Bases: <code>AsyncAdapter[T]</code></p> <p>Asynchronous SQL adapter using SQLAlchemy 2.x asyncio for database operations.</p> <p>This adapter provides async methods to: - Execute SQL queries asynchronously and convert results to Pydantic models - Insert Pydantic models as rows into database tables asynchronously - Update, delete, and upsert operations through configuration - Execute raw SQL with parameterized queries - Support for various async SQL databases through SQLAlchemy - Handle connection pooling and async context management</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"async_sql\")</p> Configuration Examples <pre><code>from pydantic import BaseModel\nfrom pydapter.extras.async_sql_ import AsyncSQLAdapter, SQLReadConfig, SQLWriteConfig\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\n# Using TypedDict for type hints (recommended for IDE support)\nconfig: SQLReadConfig = {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"table\": \"users\",\n    \"selectors\": {\"active\": True},\n    \"limit\": 10\n}\nusers = await AsyncSQLAdapter.from_obj(User, config, many=True)\n\n# Or inline dict (same as before)\nusers = await AsyncSQLAdapter.from_obj(User, {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"table\": \"users\",\n    \"selectors\": {\"active\": True},\n    \"limit\": 10\n}, many=True)\n\n# DELETE via config\nresult = await AsyncSQLAdapter.from_obj(User, {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"operation\": \"delete\",\n    \"table\": \"users\",\n    \"selectors\": {\"id\": 123}\n})\n\n# Raw SQL execution (note: table parameter NOT required)\nresult = await AsyncSQLAdapter.from_obj(User, {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"operation\": \"raw_sql\",\n    \"sql\": \"SELECT * FROM users WHERE created_at &gt; :since\",\n    \"params\": {\"since\": \"2024-01-01\"}\n}, many=True)\n\n# Or with dict for flexible results (no model validation)\nresult = await AsyncSQLAdapter.from_obj(dict, {\n    \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n    \"operation\": \"raw_sql\",\n    \"sql\": \"SELECT * FROM users ORDER BY created_at DESC LIMIT :limit\",\n    \"params\": {\"limit\": 10}\n}, many=True)\n\n# INSERT (default operation)\nresult = await AsyncSQLAdapter.to_obj(\n    new_user,\n    dsn=\"postgresql+asyncpg://user:pass@localhost/db\",\n    table=\"users\"\n)\n\n# UPDATE via config\nresult = await AsyncSQLAdapter.to_obj(\n    updated_user,\n    dsn=\"postgresql+asyncpg://user:pass@localhost/db\",\n    table=\"users\",\n    operation=\"update\",\n    where={\"id\": 123}\n)\n\n# UPSERT via config\nresult = await AsyncSQLAdapter.to_obj(\n    user_data,\n    dsn=\"postgresql+asyncpg://user:pass@localhost/db\",\n    table=\"users\",\n    operation=\"upsert\",\n    conflict_columns=[\"email\"]\n)\n</code></pre> Source code in <code>src/pydapter/extras/async_sql_.py</code> <pre><code>class AsyncSQLAdapter(AsyncAdapter[T]):\n    \"\"\"\n    Asynchronous SQL adapter using SQLAlchemy 2.x asyncio for database operations.\n\n    This adapter provides async methods to:\n    - Execute SQL queries asynchronously and convert results to Pydantic models\n    - Insert Pydantic models as rows into database tables asynchronously\n    - Update, delete, and upsert operations through configuration\n    - Execute raw SQL with parameterized queries\n    - Support for various async SQL databases through SQLAlchemy\n    - Handle connection pooling and async context management\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"async_sql\")\n\n    Configuration Examples:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.extras.async_sql_ import AsyncSQLAdapter, SQLReadConfig, SQLWriteConfig\n\n        class User(BaseModel):\n            id: int\n            name: str\n            email: str\n\n        # Using TypedDict for type hints (recommended for IDE support)\n        config: SQLReadConfig = {\n            \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n            \"table\": \"users\",\n            \"selectors\": {\"active\": True},\n            \"limit\": 10\n        }\n        users = await AsyncSQLAdapter.from_obj(User, config, many=True)\n\n        # Or inline dict (same as before)\n        users = await AsyncSQLAdapter.from_obj(User, {\n            \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n            \"table\": \"users\",\n            \"selectors\": {\"active\": True},\n            \"limit\": 10\n        }, many=True)\n\n        # DELETE via config\n        result = await AsyncSQLAdapter.from_obj(User, {\n            \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n            \"operation\": \"delete\",\n            \"table\": \"users\",\n            \"selectors\": {\"id\": 123}\n        })\n\n        # Raw SQL execution (note: table parameter NOT required)\n        result = await AsyncSQLAdapter.from_obj(User, {\n            \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n            \"operation\": \"raw_sql\",\n            \"sql\": \"SELECT * FROM users WHERE created_at &gt; :since\",\n            \"params\": {\"since\": \"2024-01-01\"}\n        }, many=True)\n\n        # Or with dict for flexible results (no model validation)\n        result = await AsyncSQLAdapter.from_obj(dict, {\n            \"dsn\": \"postgresql+asyncpg://user:pass@localhost/db\",\n            \"operation\": \"raw_sql\",\n            \"sql\": \"SELECT * FROM users ORDER BY created_at DESC LIMIT :limit\",\n            \"params\": {\"limit\": 10}\n        }, many=True)\n\n        # INSERT (default operation)\n        result = await AsyncSQLAdapter.to_obj(\n            new_user,\n            dsn=\"postgresql+asyncpg://user:pass@localhost/db\",\n            table=\"users\"\n        )\n\n        # UPDATE via config\n        result = await AsyncSQLAdapter.to_obj(\n            updated_user,\n            dsn=\"postgresql+asyncpg://user:pass@localhost/db\",\n            table=\"users\",\n            operation=\"update\",\n            where={\"id\": 123}\n        )\n\n        # UPSERT via config\n        result = await AsyncSQLAdapter.to_obj(\n            user_data,\n            dsn=\"postgresql+asyncpg://user:pass@localhost/db\",\n            table=\"users\",\n            operation=\"upsert\",\n            conflict_columns=[\"email\"]\n        )\n        ```\n    \"\"\"\n\n    obj_key = \"async_sql\"\n\n    @staticmethod\n    def _table(meta: sa.MetaData, name: str, conn=None) -&gt; sa.Table:\n        \"\"\"\n        Helper method to get a SQLAlchemy Table object for async operations.\n\n        Args:\n            meta: SQLAlchemy MetaData instance\n            name: Name of the table to load\n            conn: Optional connection for reflection\n\n        Returns:\n            SQLAlchemy Table object\n\n        Raises:\n            ResourceError: If table is not found or cannot be accessed\n        \"\"\"\n        try:\n            # For async, we can't autoload - just create table reference\n            # The actual schema validation happens at query execution\n            return sa.Table(name, meta)\n        except Exception as e:\n            raise ResourceError(\n                f\"Error accessing table '{name}': {e}\", resource=name\n            ) from e\n\n    # incoming\n    @classmethod\n    async def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: SQLReadConfig | dict,  # TypedDict for IDE support\n        /,\n        *,\n        many=True,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Get operation type (default: \"select\" for backward compatibility)\n            operation = obj.get(\"operation\", \"select\").lower()\n\n            # Validate only one engine parameter is provided\n            engine_params = sum([\"engine\" in obj, \"dsn\" in obj, \"engine_url\" in obj])\n\n            if engine_params == 0:\n                raise ValidationError(\n                    \"Missing required parameter: one of 'engine', 'dsn', or 'engine_url'\",\n                    data=obj,\n                )\n            elif engine_params &gt; 1:\n                raise ValidationError(\n                    \"Multiple engine parameters provided. Use only one of: 'engine', 'dsn', or 'engine_url'\",\n                    data=obj,\n                )\n\n            # Get engine - either passed directly or create from DSN\n            if \"engine\" in obj:\n                eng = obj[\"engine\"]  # Use provided engine\n            elif \"dsn\" in obj:\n                # Create new engine from DSN\n                try:\n                    eng = create_async_engine(obj[\"dsn\"], future=True)\n                except Exception as e:\n                    raise ConnectionError(\n                        f\"Failed to create async database engine: {e}\",\n                        adapter=\"async_sql\",\n                        url=obj[\"dsn\"],\n                    ) from e\n            else:  # engine_url\n                # Create new engine from URL\n                try:\n                    eng = create_async_engine(obj[\"engine_url\"], future=True)\n                except Exception as e:\n                    raise ConnectionError(\n                        f\"Failed to create async database engine: {e}\",\n                        adapter=\"async_sql\",\n                        url=obj[\"engine_url\"],\n                    ) from e\n\n            # Handle different operations\n            if operation == \"select\":\n                # Standard SELECT operation (existing behavior)\n                if \"table\" not in obj:\n                    raise ValidationError(\n                        \"Missing required parameter 'table'\", data=obj\n                    )\n\n                try:\n                    async with eng.begin() as conn:\n                        # Use run_sync for table reflection\n                        def sync_select(sync_conn):\n                            meta = sa.MetaData()\n                            tbl = sa.Table(obj[\"table\"], meta, autoload_with=sync_conn)\n\n                            # Build query with optional selectors\n                            stmt = sa.select(tbl)\n                            if \"selectors\" in obj and obj[\"selectors\"]:\n                                for key, value in obj[\"selectors\"].items():\n                                    stmt = stmt.where(getattr(tbl.c, key) == value)\n\n                            # Add limit/offset if specified\n                            if \"limit\" in obj:\n                                stmt = stmt.limit(obj[\"limit\"])\n                            if \"offset\" in obj:\n                                stmt = stmt.offset(obj[\"offset\"])\n                            # Add order_by if specified\n                            if \"order_by\" in obj:\n                                stmt = stmt.order_by(text(obj[\"order_by\"]))\n\n                            result = sync_conn.execute(stmt)\n                            # Convert Row objects to dicts\n                            return [dict(row._mapping) for row in result.fetchall()]\n\n                        rows = await conn.run_sync(sync_select)\n\n                except sa_exc.SQLAlchemyError as e:\n                    raise QueryError(\n                        f\"Error executing async SQL query: {e}\",\n                        query=str(obj.get(\"selectors\", {})),\n                        adapter=\"async_sql\",\n                    ) from e\n\n                # Handle empty result set\n                if not rows:\n                    if many:\n                        return []\n                    raise ResourceError(\n                        \"No rows found matching the query\",\n                        resource=obj[\"table\"],\n                        selectors=obj.get(\"selectors\", {}),\n                    )\n\n                # Convert rows to model instances (rows are already dicts from run_sync)\n                try:\n                    if many:\n                        return [\n                            getattr(subj_cls, adapt_meth)(r, **(adapt_kw or {}))\n                            for r in rows\n                        ]\n                    return getattr(subj_cls, adapt_meth)(rows[0], **(adapt_kw or {}))\n                except PydanticValidationError as e:\n                    raise ValidationError(\n                        f\"Validation error: {e}\", data=rows[0] if not many else rows\n                    ) from e\n\n            elif operation == \"delete\":\n                # DELETE operation\n                if \"table\" not in obj:\n                    raise ValidationError(\n                        \"Missing required parameter 'table' for delete operation\",\n                        data=obj,\n                    )\n\n                try:\n                    async with eng.begin() as conn:\n                        # Use run_sync for table reflection\n                        def sync_delete(sync_conn):\n                            meta = sa.MetaData()\n                            tbl = sa.Table(obj[\"table\"], meta, autoload_with=sync_conn)\n\n                            # Build DELETE statement with selectors\n                            stmt = sa.delete(tbl)\n                            if \"selectors\" in obj and obj[\"selectors\"]:\n                                for key, value in obj[\"selectors\"].items():\n                                    stmt = stmt.where(getattr(tbl.c, key) == value)\n                            else:\n                                raise ValidationError(\n                                    \"DELETE operation requires 'selectors' to prevent accidental full table deletion\",\n                                    data=obj,\n                                )\n\n                            result = sync_conn.execute(stmt)\n                            return result.rowcount\n\n                        deleted = await conn.run_sync(sync_delete)\n                        return {\"deleted_count\": deleted}\n\n                except sa_exc.SQLAlchemyError as e:\n                    raise QueryError(\n                        f\"Error executing async SQL delete: {e}\",\n                        adapter=\"async_sql\",\n                    ) from e\n\n            elif operation == \"raw_sql\":\n                # Raw SQL execution\n                if \"sql\" not in obj:\n                    raise ValidationError(\n                        \"Missing required parameter 'sql' for raw_sql operation\",\n                        data=obj,\n                    )\n\n                try:\n                    async with eng.begin() as conn:\n                        # Use SQLAlchemy text() for parameterized queries\n                        stmt = text(obj[\"sql\"])\n                        params = obj.get(\"params\", {})\n                        result = await conn.execute(stmt, params)\n\n                        # Handle result based on fetch_results flag and SQL type\n                        fetch_results = obj.get(\"fetch_results\", True)\n                        if fetch_results and result.returns_rows:\n                            rows = result.fetchall()\n                            if not rows:\n                                return [] if many else None\n\n                            # Try to convert to Pydantic models if possible\n                            try:\n                                # Convert Row objects to dicts\n                                records = [\n                                    (\n                                        dict(r._mapping)\n                                        if hasattr(r, \"_mapping\")\n                                        else dict(r)\n                                    )\n                                    for r in rows\n                                ]\n                                if (\n                                    subj_cls is not dict\n                                ):  # Only convert if not using generic dict\n                                    if many:\n                                        return [\n                                            getattr(subj_cls, adapt_meth)(\n                                                r, **(adapt_kw or {})\n                                            )\n                                            for r in records\n                                        ]\n                                    return getattr(subj_cls, adapt_meth)(\n                                        records[0], **(adapt_kw or {})\n                                    )\n                                else:\n                                    return records if many else records[0]\n                            except (PydanticValidationError, TypeError):\n                                # If conversion fails, return raw dicts\n                                records = [\n                                    (\n                                        dict(r._mapping)\n                                        if hasattr(r, \"_mapping\")\n                                        else dict(r)\n                                    )\n                                    for r in rows\n                                ]\n                                return records if many else records[0]\n                        else:\n                            # For DDL, procedures, or when fetch_results=False\n                            return {\n                                \"affected_rows\": (\n                                    result.rowcount if result.rowcount != -1 else 0\n                                )\n                            }\n\n                except sa_exc.SQLAlchemyError as e:\n                    raise QueryError(\n                        f\"Error executing raw SQL: {e}\",\n                        adapter=\"async_sql\",\n                    ) from e\n\n            else:\n                raise ValidationError(\n                    f\"Unsupported operation '{operation}' for from_obj. \"\n                    f\"Supported operations: select, delete, raw_sql\",\n                    data=obj,\n                )\n\n        except AdapterError:\n            raise\n        except Exception as e:\n            raise QueryError(\n                f\"Unexpected error in async SQL adapter: {e}\", adapter=\"async_sql\"\n            ) from e\n\n    # outgoing\n    @classmethod\n    async def to_obj(\n        cls,\n        subj: T | Sequence[T],\n        /,\n        *,\n        many: bool = True,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Get operation type (default: \"insert\" for backward compatibility)\n            operation = kw.get(\"operation\", \"insert\").lower()\n\n            # Validate required parameters\n            if \"table\" not in kw:\n                raise ValidationError(\"Missing required parameter 'table'\")\n\n            table = kw[\"table\"]\n\n            # Validate only one engine parameter is provided\n            engine_params = sum([\"engine\" in kw, \"dsn\" in kw, \"engine_url\" in kw])\n\n            if engine_params == 0:\n                raise ValidationError(\n                    \"Missing required parameter: one of 'engine', 'dsn', or 'engine_url'\"\n                )\n            elif engine_params &gt; 1:\n                raise ValidationError(\n                    \"Multiple engine parameters provided. Use only one of: 'engine', 'dsn', or 'engine_url'\"\n                )\n\n            # Get engine - either passed directly or create from DSN\n            if \"engine\" in kw:\n                eng = kw[\"engine\"]  # Use provided engine\n            elif \"dsn\" in kw:\n                # Create new engine from DSN\n                try:\n                    eng = create_async_engine(kw[\"dsn\"], future=True)\n                except Exception as e:\n                    raise ConnectionError(\n                        f\"Failed to create async database engine: {e}\",\n                        adapter=\"async_sql\",\n                        url=kw[\"dsn\"],\n                    ) from e\n            else:  # engine_url\n                # Create new engine from URL\n                try:\n                    eng = create_async_engine(kw[\"engine_url\"], future=True)\n                except Exception as e:\n                    raise ConnectionError(\n                        f\"Failed to create async database engine: {e}\",\n                        adapter=\"async_sql\",\n                        url=kw[\"engine_url\"],\n                    ) from e\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return {\"affected_count\": 0}\n\n            # Handle different operations\n            if operation == \"insert\":\n                # Standard INSERT operation (existing behavior)\n                rows = [getattr(i, adapt_meth)(**(adapt_kw or {})) for i in items]\n\n                try:\n                    async with eng.begin() as conn:\n                        # Use run_sync to handle table reflection properly\n                        def sync_insert(sync_conn):\n                            meta = sa.MetaData()\n                            tbl = sa.Table(table, meta, autoload_with=sync_conn)\n                            # Filter out None values from rows to let DB handle defaults\n                            clean_rows = [\n                                {k: v for k, v in row.items() if v is not None}\n                                for row in rows\n                            ]\n                            sync_conn.execute(sa.insert(tbl), clean_rows)\n                            return len(clean_rows)\n\n                        count = await conn.run_sync(sync_insert)\n                        return {\"inserted_count\": count}\n\n                except sa_exc.SQLAlchemyError as e:\n                    raise QueryError(\n                        f\"Error executing async SQL insert: {e}\",\n                        query=f\"INSERT INTO {table}\",\n                        adapter=\"async_sql\",\n                    ) from e\n\n            elif operation == \"update\":\n                # UPDATE operation\n                if \"where\" not in kw:\n                    raise ValidationError(\"UPDATE operation requires 'where' parameter\")\n\n                where_conditions = kw[\"where\"]\n                update_data = [\n                    getattr(i, adapt_meth)(**(adapt_kw or {})) for i in items\n                ]\n\n                try:\n                    async with eng.begin() as conn:\n                        # Use run_sync for table reflection\n                        def sync_update(sync_conn):\n                            meta = sa.MetaData()\n                            tbl = sa.Table(table, meta, autoload_with=sync_conn)\n\n                            total_updated = 0\n                            for data in update_data:\n                                # Filter out None values and don't update primary keys\n                                clean_data = {\n                                    k: v\n                                    for k, v in data.items()\n                                    if v is not None and k != \"id\"\n                                }\n                                if not clean_data:\n                                    continue\n\n                                # Build WHERE clause from conditions\n                                stmt = sa.update(tbl)\n                                for key, value in where_conditions.items():\n                                    stmt = stmt.where(getattr(tbl.c, key) == value)\n\n                                # Apply updates\n                                stmt = stmt.values(**clean_data)\n                                result = sync_conn.execute(stmt)\n                                total_updated += result.rowcount\n\n                            return total_updated\n\n                        count = await conn.run_sync(sync_update)\n                        return {\"updated_count\": count}\n\n                except sa_exc.SQLAlchemyError as e:\n                    raise QueryError(\n                        f\"Error executing async SQL update: {e}\",\n                        adapter=\"async_sql\",\n                    ) from e\n\n            elif operation == \"upsert\":\n                # UPSERT operation (basic implementation, PostgreSQL adapter has better version)\n                if \"conflict_columns\" not in kw:\n                    raise ValidationError(\n                        \"UPSERT operation requires 'conflict_columns' parameter\"\n                    )\n\n                # For basic SQL adapter, implement as INSERT with error handling\n                # PostgreSQL adapter will override with proper ON CONFLICT\n                rows = [getattr(i, adapt_meth)(**(adapt_kw or {})) for i in items]\n                conflict_columns = kw[\"conflict_columns\"]\n\n                try:\n                    async with eng.begin() as conn:\n                        # Use run_sync for table reflection\n                        def sync_upsert(sync_conn):\n                            meta = sa.MetaData()\n                            tbl = sa.Table(table, meta, autoload_with=sync_conn)\n\n                            inserted_count = 0\n                            updated_count = 0\n\n                            for row in rows:\n                                # Clean the row data - remove None values\n                                clean_row = {\n                                    k: v for k, v in row.items() if v is not None\n                                }\n\n                                # Check if record exists\n                                select_stmt = sa.select(tbl)\n                                for col in conflict_columns:\n                                    if col in clean_row:\n                                        select_stmt = select_stmt.where(\n                                            getattr(tbl.c, col) == clean_row[col]\n                                        )\n\n                                existing = sync_conn.execute(select_stmt).fetchone()\n\n                                if existing:\n                                    # Update existing record - don't update primary keys\n                                    update_data = {\n                                        k: v for k, v in clean_row.items() if k != \"id\"\n                                    }\n                                    if update_data:\n                                        update_stmt = sa.update(tbl)\n                                        for col in conflict_columns:\n                                            if col in clean_row:\n                                                update_stmt = update_stmt.where(\n                                                    getattr(tbl.c, col)\n                                                    == clean_row[col]\n                                                )\n                                        update_stmt = update_stmt.values(**update_data)\n                                        sync_conn.execute(update_stmt)\n                                    updated_count += 1\n                                else:\n                                    # Insert new record\n                                    insert_stmt = sa.insert(tbl).values(**clean_row)\n                                    sync_conn.execute(insert_stmt)\n                                    inserted_count += 1\n\n                            return {\n                                \"inserted_count\": inserted_count,\n                                \"updated_count\": updated_count,\n                                \"total_count\": inserted_count + updated_count,\n                            }\n\n                        return await conn.run_sync(sync_upsert)\n\n                except sa_exc.SQLAlchemyError as e:\n                    raise QueryError(\n                        f\"Error executing async SQL upsert: {e}\",\n                        adapter=\"async_sql\",\n                    ) from e\n\n            else:\n                raise ValidationError(\n                    f\"Unsupported operation '{operation}' for to_obj. \"\n                    f\"Supported operations: insert, update, upsert\"\n                )\n\n        except AdapterError:\n            raise\n        except Exception as e:\n            raise QueryError(\n                f\"Unexpected error in async SQL adapter: {e}\", adapter=\"async_sql\"\n            ) from e\n</code></pre>"},{"location":"api/extras/#pydapter.extras.async_sql_.SQLReadConfig","title":"<code>SQLReadConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for SQL read operations (from_obj).</p> Source code in <code>src/pydapter/extras/async_sql_.py</code> <pre><code>class SQLReadConfig(TypedDict):\n    \"\"\"Configuration for SQL read operations (from_obj).\"\"\"\n\n    # Connection (exactly one required)\n    dsn: NotRequired[str]  # Database connection string\n    engine_url: NotRequired[str]  # Legacy: Database connection string\n    engine: NotRequired[AsyncEngine]  # Pre-existing SQLAlchemy engine\n\n    # Operation type\n    operation: NotRequired[Literal[\"select\", \"delete\", \"raw_sql\"]]  # Default: \"select\"\n\n    # For select/delete operations (table required for these)\n    table: NotRequired[str]  # Table name (NOT required for raw_sql)\n    selectors: NotRequired[dict[str, Any]]  # WHERE conditions\n    limit: NotRequired[int]  # LIMIT clause\n    offset: NotRequired[int]  # OFFSET clause\n    order_by: NotRequired[str]  # ORDER BY clause\n\n    # For raw_sql operation (table NOT required)\n    sql: NotRequired[str]  # Raw SQL statement\n    params: NotRequired[dict[str, Any]]  # SQL parameters for safe binding\n    fetch_results: NotRequired[bool]  # Whether to fetch results (default: True)\n</code></pre>"},{"location":"api/extras/#pydapter.extras.async_sql_.SQLWriteConfig","title":"<code>SQLWriteConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for SQL write operations (to_obj as **kwargs).</p> Source code in <code>src/pydapter/extras/async_sql_.py</code> <pre><code>class SQLWriteConfig(TypedDict):\n    \"\"\"Configuration for SQL write operations (to_obj as **kwargs).\"\"\"\n\n    # Connection (exactly one required)\n    dsn: NotRequired[str]  # Database connection string\n    engine_url: NotRequired[str]  # Legacy: Database connection string\n    engine: NotRequired[AsyncEngine]  # Pre-existing SQLAlchemy engine\n\n    # Required\n    table: Required[str]  # Table name\n\n    # Operation type\n    operation: NotRequired[Literal[\"insert\", \"update\", \"upsert\"]]  # Default: \"insert\"\n\n    # For update operations\n    where: NotRequired[dict[str, Any]]  # WHERE conditions for UPDATE\n\n    # For upsert operations\n    conflict_columns: NotRequired[list[str]]  # Columns that define conflicts\n    update_columns: NotRequired[list[str]]  # Columns to update on conflict\n</code></pre>"},{"location":"api/extras/#async-postgresql-adapter","title":"Async PostgreSQL Adapter","text":""},{"location":"api/extras/#pydapter.extras.async_postgres_","title":"<code>pydapter.extras.async_postgres_</code>","text":"<p>AsyncPostgresAdapter - presets AsyncSQLAdapter for PostgreSQL/pgvector.</p>"},{"location":"api/extras/#pydapter.extras.async_postgres_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.async_postgres_.AsyncPostgresAdapter","title":"<code>AsyncPostgresAdapter</code>","text":"<p>               Bases: <code>AsyncSQLAdapter[T]</code></p> <p>Asynchronous PostgreSQL adapter extending AsyncSQLAdapter with PostgreSQL-specific optimizations.</p> <p>This adapter provides: - Async PostgreSQL operations using asyncpg driver - Enhanced error handling for PostgreSQL-specific issues - Support for pgvector when vector columns are present - Default PostgreSQL connection string management</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"async_pg\")</p> <code>DEFAULT</code> <p>Default PostgreSQL+asyncpg connection string</p> Example <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\nasync def main():\n    # Query with custom connection\n    query_config = {\n        \"query\": \"SELECT id, name, email FROM users WHERE active = true\",\n        \"dsn\": \"postgresql+asyncpg://user:pass@localhost/mydb\"\n    }\n    users = await AsyncPostgresAdapter.from_obj(User, query_config, many=True)\n\n    # Insert with default connection\n    insert_config = {\n        \"table\": \"users\"\n    }\n    new_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\n    await AsyncPostgresAdapter.to_obj(new_users, insert_config, many=True)\n\nasyncio.run(main())\n</code></pre> Source code in <code>src/pydapter/extras/async_postgres_.py</code> <pre><code>class AsyncPostgresAdapter(AsyncSQLAdapter[T]):\n    \"\"\"\n    Asynchronous PostgreSQL adapter extending AsyncSQLAdapter with PostgreSQL-specific optimizations.\n\n    This adapter provides:\n    - Async PostgreSQL operations using asyncpg driver\n    - Enhanced error handling for PostgreSQL-specific issues\n    - Support for pgvector when vector columns are present\n    - Default PostgreSQL connection string management\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"async_pg\")\n        DEFAULT: Default PostgreSQL+asyncpg connection string\n\n    Example:\n        ```python\n        import asyncio\n        from pydantic import BaseModel\n        from pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\n        class User(BaseModel):\n            id: int\n            name: str\n            email: str\n\n        async def main():\n            # Query with custom connection\n            query_config = {\n                \"query\": \"SELECT id, name, email FROM users WHERE active = true\",\n                \"dsn\": \"postgresql+asyncpg://user:pass@localhost/mydb\"\n            }\n            users = await AsyncPostgresAdapter.from_obj(User, query_config, many=True)\n\n            # Insert with default connection\n            insert_config = {\n                \"table\": \"users\"\n            }\n            new_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\n            await AsyncPostgresAdapter.to_obj(new_users, insert_config, many=True)\n\n        asyncio.run(main())\n        ```\n    \"\"\"\n\n    obj_key = \"async_pg\"\n    DEFAULT = \"postgresql+asyncpg://test:test@localhost/test\"\n\n    @classmethod\n    async def from_obj(\n        cls,\n        subj_cls,\n        obj: dict,\n        /,\n        *,\n        many: bool = True,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Validate only one engine parameter is provided\n            engine_params = sum(\n                [\"engine\" in obj, \"dsn\" in obj, \"dsn\" in kw, \"engine_url\" in obj]\n            )\n\n            if engine_params &gt; 1:\n                raise AdapterValidationError(\n                    \"Multiple engine parameters provided. Use only one of: 'engine', 'dsn', or 'engine_url'\"\n                )\n\n            # Handle DSN/engine setup\n            if \"engine\" not in obj:\n                # Get DSN from obj, kw, or use default\n                if \"dsn\" in obj:\n                    dsn = obj[\"dsn\"]\n                elif \"dsn\" in kw:\n                    dsn = kw[\"dsn\"]\n                    obj[\"dsn\"] = dsn  # Move to obj for parent class\n                elif \"engine_url\" in obj:  # Backward compatibility\n                    dsn = obj[\"engine_url\"]\n                    obj[\"dsn\"] = dsn  # Convert to dsn\n                    del obj[\"engine_url\"]  # Remove to avoid confusion\n                else:\n                    dsn = cls.DEFAULT\n                    obj[\"dsn\"] = dsn\n\n                # Convert PostgreSQL URL to SQLAlchemy format if needed\n                # BUT skip this for SQLite DSNs\n                if dsn.startswith(\"sqlite\"):\n                    # Keep SQLite DSN as-is\n                    pass\n                elif not dsn.startswith(\"postgresql+asyncpg://\"):\n                    obj[\"dsn\"] = dsn.replace(\"postgresql://\", \"postgresql+asyncpg://\")\n\n            # Add PostgreSQL-specific error handling\n            try:\n                return await super().from_obj(\n                    subj_cls,\n                    obj,\n                    many=many,\n                    adapt_meth=adapt_meth,\n                    adapt_kw=adapt_kw,\n                    **kw,\n                )\n            except Exception as e:\n                # Check for common PostgreSQL-specific errors\n                error_str = str(e).lower()\n                conn_url = obj.get(\"dsn\", obj.get(\"engine_url\", cls.DEFAULT))\n                if \"authentication\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL authentication failed: {e}\",\n                        adapter=\"async_pg\",\n                        url=conn_url,\n                    ) from e\n                elif \"connection\" in error_str and \"refused\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL connection refused: {e}\",\n                        adapter=\"async_pg\",\n                        url=conn_url,\n                    ) from e\n                elif \"does not exist\" in error_str and \"database\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL database does not exist: {e}\",\n                        adapter=\"async_pg\",\n                        url=conn_url,\n                    ) from e\n                # Re-raise the original exception\n                raise\n\n        except ConnectionError:\n            # Re-raise ConnectionError\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ConnectionError(\n                f\"Unexpected error in async PostgreSQL adapter: {e}\",\n                adapter=\"async_pg\",\n                url=obj.get(\"engine_url\", cls.DEFAULT),\n            ) from e\n\n    @classmethod\n    async def to_obj(\n        cls,\n        subj,\n        /,\n        *,\n        many: bool = True,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Validate only one engine parameter is provided\n            engine_params = sum([\"engine\" in kw, \"dsn\" in kw, \"engine_url\" in kw])\n\n            if engine_params &gt; 1:\n                raise AdapterValidationError(\n                    \"Multiple engine parameters provided. Use only one of: 'engine', 'dsn', or 'engine_url'\"\n                )\n\n            # Handle DSN/engine setup\n            if \"engine\" not in kw:\n                # Get DSN from kw or use default\n                if \"dsn\" in kw:\n                    dsn = kw[\"dsn\"]\n                elif \"engine_url\" in kw:  # Backward compatibility\n                    dsn = kw[\"engine_url\"]\n                    kw[\"dsn\"] = dsn  # Convert to dsn\n                    del kw[\"engine_url\"]  # Remove to avoid confusion\n                else:\n                    dsn = cls.DEFAULT\n                    kw[\"dsn\"] = dsn\n\n                # Convert PostgreSQL URL to SQLAlchemy format if needed\n                if not dsn.startswith(\"postgresql+asyncpg://\"):\n                    kw[\"dsn\"] = dsn.replace(\"postgresql://\", \"postgresql+asyncpg://\")\n\n            # Add PostgreSQL-specific error handling\n            try:\n                return await super().to_obj(\n                    subj, many=many, adapt_meth=adapt_meth, adapt_kw=adapt_kw, **kw\n                )\n            except Exception as e:\n                # Check for common PostgreSQL-specific errors\n                error_str = str(e).lower()\n                conn_url = kw.get(\"dsn\", kw.get(\"engine_url\", cls.DEFAULT))\n                if \"authentication\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL authentication failed: {e}\",\n                        adapter=\"async_pg\",\n                        url=conn_url,\n                    ) from e\n                elif \"connection\" in error_str and \"refused\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL connection refused: {e}\",\n                        adapter=\"async_pg\",\n                        url=conn_url,\n                    ) from e\n                elif \"does not exist\" in error_str and \"database\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL database does not exist: {e}\",\n                        adapter=\"async_pg\",\n                        url=conn_url,\n                    ) from e\n                # Re-raise the original exception\n                raise\n\n        except ConnectionError:\n            # Re-raise ConnectionError\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ConnectionError(\n                f\"Unexpected error in async PostgreSQL adapter: {e}\",\n                adapter=\"async_pg\",\n                url=kw.get(\"engine_url\", cls.DEFAULT),\n            ) from e\n</code></pre>"},{"location":"api/extras/#async-mongodb-adapter","title":"Async MongoDB Adapter","text":""},{"location":"api/extras/#pydapter.extras.async_mongo_","title":"<code>pydapter.extras.async_mongo_</code>","text":"<p>AsyncMongoAdapter - uses <code>motor.motor_asyncio</code>.</p>"},{"location":"api/extras/#pydapter.extras.async_mongo_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.async_mongo_.AsyncMongoAdapter","title":"<code>AsyncMongoAdapter</code>","text":"<p>               Bases: <code>AsyncAdapter[T]</code></p> <p>Asynchronous MongoDB adapter for converting between Pydantic models and MongoDB documents.</p> <p>This adapter provides async methods to: - Query MongoDB collections asynchronously and convert documents to Pydantic models - Insert Pydantic models as documents into MongoDB collections asynchronously - Handle async MongoDB operations using Motor (async MongoDB driver) - Support for various async MongoDB operations (find, insert, update, delete)</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"async_mongo\")</p> Example <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom pydapter.extras.async_mongo_ import AsyncMongoAdapter\n\nclass User(BaseModel):\n    name: str\n    email: str\n    age: int\n\nasync def main():\n    # Query from MongoDB\n    query_config = {\n        \"url\": \"mongodb://localhost:27017\",\n        \"database\": \"myapp\",\n        \"collection\": \"users\",\n        \"filter\": {\"age\": {\"$gte\": 18}}\n    }\n    users = await AsyncMongoAdapter.from_obj(User, query_config, many=True)\n\n    # Insert to MongoDB\n    insert_config = {\n        \"url\": \"mongodb://localhost:27017\",\n        \"database\": \"myapp\",\n        \"collection\": \"users\"\n    }\n    new_users = [User(name=\"John\", email=\"john@example.com\", age=30)]\n    await AsyncMongoAdapter.to_obj(new_users, insert_config, many=True)\n\nasyncio.run(main())\n</code></pre> Source code in <code>src/pydapter/extras/async_mongo_.py</code> <pre><code>class AsyncMongoAdapter(AsyncAdapter[T]):\n    \"\"\"\n    Asynchronous MongoDB adapter for converting between Pydantic models and MongoDB documents.\n\n    This adapter provides async methods to:\n    - Query MongoDB collections asynchronously and convert documents to Pydantic models\n    - Insert Pydantic models as documents into MongoDB collections asynchronously\n    - Handle async MongoDB operations using Motor (async MongoDB driver)\n    - Support for various async MongoDB operations (find, insert, update, delete)\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"async_mongo\")\n\n    Example:\n        ```python\n        import asyncio\n        from pydantic import BaseModel\n        from pydapter.extras.async_mongo_ import AsyncMongoAdapter\n\n        class User(BaseModel):\n            name: str\n            email: str\n            age: int\n\n        async def main():\n            # Query from MongoDB\n            query_config = {\n                \"url\": \"mongodb://localhost:27017\",\n                \"database\": \"myapp\",\n                \"collection\": \"users\",\n                \"filter\": {\"age\": {\"$gte\": 18}}\n            }\n            users = await AsyncMongoAdapter.from_obj(User, query_config, many=True)\n\n            # Insert to MongoDB\n            insert_config = {\n                \"url\": \"mongodb://localhost:27017\",\n                \"database\": \"myapp\",\n                \"collection\": \"users\"\n            }\n            new_users = [User(name=\"John\", email=\"john@example.com\", age=30)]\n            await AsyncMongoAdapter.to_obj(new_users, insert_config, many=True)\n\n        asyncio.run(main())\n        ```\n    \"\"\"\n\n    obj_key = \"async_mongo\"\n\n    @classmethod\n    def _client(cls, url: str) -&gt; AsyncIOMotorClient:\n        try:\n            return AsyncIOMotorClient(url, serverSelectionTimeoutMS=5000)\n        except pymongo.errors.ConfigurationError as e:\n            raise ConnectionError(\n                f\"Invalid MongoDB connection string: {e}\",\n                adapter=\"async_mongo\",\n                url=url,\n            ) from e\n        except Exception as e:\n            raise ConnectionError(\n                f\"Failed to create MongoDB client: {e}\", adapter=\"async_mongo\", url=url\n            ) from e\n\n    @classmethod\n    async def _validate_connection(cls, client: AsyncIOMotorClient) -&gt; None:\n        \"\"\"Validate that the MongoDB connection is working.\"\"\"\n        try:\n            # This will raise an exception if the connection fails\n            await client.admin.command(\"ping\")\n        except pymongo.errors.ServerSelectionTimeoutError as e:\n            raise ConnectionError(\n                f\"MongoDB server selection timeout: {e}\", adapter=\"async_mongo\"\n            ) from e\n        except pymongo.errors.OperationFailure as e:\n            if \"auth failed\" in str(e).lower():\n                raise ConnectionError(\n                    f\"MongoDB authentication failed: {e}\", adapter=\"async_mongo\"\n                ) from e\n            raise QueryError(\n                f\"MongoDB operation failure: {e}\", adapter=\"async_mongo\"\n            ) from e\n        except Exception as e:\n            raise ConnectionError(\n                f\"Failed to connect to MongoDB: {e}\", adapter=\"async_mongo\"\n            ) from e\n\n    # incoming\n    @classmethod\n    async def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: dict,\n        /,\n        *,\n        many=True,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if \"url\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'url'\", data=obj\n                )\n            if \"db\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'db'\", data=obj\n                )\n            if \"collection\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'collection'\", data=obj\n                )\n\n            # Create client and validate connection\n            client = cls._client(obj[\"url\"])\n            await cls._validate_connection(client)\n\n            # Get collection and execute query\n            try:\n                coll = client[obj[\"db\"]][obj[\"collection\"]]\n                filter_query = obj.get(\"filter\") or {}\n\n                # Validate filter query if provided\n                if filter_query and not isinstance(filter_query, dict):\n                    raise AdapterValidationError(\n                        \"Filter must be a dictionary\",\n                        data=filter_query,\n                    )\n\n                docs = await coll.find(filter_query).to_list(length=None)\n            except pymongo.errors.OperationFailure as e:\n                if \"not authorized\" in str(e).lower():\n                    raise ConnectionError(\n                        f\"Not authorized to access {obj['db']}.{obj['collection']}: {e}\",\n                        adapter=\"async_mongo\",\n                        url=obj[\"url\"],\n                    ) from e\n                raise QueryError(\n                    f\"MongoDB query error: {e}\",\n                    query=filter_query,\n                    adapter=\"async_mongo\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Error executing MongoDB query: {e}\",\n                    query=filter_query,\n                    adapter=\"async_mongo\",\n                ) from e\n\n            # Handle empty result set\n            if not docs:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No documents found matching the query\",\n                    resource=f\"{obj['db']}.{obj['collection']}\",\n                    filter=filter_query,\n                )\n\n            # Convert documents to model instances\n            try:\n                if many:\n                    return [\n                        getattr(subj_cls, adapt_meth)(d, **(adapt_kw or {}))\n                        for d in docs\n                    ]\n                return getattr(subj_cls, adapt_meth)(docs[0], **(adapt_kw or {}))\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=docs[0] if not many else docs,\n                    errors=e.errors(),\n                ) from e\n\n        except (ConnectionError, QueryError, ResourceError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(\n                f\"Unexpected error in async MongoDB adapter: {e}\", adapter=\"async_mongo\"\n            )\n\n    # outgoing\n    @classmethod\n    async def to_obj(\n        cls,\n        subj: T | Sequence[T],\n        /,\n        *,\n        url,\n        db,\n        collection,\n        many=True,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if not url:\n                raise AdapterValidationError(\"Missing required parameter 'url'\")\n            if not db:\n                raise AdapterValidationError(\"Missing required parameter 'db'\")\n            if not collection:\n                raise AdapterValidationError(\"Missing required parameter 'collection'\")\n\n            # Create client and validate connection\n            client = cls._client(url)\n            await cls._validate_connection(client)\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return None  # Nothing to insert\n\n            payload = [getattr(i, adapt_meth)(**(adapt_kw or {})) for i in items]\n\n            # Execute insert\n            try:\n                result = await client[db][collection].insert_many(payload)\n                return {\"inserted_count\": len(result.inserted_ids)}\n            except pymongo.errors.BulkWriteError as e:\n                raise QueryError(\n                    f\"MongoDB bulk write error: {e}\",\n                    adapter=\"async_mongo\",\n                ) from e\n            except pymongo.errors.OperationFailure as e:\n                if \"not authorized\" in str(e).lower():\n                    raise ConnectionError(\n                        f\"Not authorized to write to {db}.{collection}: {e}\",\n                        adapter=\"async_mongo\",\n                        url=url,\n                    ) from e\n                raise QueryError(\n                    f\"MongoDB operation failure: {e}\",\n                    adapter=\"async_mongo\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Error inserting documents into MongoDB: {e}\",\n                    adapter=\"async_mongo\",\n                ) from e\n\n        except (ConnectionError, QueryError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(\n                f\"Unexpected error in async MongoDB adapter: {e}\", adapter=\"async_mongo\"\n            )\n</code></pre>"},{"location":"api/extras/#async-qdrant-adapter","title":"Async Qdrant Adapter","text":""},{"location":"api/extras/#pydapter.extras.async_qdrant_","title":"<code>pydapter.extras.async_qdrant_</code>","text":"<p>AsyncQdrantAdapter - vector upsert / search using AsyncQdrantClient.</p>"},{"location":"api/extras/#pydapter.extras.async_qdrant_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.async_qdrant_.AsyncQdrantAdapter","title":"<code>AsyncQdrantAdapter</code>","text":"<p>               Bases: <code>AsyncAdapter[T]</code></p> <p>Asynchronous Qdrant vector database adapter for async vector operations.</p> <p>This adapter provides async methods to: - Search for similar vectors asynchronously and convert results to Pydantic models - Insert Pydantic models as vector points into Qdrant collections asynchronously - Handle async vector similarity operations and metadata filtering - Support for both cloud and self-hosted Qdrant instances with async operations</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"async_qdrant\")</p> Example <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\nclass Document(BaseModel):\n    id: str\n    text: str\n    embedding: list[float]\n    category: str\n\nasync def main():\n    # Search for similar vectors\n    search_config = {\n        \"url\": \"http://localhost:6333\",\n        \"collection_name\": \"documents\",\n        \"query_vector\": [0.1, 0.2, 0.3, ...],  # 768-dim vector\n        \"limit\": 10,\n        \"score_threshold\": 0.8\n    }\n    similar_docs = await AsyncQdrantAdapter.from_obj(Document, search_config, many=True)\n\n    # Insert documents with vectors\n    insert_config = {\n        \"url\": \"http://localhost:6333\",\n        \"collection_name\": \"documents\"\n    }\n    new_docs = [Document(\n        id=\"doc1\",\n        text=\"Sample text\",\n        embedding=[0.1, 0.2, 0.3, ...],\n        category=\"tech\"\n    )]\n    await AsyncQdrantAdapter.to_obj(new_docs, insert_config, many=True)\n\nasyncio.run(main())\n</code></pre> Source code in <code>src/pydapter/extras/async_qdrant_.py</code> <pre><code>class AsyncQdrantAdapter(AsyncAdapter[T]):\n    \"\"\"\n    Asynchronous Qdrant vector database adapter for async vector operations.\n\n    This adapter provides async methods to:\n    - Search for similar vectors asynchronously and convert results to Pydantic models\n    - Insert Pydantic models as vector points into Qdrant collections asynchronously\n    - Handle async vector similarity operations and metadata filtering\n    - Support for both cloud and self-hosted Qdrant instances with async operations\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"async_qdrant\")\n\n    Example:\n        ```python\n        import asyncio\n        from pydantic import BaseModel\n        from pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n        class Document(BaseModel):\n            id: str\n            text: str\n            embedding: list[float]\n            category: str\n\n        async def main():\n            # Search for similar vectors\n            search_config = {\n                \"url\": \"http://localhost:6333\",\n                \"collection_name\": \"documents\",\n                \"query_vector\": [0.1, 0.2, 0.3, ...],  # 768-dim vector\n                \"limit\": 10,\n                \"score_threshold\": 0.8\n            }\n            similar_docs = await AsyncQdrantAdapter.from_obj(Document, search_config, many=True)\n\n            # Insert documents with vectors\n            insert_config = {\n                \"url\": \"http://localhost:6333\",\n                \"collection_name\": \"documents\"\n            }\n            new_docs = [Document(\n                id=\"doc1\",\n                text=\"Sample text\",\n                embedding=[0.1, 0.2, 0.3, ...],\n                category=\"tech\"\n            )]\n            await AsyncQdrantAdapter.to_obj(new_docs, insert_config, many=True)\n\n        asyncio.run(main())\n        ```\n    \"\"\"\n\n    obj_key = \"async_qdrant\"\n\n    @staticmethod\n    def _client(url: str | None):\n        \"\"\"\n        Create an async Qdrant client with proper error handling.\n\n        Args:\n            url: Qdrant server URL or None for in-memory instance\n\n        Returns:\n            AsyncQdrantClient instance\n\n        Raises:\n            ConnectionError: If connection cannot be established\n        \"\"\"\n        try:\n            return AsyncQdrantClient(url=url) if url else AsyncQdrantClient(\":memory:\")\n        except UnexpectedResponse as e:\n            raise ConnectionError(\n                f\"Failed to connect to Qdrant: {e}\", adapter=\"async_qdrant\", url=url\n            ) from e\n        except Exception as e:\n            raise ConnectionError(\n                f\"Unexpected error connecting to Qdrant: {e}\",\n                adapter=\"async_qdrant\",\n                url=url,\n            ) from e\n\n    @staticmethod\n    def _validate_vector_dimensions(vector, expected_dim=None):\n        \"\"\"Validate that the vector has the correct dimensions.\"\"\"\n        if not isinstance(vector, (list, tuple)) or not all(\n            isinstance(x, (int, float)) for x in vector\n        ):\n            raise AdapterValidationError(\n                \"Vector must be a list or tuple of numbers\",\n                data=vector,\n            )\n\n        if expected_dim is not None and len(vector) != expected_dim:\n            raise AdapterValidationError(\n                f\"Vector dimension mismatch: expected {expected_dim}, got {len(vector)}\",\n                data=vector,\n            )\n\n    # outgoing\n    @classmethod\n    async def to_obj(\n        cls,\n        subj: T | Sequence[T],\n        /,\n        *,\n        collection,\n        vector_field=\"embedding\",\n        id_field=\"id\",\n        url=None,\n        many: bool = True,\n        adapt_meth: str = \"model_dump\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if not collection:\n                raise AdapterValidationError(\"Missing required parameter 'collection'\")\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return None  # Nothing to insert\n\n            # Validate vector field exists\n            if not hasattr(items[0], vector_field):\n                raise AdapterValidationError(\n                    f\"Vector field '{vector_field}' not found in model\",\n                    data=getattr(items[0], adapt_meth)(**(adapt_kw or {})),\n                )\n\n            # Validate ID field exists\n            if not hasattr(items[0], id_field):\n                raise AdapterValidationError(\n                    f\"ID field '{id_field}' not found in model\",\n                    data=getattr(items[0], adapt_meth)(**(adapt_kw or {})),\n                )\n\n            # Get vector dimension\n            vector = getattr(items[0], vector_field)\n            cls._validate_vector_dimensions(vector)\n            dim = len(vector)\n\n            # Create client\n            client = cls._client(url)\n\n            # Create or recreate collection\n            try:\n                await client.recreate_collection(\n                    collection,\n                    vectors_config=qd.VectorParams(size=dim, distance=\"Cosine\"),\n                )\n            except UnexpectedResponse as e:\n                raise QueryError(\n                    f\"Failed to create Qdrant collection: {e}\",\n                    adapter=\"async_qdrant\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Unexpected error creating Qdrant collection: {e}\",\n                    adapter=\"async_qdrant\",\n                ) from e\n\n            # Create points\n            try:\n                points = []\n                for i, item in enumerate(items):\n                    vector = getattr(item, vector_field)\n                    cls._validate_vector_dimensions(vector, dim)\n\n                    points.append(\n                        qd.PointStruct(\n                            id=getattr(item, id_field),\n                            vector=vector,\n                            payload=getattr(item, adapt_meth)(\n                                exclude={vector_field}, **(adapt_kw or {})\n                            ),\n                        )\n                    )\n            except AdapterValidationError:\n                # Re-raise validation errors\n                raise\n            except Exception as e:\n                raise AdapterValidationError(\n                    f\"Error creating Qdrant points: {e}\",\n                    data=items,\n                ) from e\n\n            # Upsert points\n            try:\n                await client.upsert(collection, points)\n                return {\"upserted_count\": len(points)}\n            except UnexpectedResponse as e:\n                raise QueryError(\n                    f\"Failed to upsert points to Qdrant: {e}\",\n                    adapter=\"async_qdrant\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Unexpected error upserting points to Qdrant: {e}\",\n                    adapter=\"async_qdrant\",\n                ) from e\n\n        except AdapterError:\n            raise\n\n        except Exception as e:\n            raise QueryError(\n                f\"Unexpected error in async Qdrant adapter: {e}\", adapter=\"async_qdrant\"\n            )\n\n    # incoming\n    @classmethod\n    async def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: dict,\n        /,\n        *,\n        many=True,\n        adapt_meth: str = \"model_validate\",\n        adapt_kw: dict | None = None,\n        **kw,\n    ):\n        try:\n            if \"collection\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'collection'\", data=obj\n                )\n            if \"query_vector\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'query_vector'\", data=obj\n                )\n\n            # Validate query vector &amp; Create client\n            cls._validate_vector_dimensions(obj[\"query_vector\"])\n            client = cls._client(obj.get(\"url\"))\n\n            # Execute search\n            try:\n                res = await client.search(\n                    obj[\"collection\"],\n                    obj[\"query_vector\"],\n                    limit=obj.get(\"top_k\", 5),\n                    with_payload=True,\n                )\n            except UnexpectedResponse as e:\n                if \"not found\" in str(e).lower():\n                    raise ResourceError(\n                        f\"Qdrant collection not found: {e}\",\n                        resource=obj[\"collection\"],\n                    ) from e\n                raise QueryError(\n                    f\"Failed to search Qdrant: {e}\",\n                    adapter=\"async_qdrant\",\n                ) from e\n            except grpc.RpcError as e:\n                raise ConnectionError(\n                    f\"Qdrant RPC error: {e}\",\n                    adapter=\"async_qdrant\",\n                    url=obj.get(\"url\"),\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Unexpected error searching Qdrant: {e}\",\n                    adapter=\"async_qdrant\",\n                ) from e\n\n            # Extract payloads\n            docs = [r.payload for r in res]\n\n            # Handle empty result set\n            if not docs:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No points found matching the query vector\",\n                    resource=obj[\"collection\"],\n                )\n\n            # Convert documents to model instances\n            try:\n                if many:\n                    return [\n                        getattr(subj_cls, adapt_meth)(d, **(adapt_kw or {}))\n                        for d in docs\n                    ]\n                return getattr(subj_cls, adapt_meth)(docs[0], **(adapt_kw or {}))\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=docs[0] if not many else docs,\n                    errors=e.errors(),\n                ) from e\n\n        except AdapterError:\n            raise\n\n        except Exception as e:\n            raise QueryError(\n                f\"Unexpected error in async Qdrant adapter: {e}\", adapter=\"async_qdrant\"\n            )\n</code></pre>"},{"location":"api/migrations/","title":"Migrations API Reference","text":"<p>This page provides detailed API documentation for the <code>pydapter.migrations</code> module.</p>"},{"location":"api/migrations/#installation","title":"Installation","text":"<p>The migrations module is available as optional dependencies:</p> <pre><code># Core migrations functionality\npip install \"pydapter[migrations-core]\"\n\n# SQL migrations with Alembic support\npip install \"pydapter[migrations-sql]\"\n\n# All migrations components\npip install \"pydapter[migrations]\"\n</code></pre>"},{"location":"api/migrations/#module-overview","title":"Module Overview","text":"<p>The migrations module provides a framework for managing database schema changes, following the adapter pattern:</p> <pre><code>MigrationProtocol\n       \u2502\n       \u25bc\nBaseMigrationAdapter\n       \u2502\n       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502                     \u2502\n       \u25bc                     \u25bc\nSyncMigrationAdapter    AsyncMigrationAdapter\n       \u2502                     \u2502\n       \u25bc                     \u25bc\n AlembicAdapter        AsyncAlembicAdapter\n</code></pre>"},{"location":"api/migrations/#protocols","title":"Protocols","text":""},{"location":"api/migrations/#migrationprotocol","title":"MigrationProtocol","text":""},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol","title":"<code>pydapter.migrations.protocols.MigrationProtocol</code>","text":"<p>               Bases: <code>Protocol[T]</code></p> <p>Protocol defining synchronous migration operations.</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@runtime_checkable\nclass MigrationProtocol(Protocol[T]):\n    \"\"\"Protocol defining synchronous migration operations.\"\"\"\n\n    migration_key: ClassVar[str]\n\n    @classmethod\n    def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize migration environment in the specified directory.\n\n        Args:\n            directory: Path to the directory where migrations will be stored\n            **kwargs: Additional adapter-specific arguments\n        \"\"\"\n        ...\n\n    @classmethod\n    def create_migration(\n        cls, message: str, autogenerate: bool = True, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"\n        Create a new migration.\n\n        Args:\n            message: Description of the migration\n            autogenerate: Whether to auto-generate the migration based on model changes\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The revision identifier of the created migration\n        \"\"\"\n        ...\n\n    @classmethod\n    def upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n        \"\"\"\n        Upgrade to the specified revision.\n\n        Args:\n            revision: The target revision to upgrade to (default: \"head\")\n            **kwargs: Additional adapter-specific arguments\n        \"\"\"\n        ...\n\n    @classmethod\n    def downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Downgrade to the specified revision.\n\n        Args:\n            revision: The target revision to downgrade to\n            **kwargs: Additional adapter-specific arguments\n        \"\"\"\n        ...\n\n    @classmethod\n    def get_current_revision(cls, **kwargs: Any) -&gt; str | None:\n        \"\"\"\n        Get the current migration revision.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The current revision identifier, or None if no migrations have been applied\n        \"\"\"\n        ...\n\n    @classmethod\n    def get_migration_history(cls, **kwargs: Any) -&gt; list[dict]:\n        \"\"\"\n        Get the migration history.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            A list of dictionaries containing migration information\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol.create_migration","title":"<code>create_migration(message, autogenerate=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new migration.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Description of the migration</p> required <code>autogenerate</code> <code>bool</code> <p>Whether to auto-generate the migration based on model changes</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The revision identifier of the created migration</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\ndef create_migration(\n    cls, message: str, autogenerate: bool = True, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Create a new migration.\n\n    Args:\n        message: Description of the migration\n        autogenerate: Whether to auto-generate the migration based on model changes\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The revision identifier of the created migration\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol.downgrade","title":"<code>downgrade(revision, **kwargs)</code>  <code>classmethod</code>","text":"<p>Downgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to downgrade to</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\ndef downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Downgrade to the specified revision.\n\n    Args:\n        revision: The target revision to downgrade to\n        **kwargs: Additional adapter-specific arguments\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol.get_current_revision","title":"<code>get_current_revision(**kwargs)</code>  <code>classmethod</code>","text":"<p>Get the current migration revision.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str | None</code> <p>The current revision identifier, or None if no migrations have been applied</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\ndef get_current_revision(cls, **kwargs: Any) -&gt; str | None:\n    \"\"\"\n    Get the current migration revision.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The current revision identifier, or None if no migrations have been applied\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol.get_migration_history","title":"<code>get_migration_history(**kwargs)</code>  <code>classmethod</code>","text":"<p>Get the migration history.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries containing migration information</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\ndef get_migration_history(cls, **kwargs: Any) -&gt; list[dict]:\n    \"\"\"\n    Get the migration history.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        A list of dictionaries containing migration information\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol.init_migrations","title":"<code>init_migrations(directory, **kwargs)</code>  <code>classmethod</code>","text":"<p>Initialize migration environment in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory where migrations will be stored</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\ndef init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize migration environment in the specified directory.\n\n    Args:\n        directory: Path to the directory where migrations will be stored\n        **kwargs: Additional adapter-specific arguments\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol.upgrade","title":"<code>upgrade(revision='head', **kwargs)</code>  <code>classmethod</code>","text":"<p>Upgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to upgrade to (default: \"head\")</p> <code>'head'</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\ndef upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n    \"\"\"\n    Upgrade to the specified revision.\n\n    Args:\n        revision: The target revision to upgrade to (default: \"head\")\n        **kwargs: Additional adapter-specific arguments\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#asyncmigrationprotocol","title":"AsyncMigrationProtocol","text":""},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol","title":"<code>pydapter.migrations.protocols.AsyncMigrationProtocol</code>","text":"<p>               Bases: <code>Protocol[T]</code></p> <p>Protocol defining asynchronous migration operations.</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@runtime_checkable\nclass AsyncMigrationProtocol(Protocol[T]):\n    \"\"\"Protocol defining asynchronous migration operations.\"\"\"\n\n    migration_key: ClassVar[str]\n\n    @classmethod\n    async def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize migration environment in the specified directory.\n\n        Args:\n            directory: Path to the directory where migrations will be stored\n            **kwargs: Additional adapter-specific arguments\n        \"\"\"\n        ...\n\n    @classmethod\n    async def create_migration(\n        cls, message: str, autogenerate: bool = True, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"\n        Create a new migration.\n\n        Args:\n            message: Description of the migration\n            autogenerate: Whether to auto-generate the migration based on model changes\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The revision identifier of the created migration\n        \"\"\"\n        ...\n\n    @classmethod\n    async def upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n        \"\"\"\n        Upgrade to the specified revision.\n\n        Args:\n            revision: The target revision to upgrade to (default: \"head\")\n            **kwargs: Additional adapter-specific arguments\n        \"\"\"\n        ...\n\n    @classmethod\n    async def downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Downgrade to the specified revision.\n\n        Args:\n            revision: The target revision to downgrade to\n            **kwargs: Additional adapter-specific arguments\n        \"\"\"\n        ...\n\n    @classmethod\n    async def get_current_revision(cls, **kwargs: Any) -&gt; str | None:\n        \"\"\"\n        Get the current migration revision.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The current revision identifier, or None if no migrations have been applied\n        \"\"\"\n        ...\n\n    @classmethod\n    async def get_migration_history(cls, **kwargs: Any) -&gt; list[dict]:\n        \"\"\"\n        Get the migration history.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            A list of dictionaries containing migration information\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol.create_migration","title":"<code>create_migration(message, autogenerate=True, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Create a new migration.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Description of the migration</p> required <code>autogenerate</code> <code>bool</code> <p>Whether to auto-generate the migration based on model changes</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The revision identifier of the created migration</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\nasync def create_migration(\n    cls, message: str, autogenerate: bool = True, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Create a new migration.\n\n    Args:\n        message: Description of the migration\n        autogenerate: Whether to auto-generate the migration based on model changes\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The revision identifier of the created migration\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol.downgrade","title":"<code>downgrade(revision, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Downgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to downgrade to</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\nasync def downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Downgrade to the specified revision.\n\n    Args:\n        revision: The target revision to downgrade to\n        **kwargs: Additional adapter-specific arguments\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol.get_current_revision","title":"<code>get_current_revision(**kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Get the current migration revision.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str | None</code> <p>The current revision identifier, or None if no migrations have been applied</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\nasync def get_current_revision(cls, **kwargs: Any) -&gt; str | None:\n    \"\"\"\n    Get the current migration revision.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The current revision identifier, or None if no migrations have been applied\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol.get_migration_history","title":"<code>get_migration_history(**kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Get the migration history.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries containing migration information</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\nasync def get_migration_history(cls, **kwargs: Any) -&gt; list[dict]:\n    \"\"\"\n    Get the migration history.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        A list of dictionaries containing migration information\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol.init_migrations","title":"<code>init_migrations(directory, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Initialize migration environment in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory where migrations will be stored</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\nasync def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize migration environment in the specified directory.\n\n    Args:\n        directory: Path to the directory where migrations will be stored\n        **kwargs: Additional adapter-specific arguments\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol.upgrade","title":"<code>upgrade(revision='head', **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Upgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to upgrade to (default: \"head\")</p> <code>'head'</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\nasync def upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n    \"\"\"\n    Upgrade to the specified revision.\n\n    Args:\n        revision: The target revision to upgrade to (default: \"head\")\n        **kwargs: Additional adapter-specific arguments\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#base-classes","title":"Base Classes","text":""},{"location":"api/migrations/#basemigrationadapter","title":"BaseMigrationAdapter","text":""},{"location":"api/migrations/#pydapter.migrations.base.BaseMigrationAdapter","title":"<code>pydapter.migrations.base.BaseMigrationAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for migration adapters.</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>class BaseMigrationAdapter(ABC):\n    \"\"\"Base class for migration adapters.\"\"\"\n\n    migration_key: ClassVar[str]\n\n    def __init__(self, connection_string: str, models_module: Any = None):\n        \"\"\"\n        Initialize the migration adapter.\n\n        Args:\n            connection_string: Database connection string\n            models_module: Optional module containing model definitions\n        \"\"\"\n        self.connection_string = connection_string\n        self.models_module = models_module\n        self._initialized = False\n        self._migrations_dir = None\n\n    def _ensure_directory(self, directory: str) -&gt; None:\n        \"\"\"\n        Ensure the directory exists.\n\n        Args:\n            directory: Directory path\n        \"\"\"\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n    def _check_initialized(self) -&gt; None:\n        \"\"\"\n        Check if migrations have been initialized.\n\n        Raises:\n            MigrationError: If migrations have not been initialized\n        \"\"\"\n        if not self._initialized:\n            raise MigrationError(\n                \"Migrations have not been initialized. Call init_migrations first.\",\n                adapter=self.__class__.migration_key,\n            )\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.BaseMigrationAdapter-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.base.BaseMigrationAdapter.__init__","title":"<code>__init__(connection_string, models_module=None)</code>","text":"<p>Initialize the migration adapter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_string</code> <code>str</code> <p>Database connection string</p> required <code>models_module</code> <code>Any</code> <p>Optional module containing model definitions</p> <code>None</code> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>def __init__(self, connection_string: str, models_module: Any = None):\n    \"\"\"\n    Initialize the migration adapter.\n\n    Args:\n        connection_string: Database connection string\n        models_module: Optional module containing model definitions\n    \"\"\"\n    self.connection_string = connection_string\n    self.models_module = models_module\n    self._initialized = False\n    self._migrations_dir = None\n</code></pre>"},{"location":"api/migrations/#syncmigrationadapter","title":"SyncMigrationAdapter","text":""},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter","title":"<code>pydapter.migrations.base.SyncMigrationAdapter</code>","text":"<p>               Bases: <code>BaseMigrationAdapter</code>, <code>MigrationProtocol</code></p> <p>Base class for synchronous migration adapters.</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>class SyncMigrationAdapter(BaseMigrationAdapter, MigrationProtocol):\n    \"\"\"Base class for synchronous migration adapters.\"\"\"\n\n    @classmethod\n    def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize migration environment in the specified directory.\n\n        Args:\n            directory: Path to the directory where migrations will be stored\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationInitError: If initialization fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement init_migrations\")\n\n    @classmethod\n    def create_migration(\n        cls, message: str, autogenerate: bool = True, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"\n        Create a new migration.\n\n        Args:\n            message: Description of the migration\n            autogenerate: Whether to auto-generate the migration based on model changes\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The revision identifier of the created migration\n\n        Raises:\n            MigrationCreationError: If creation fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement create_migration\")\n\n    @classmethod\n    def upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n        \"\"\"\n        Upgrade to the specified revision.\n\n        Args:\n            revision: The target revision to upgrade to (default: \"head\")\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationUpgradeError: If upgrade fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement upgrade\")\n\n    @classmethod\n    def downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Downgrade to the specified revision.\n\n        Args:\n            revision: The target revision to downgrade to\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationDowngradeError: If downgrade fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement downgrade\")\n\n    @classmethod\n    def get_current_revision(cls, **kwargs: Any) -&gt; Optional[str]:\n        \"\"\"\n        Get the current migration revision.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The current revision identifier, or None if no migrations have been applied\n\n        Raises:\n            MigrationError: If getting the current revision fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement get_current_revision\")\n\n    @classmethod\n    def get_migration_history(cls, **kwargs: Any) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Get the migration history.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            A list of dictionaries containing migration information\n\n        Raises:\n            MigrationError: If getting the migration history fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement get_migration_history\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter.create_migration","title":"<code>create_migration(message, autogenerate=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new migration.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Description of the migration</p> required <code>autogenerate</code> <code>bool</code> <p>Whether to auto-generate the migration based on model changes</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The revision identifier of the created migration</p> <p>Raises:</p> Type Description <code>MigrationCreationError</code> <p>If creation fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\ndef create_migration(\n    cls, message: str, autogenerate: bool = True, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Create a new migration.\n\n    Args:\n        message: Description of the migration\n        autogenerate: Whether to auto-generate the migration based on model changes\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The revision identifier of the created migration\n\n    Raises:\n        MigrationCreationError: If creation fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement create_migration\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter.downgrade","title":"<code>downgrade(revision, **kwargs)</code>  <code>classmethod</code>","text":"<p>Downgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to downgrade to</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationDowngradeError</code> <p>If downgrade fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\ndef downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Downgrade to the specified revision.\n\n    Args:\n        revision: The target revision to downgrade to\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationDowngradeError: If downgrade fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement downgrade\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter.get_current_revision","title":"<code>get_current_revision(**kwargs)</code>  <code>classmethod</code>","text":"<p>Get the current migration revision.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The current revision identifier, or None if no migrations have been applied</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the current revision fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\ndef get_current_revision(cls, **kwargs: Any) -&gt; Optional[str]:\n    \"\"\"\n    Get the current migration revision.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The current revision identifier, or None if no migrations have been applied\n\n    Raises:\n        MigrationError: If getting the current revision fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement get_current_revision\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter.get_migration_history","title":"<code>get_migration_history(**kwargs)</code>  <code>classmethod</code>","text":"<p>Get the migration history.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of dictionaries containing migration information</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the migration history fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\ndef get_migration_history(cls, **kwargs: Any) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Get the migration history.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        A list of dictionaries containing migration information\n\n    Raises:\n        MigrationError: If getting the migration history fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement get_migration_history\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter.init_migrations","title":"<code>init_migrations(directory, **kwargs)</code>  <code>classmethod</code>","text":"<p>Initialize migration environment in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory where migrations will be stored</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationInitError</code> <p>If initialization fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\ndef init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize migration environment in the specified directory.\n\n    Args:\n        directory: Path to the directory where migrations will be stored\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationInitError: If initialization fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement init_migrations\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter.upgrade","title":"<code>upgrade(revision='head', **kwargs)</code>  <code>classmethod</code>","text":"<p>Upgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to upgrade to (default: \"head\")</p> <code>'head'</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationUpgradeError</code> <p>If upgrade fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\ndef upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n    \"\"\"\n    Upgrade to the specified revision.\n\n    Args:\n        revision: The target revision to upgrade to (default: \"head\")\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationUpgradeError: If upgrade fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement upgrade\")\n</code></pre>"},{"location":"api/migrations/#asyncmigrationadapter","title":"AsyncMigrationAdapter","text":""},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter","title":"<code>pydapter.migrations.base.AsyncMigrationAdapter</code>","text":"<p>               Bases: <code>BaseMigrationAdapter</code>, <code>AsyncMigrationProtocol</code></p> <p>Base class for asynchronous migration adapters.</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>class AsyncMigrationAdapter(BaseMigrationAdapter, AsyncMigrationProtocol):\n    \"\"\"Base class for asynchronous migration adapters.\"\"\"\n\n    @classmethod\n    async def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize migration environment in the specified directory.\n\n        Args:\n            directory: Path to the directory where migrations will be stored\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationInitError: If initialization fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement init_migrations\")\n\n    @classmethod\n    async def create_migration(\n        cls, message: str, autogenerate: bool = True, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"\n        Create a new migration.\n\n        Args:\n            message: Description of the migration\n            autogenerate: Whether to auto-generate the migration based on model changes\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The revision identifier of the created migration\n\n        Raises:\n            MigrationCreationError: If creation fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement create_migration\")\n\n    @classmethod\n    async def upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n        \"\"\"\n        Upgrade to the specified revision.\n\n        Args:\n            revision: The target revision to upgrade to (default: \"head\")\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationUpgradeError: If upgrade fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement upgrade\")\n\n    @classmethod\n    async def downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Downgrade to the specified revision.\n\n        Args:\n            revision: The target revision to downgrade to\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationDowngradeError: If downgrade fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement downgrade\")\n\n    @classmethod\n    async def get_current_revision(cls, **kwargs: Any) -&gt; Optional[str]:\n        \"\"\"\n        Get the current migration revision.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The current revision identifier, or None if no migrations have been applied\n\n        Raises:\n            MigrationError: If getting the current revision fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement get_current_revision\")\n\n    @classmethod\n    async def get_migration_history(cls, **kwargs: Any) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Get the migration history.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            A list of dictionaries containing migration information\n\n        Raises:\n            MigrationError: If getting the migration history fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement get_migration_history\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter.create_migration","title":"<code>create_migration(message, autogenerate=True, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Create a new migration.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Description of the migration</p> required <code>autogenerate</code> <code>bool</code> <p>Whether to auto-generate the migration based on model changes</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The revision identifier of the created migration</p> <p>Raises:</p> Type Description <code>MigrationCreationError</code> <p>If creation fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\nasync def create_migration(\n    cls, message: str, autogenerate: bool = True, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Create a new migration.\n\n    Args:\n        message: Description of the migration\n        autogenerate: Whether to auto-generate the migration based on model changes\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The revision identifier of the created migration\n\n    Raises:\n        MigrationCreationError: If creation fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement create_migration\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter.downgrade","title":"<code>downgrade(revision, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Downgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to downgrade to</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationDowngradeError</code> <p>If downgrade fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\nasync def downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Downgrade to the specified revision.\n\n    Args:\n        revision: The target revision to downgrade to\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationDowngradeError: If downgrade fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement downgrade\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter.get_current_revision","title":"<code>get_current_revision(**kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Get the current migration revision.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The current revision identifier, or None if no migrations have been applied</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the current revision fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\nasync def get_current_revision(cls, **kwargs: Any) -&gt; Optional[str]:\n    \"\"\"\n    Get the current migration revision.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The current revision identifier, or None if no migrations have been applied\n\n    Raises:\n        MigrationError: If getting the current revision fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement get_current_revision\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter.get_migration_history","title":"<code>get_migration_history(**kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Get the migration history.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of dictionaries containing migration information</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the migration history fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\nasync def get_migration_history(cls, **kwargs: Any) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Get the migration history.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        A list of dictionaries containing migration information\n\n    Raises:\n        MigrationError: If getting the migration history fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement get_migration_history\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter.init_migrations","title":"<code>init_migrations(directory, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Initialize migration environment in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory where migrations will be stored</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationInitError</code> <p>If initialization fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\nasync def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize migration environment in the specified directory.\n\n    Args:\n        directory: Path to the directory where migrations will be stored\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationInitError: If initialization fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement init_migrations\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter.upgrade","title":"<code>upgrade(revision='head', **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Upgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to upgrade to (default: \"head\")</p> <code>'head'</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationUpgradeError</code> <p>If upgrade fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\nasync def upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n    \"\"\"\n    Upgrade to the specified revision.\n\n    Args:\n        revision: The target revision to upgrade to (default: \"head\")\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationUpgradeError: If upgrade fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement upgrade\")\n</code></pre>"},{"location":"api/migrations/#sql-adapters","title":"SQL Adapters","text":""},{"location":"api/migrations/#alembicadapter","title":"AlembicAdapter","text":""},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter","title":"<code>pydapter.migrations.sql.alembic_adapter.AlembicAdapter</code>","text":"<p>               Bases: <code>SyncMigrationAdapter</code></p> <p>Alembic implementation of the MigrationAdapter interface.</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>class AlembicAdapter(SyncMigrationAdapter):\n    \"\"\"Alembic implementation of the MigrationAdapter interface.\"\"\"\n\n    migration_key: ClassVar[str] = \"alembic\"\n\n    def __init__(self, connection_string: str, models_module: Any = None):\n        \"\"\"\n        Initialize the Alembic migration adapter.\n\n        Args:\n            connection_string: Database connection string\n            models_module: Optional module containing SQLAlchemy models\n        \"\"\"\n        super().__init__(connection_string, models_module)\n        self.engine = sa.create_engine(connection_string)\n        self.alembic_cfg = None\n\n    @classmethod\n    def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize migration environment in the specified directory.\n\n        Args:\n            directory: Path to the directory where migrations will be stored\n            **kwargs: Additional adapter-specific arguments\n                connection_string: Database connection string\n                models_module: Optional module containing SQLAlchemy models\n                template: Optional template to use for migration environment\n        \"\"\"\n        try:\n            # Create a new instance with the provided connection string\n            connection_string = kwargs.get(\"connection_string\")\n            if not connection_string:\n                raise MigrationInitError(\n                    \"Connection string is required for Alembic initialization\",\n                    directory=directory,\n                )\n\n            adapter = cls(connection_string, kwargs.get(\"models_module\"))\n\n            # Check if the directory exists and is not empty\n            force_clean = kwargs.get(\"force_clean\", False)\n            if os.path.exists(directory) and os.listdir(directory):\n                if force_clean:\n                    # If force_clean is specified, remove the directory and recreate it\n                    shutil.rmtree(directory)\n                    os.makedirs(directory)\n            else:\n                # Create the directory if it doesn't exist\n                adapter._ensure_directory(directory)\n\n            # Initialize Alembic directory structure\n            template = kwargs.get(\"template\", \"generic\")\n\n            # Create a temporary config file\n            ini_path = os.path.join(directory, \"alembic.ini\")\n            with open(ini_path, \"w\") as f:\n                f.write(\n                    f\"\"\"\n[alembic]\nscript_location = {directory}\nsqlalchemy.url = {connection_string}\n\n[loggers]\nkeys = root,sqlalchemy,alembic\n\n[handlers]\nkeys = console\n\n[formatters]\nkeys = generic\n\n[logger_root]\nlevel = WARN\nhandlers = console\nqualname =\n\n[logger_sqlalchemy]\nlevel = WARN\nhandlers =\nqualname = sqlalchemy.engine\n\n[logger_alembic]\nlevel = INFO\nhandlers =\nqualname = alembic\n\n[handler_console]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatter_generic]\nformat = %(levelname)-5.5s [%(name)s] %(message)s\ndatefmt = %H:%M:%S\n\"\"\"\n                )\n\n            # Initialize Alembic in the specified directory\n            adapter.alembic_cfg = config.Config(ini_path)\n            adapter.alembic_cfg.set_main_option(\"script_location\", directory)\n            adapter.alembic_cfg.set_main_option(\"sqlalchemy.url\", connection_string)\n\n            # Initialize Alembic directory structure\n            try:\n                command.init(adapter.alembic_cfg, directory, template=template)\n            except Exception as e:\n                if \"already exists and is not empty\" in str(e) and force_clean:\n                    # If the directory exists and is not empty, and force_clean is True,\n                    # try to clean it up again and retry\n                    shutil.rmtree(directory)\n                    os.makedirs(directory)\n                    command.init(adapter.alembic_cfg, directory, template=template)\n                else:\n                    raise\n\n            # Update env.py to use the models_module for autogeneration if provided\n            if adapter.models_module:\n                env_path = os.path.join(directory, \"env.py\")\n                adapter._update_env_py(env_path)\n\n            adapter._migrations_dir = directory\n            adapter._initialized = True\n\n            # Return the adapter instance\n            return adapter\n\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationInitError(\n                f\"Failed to initialize Alembic migrations: {str(exc)}\",\n                directory=directory,\n                original_error=str(exc),\n            ) from exc\n\n    def _update_env_py(self, env_path: str) -&gt; None:\n        \"\"\"\n        Update the env.py file to use the models_module for autogeneration.\n\n        Args:\n            env_path: Path to the env.py file\n        \"\"\"\n        if not os.path.exists(env_path):\n            return\n\n        # Read the env.py file\n        with open(env_path) as f:\n            env_content = f.read()\n\n        # Update the target_metadata\n        if \"target_metadata = None\" in env_content:\n            # Import the models module\n            import_statement = f\"from {self.models_module.__name__} import Base\\n\"\n            env_content = env_content.replace(\n                \"from alembic import context\",\n                \"from alembic import context\\n\" + import_statement,\n            )\n\n            # Update the target_metadata\n            env_content = env_content.replace(\n                \"target_metadata = None\", \"target_metadata = Base.metadata\"\n            )\n\n            # Write the updated env.py file\n            with open(env_path, \"w\") as f:\n                f.write(env_content)\n\n    def create_migration(\n        self, message: str, autogenerate: bool = True, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"\n        Create a new migration.\n\n        Args:\n            message: Description of the migration\n            autogenerate: Whether to auto-generate the migration based on model changes\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The revision identifier of the created migration\n\n        Raises:\n            MigrationCreationError: If creation fails\n        \"\"\"\n        try:\n            if not self._initialized:\n                raise MigrationCreationError(\n                    \"Migrations have not been initialized. Call init_migrations first.\"\n                )\n\n            # Create the migration\n            command.revision(\n                self.alembic_cfg,\n                message=message,\n                autogenerate=autogenerate,\n            )\n\n            # Get the revision ID from the latest revision\n            from alembic.script import ScriptDirectory\n\n            script = ScriptDirectory.from_config(self.alembic_cfg)\n            revision = script.get_current_head()\n\n            return revision\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationCreationError(\n                f\"Failed to create migration: {str(exc)}\",\n                autogenerate=autogenerate,\n                original_error=str(exc),\n            ) from exc\n\n    def upgrade(self, revision: str = \"head\", **kwargs: Any) -&gt; None:\n        \"\"\"\n        Upgrade to the specified revision.\n\n        Args:\n            revision: Revision to upgrade to (default: \"head\" for latest)\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationUpgradeError: If upgrade fails\n        \"\"\"\n        try:\n            if not self._initialized:\n                raise MigrationUpgradeError(\n                    \"Migrations have not been initialized. Call init_migrations first.\"\n                )\n\n            # Upgrade to the specified revision\n            command.upgrade(self.alembic_cfg, revision)\n\n            return None\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationUpgradeError(\n                f\"Failed to upgrade: {str(exc)}\",\n                revision=revision,\n                original_error=str(exc),\n            ) from exc\n\n    def downgrade(self, revision: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Downgrade to the specified revision.\n\n        Args:\n            revision: Revision to downgrade to\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationDowngradeError: If downgrade fails\n        \"\"\"\n        try:\n            if not self._initialized:\n                raise MigrationDowngradeError(\n                    \"Migrations have not been initialized. Call init_migrations first.\"\n                )\n\n            # Downgrade to the specified revision\n            command.downgrade(self.alembic_cfg, revision)\n\n            return None\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationDowngradeError(\n                f\"Failed to downgrade: {str(exc)}\",\n                revision=revision,\n                original_error=str(exc),\n            ) from exc\n\n    def get_current_revision(self, **kwargs: Any) -&gt; Optional[str]:\n        \"\"\"\n        Get the current revision.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The current revision, or None if no migrations have been applied\n\n        Raises:\n            MigrationError: If getting the current revision fails\n        \"\"\"\n        try:\n            if not self._initialized:\n                raise MigrationError(\n                    \"Migrations have not been initialized. Call init_migrations first.\"\n                )\n\n            # Get the current revision\n            from alembic.migration import MigrationContext\n\n            # Get the database connection\n            connection = self.engine.connect()\n\n            # Create a migration context\n            migration_context = MigrationContext.configure(connection)\n\n            # Get the current revision\n            current_revision = migration_context.get_current_revision()\n\n            # Close the connection\n            connection.close()\n\n            return current_revision\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationError(\n                f\"Failed to get current revision: {str(exc)}\",\n                original_error=exc,\n            ) from exc\n\n    def get_migration_history(self, **kwargs: Any) -&gt; list[dict]:\n        \"\"\"\n        Get the migration history.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            A list of dictionaries containing migration information\n\n        Raises:\n            MigrationError: If getting the migration history fails\n        \"\"\"\n        try:\n            if not self._initialized:\n                raise MigrationError(\n                    \"Migrations have not been initialized. Call init_migrations first.\"\n                )\n\n            # Get the migration history\n            from alembic.script import ScriptDirectory\n\n            script = ScriptDirectory.from_config(self.alembic_cfg)\n\n            # Get all revisions\n            revisions = []\n            for revision in script.walk_revisions():\n                revisions.append(\n                    {\n                        \"revision\": revision.revision,\n                        \"message\": revision.doc,\n                        \"created\": None,  # Alembic doesn't store creation dates\n                    }\n                )\n\n            return revisions\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationError(\n                f\"Failed to get migration history: {str(exc)}\",\n                original_error=exc,\n            ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.__init__","title":"<code>__init__(connection_string, models_module=None)</code>","text":"<p>Initialize the Alembic migration adapter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_string</code> <code>str</code> <p>Database connection string</p> required <code>models_module</code> <code>Any</code> <p>Optional module containing SQLAlchemy models</p> <code>None</code> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def __init__(self, connection_string: str, models_module: Any = None):\n    \"\"\"\n    Initialize the Alembic migration adapter.\n\n    Args:\n        connection_string: Database connection string\n        models_module: Optional module containing SQLAlchemy models\n    \"\"\"\n    super().__init__(connection_string, models_module)\n    self.engine = sa.create_engine(connection_string)\n    self.alembic_cfg = None\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.create_migration","title":"<code>create_migration(message, autogenerate=True, **kwargs)</code>","text":"<p>Create a new migration.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Description of the migration</p> required <code>autogenerate</code> <code>bool</code> <p>Whether to auto-generate the migration based on model changes</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The revision identifier of the created migration</p> <p>Raises:</p> Type Description <code>MigrationCreationError</code> <p>If creation fails</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def create_migration(\n    self, message: str, autogenerate: bool = True, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Create a new migration.\n\n    Args:\n        message: Description of the migration\n        autogenerate: Whether to auto-generate the migration based on model changes\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The revision identifier of the created migration\n\n    Raises:\n        MigrationCreationError: If creation fails\n    \"\"\"\n    try:\n        if not self._initialized:\n            raise MigrationCreationError(\n                \"Migrations have not been initialized. Call init_migrations first.\"\n            )\n\n        # Create the migration\n        command.revision(\n            self.alembic_cfg,\n            message=message,\n            autogenerate=autogenerate,\n        )\n\n        # Get the revision ID from the latest revision\n        from alembic.script import ScriptDirectory\n\n        script = ScriptDirectory.from_config(self.alembic_cfg)\n        revision = script.get_current_head()\n\n        return revision\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationCreationError(\n            f\"Failed to create migration: {str(exc)}\",\n            autogenerate=autogenerate,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.downgrade","title":"<code>downgrade(revision, **kwargs)</code>","text":"<p>Downgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>Revision to downgrade to</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationDowngradeError</code> <p>If downgrade fails</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def downgrade(self, revision: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Downgrade to the specified revision.\n\n    Args:\n        revision: Revision to downgrade to\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationDowngradeError: If downgrade fails\n    \"\"\"\n    try:\n        if not self._initialized:\n            raise MigrationDowngradeError(\n                \"Migrations have not been initialized. Call init_migrations first.\"\n            )\n\n        # Downgrade to the specified revision\n        command.downgrade(self.alembic_cfg, revision)\n\n        return None\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationDowngradeError(\n            f\"Failed to downgrade: {str(exc)}\",\n            revision=revision,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.get_current_revision","title":"<code>get_current_revision(**kwargs)</code>","text":"<p>Get the current revision.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The current revision, or None if no migrations have been applied</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the current revision fails</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def get_current_revision(self, **kwargs: Any) -&gt; Optional[str]:\n    \"\"\"\n    Get the current revision.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The current revision, or None if no migrations have been applied\n\n    Raises:\n        MigrationError: If getting the current revision fails\n    \"\"\"\n    try:\n        if not self._initialized:\n            raise MigrationError(\n                \"Migrations have not been initialized. Call init_migrations first.\"\n            )\n\n        # Get the current revision\n        from alembic.migration import MigrationContext\n\n        # Get the database connection\n        connection = self.engine.connect()\n\n        # Create a migration context\n        migration_context = MigrationContext.configure(connection)\n\n        # Get the current revision\n        current_revision = migration_context.get_current_revision()\n\n        # Close the connection\n        connection.close()\n\n        return current_revision\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationError(\n            f\"Failed to get current revision: {str(exc)}\",\n            original_error=exc,\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.get_migration_history","title":"<code>get_migration_history(**kwargs)</code>","text":"<p>Get the migration history.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries containing migration information</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the migration history fails</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def get_migration_history(self, **kwargs: Any) -&gt; list[dict]:\n    \"\"\"\n    Get the migration history.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        A list of dictionaries containing migration information\n\n    Raises:\n        MigrationError: If getting the migration history fails\n    \"\"\"\n    try:\n        if not self._initialized:\n            raise MigrationError(\n                \"Migrations have not been initialized. Call init_migrations first.\"\n            )\n\n        # Get the migration history\n        from alembic.script import ScriptDirectory\n\n        script = ScriptDirectory.from_config(self.alembic_cfg)\n\n        # Get all revisions\n        revisions = []\n        for revision in script.walk_revisions():\n            revisions.append(\n                {\n                    \"revision\": revision.revision,\n                    \"message\": revision.doc,\n                    \"created\": None,  # Alembic doesn't store creation dates\n                }\n            )\n\n        return revisions\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationError(\n            f\"Failed to get migration history: {str(exc)}\",\n            original_error=exc,\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.init_migrations","title":"<code>init_migrations(directory, **kwargs)</code>  <code>classmethod</code>","text":"<p>Initialize migration environment in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory where migrations will be stored</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments connection_string: Database connection string models_module: Optional module containing SQLAlchemy models template: Optional template to use for migration environment</p> <code>{}</code> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>    @classmethod\n    def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize migration environment in the specified directory.\n\n        Args:\n            directory: Path to the directory where migrations will be stored\n            **kwargs: Additional adapter-specific arguments\n                connection_string: Database connection string\n                models_module: Optional module containing SQLAlchemy models\n                template: Optional template to use for migration environment\n        \"\"\"\n        try:\n            # Create a new instance with the provided connection string\n            connection_string = kwargs.get(\"connection_string\")\n            if not connection_string:\n                raise MigrationInitError(\n                    \"Connection string is required for Alembic initialization\",\n                    directory=directory,\n                )\n\n            adapter = cls(connection_string, kwargs.get(\"models_module\"))\n\n            # Check if the directory exists and is not empty\n            force_clean = kwargs.get(\"force_clean\", False)\n            if os.path.exists(directory) and os.listdir(directory):\n                if force_clean:\n                    # If force_clean is specified, remove the directory and recreate it\n                    shutil.rmtree(directory)\n                    os.makedirs(directory)\n            else:\n                # Create the directory if it doesn't exist\n                adapter._ensure_directory(directory)\n\n            # Initialize Alembic directory structure\n            template = kwargs.get(\"template\", \"generic\")\n\n            # Create a temporary config file\n            ini_path = os.path.join(directory, \"alembic.ini\")\n            with open(ini_path, \"w\") as f:\n                f.write(\n                    f\"\"\"\n[alembic]\nscript_location = {directory}\nsqlalchemy.url = {connection_string}\n\n[loggers]\nkeys = root,sqlalchemy,alembic\n\n[handlers]\nkeys = console\n\n[formatters]\nkeys = generic\n\n[logger_root]\nlevel = WARN\nhandlers = console\nqualname =\n\n[logger_sqlalchemy]\nlevel = WARN\nhandlers =\nqualname = sqlalchemy.engine\n\n[logger_alembic]\nlevel = INFO\nhandlers =\nqualname = alembic\n\n[handler_console]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatter_generic]\nformat = %(levelname)-5.5s [%(name)s] %(message)s\ndatefmt = %H:%M:%S\n\"\"\"\n                )\n\n            # Initialize Alembic in the specified directory\n            adapter.alembic_cfg = config.Config(ini_path)\n            adapter.alembic_cfg.set_main_option(\"script_location\", directory)\n            adapter.alembic_cfg.set_main_option(\"sqlalchemy.url\", connection_string)\n\n            # Initialize Alembic directory structure\n            try:\n                command.init(adapter.alembic_cfg, directory, template=template)\n            except Exception as e:\n                if \"already exists and is not empty\" in str(e) and force_clean:\n                    # If the directory exists and is not empty, and force_clean is True,\n                    # try to clean it up again and retry\n                    shutil.rmtree(directory)\n                    os.makedirs(directory)\n                    command.init(adapter.alembic_cfg, directory, template=template)\n                else:\n                    raise\n\n            # Update env.py to use the models_module for autogeneration if provided\n            if adapter.models_module:\n                env_path = os.path.join(directory, \"env.py\")\n                adapter._update_env_py(env_path)\n\n            adapter._migrations_dir = directory\n            adapter._initialized = True\n\n            # Return the adapter instance\n            return adapter\n\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationInitError(\n                f\"Failed to initialize Alembic migrations: {str(exc)}\",\n                directory=directory,\n                original_error=str(exc),\n            ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.upgrade","title":"<code>upgrade(revision='head', **kwargs)</code>","text":"<p>Upgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>Revision to upgrade to (default: \"head\" for latest)</p> <code>'head'</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationUpgradeError</code> <p>If upgrade fails</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def upgrade(self, revision: str = \"head\", **kwargs: Any) -&gt; None:\n    \"\"\"\n    Upgrade to the specified revision.\n\n    Args:\n        revision: Revision to upgrade to (default: \"head\" for latest)\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationUpgradeError: If upgrade fails\n    \"\"\"\n    try:\n        if not self._initialized:\n            raise MigrationUpgradeError(\n                \"Migrations have not been initialized. Call init_migrations first.\"\n            )\n\n        # Upgrade to the specified revision\n        command.upgrade(self.alembic_cfg, revision)\n\n        return None\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationUpgradeError(\n            f\"Failed to upgrade: {str(exc)}\",\n            revision=revision,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#asyncalembicadapter","title":"AsyncAlembicAdapter","text":""},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AsyncAlembicAdapter","title":"<code>pydapter.migrations.sql.alembic_adapter.AsyncAlembicAdapter</code>","text":"<p>               Bases: <code>AsyncMigrationAdapter</code></p> <p>Async implementation of the Alembic migration adapter.</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>class AsyncAlembicAdapter(AsyncMigrationAdapter):\n    \"\"\"Async implementation of the Alembic migration adapter.\"\"\"\n\n    migration_key: ClassVar[str] = \"async_alembic\"\n\n    def __init__(self, connection_string: str, models_module: Any = None):\n        \"\"\"\n        Initialize the async Alembic migration adapter.\n\n        Args:\n            connection_string: Database connection string\n            models_module: Optional module containing SQLAlchemy models\n        \"\"\"\n        super().__init__(connection_string, models_module)\n        self.engine = create_async_engine(connection_string)\n        self.alembic_cfg = None\n\n    async def _update_env_py_async(self, env_path: str) -&gt; None:\n        \"\"\"\n        Update the env.py file to use the models_module for autogeneration.\n\n        Args:\n            env_path: Path to the env.py file\n        \"\"\"\n        if not os.path.exists(env_path):\n            return\n\n        # Read the env.py file\n        with open(env_path) as f:\n            env_content = f.read()\n\n        # Update the target_metadata\n        if \"target_metadata = None\" in env_content:\n            # Import the models module\n            import_statement = f\"from {self.models_module.__name__} import Base\\n\"\n            env_content = env_content.replace(\n                \"from alembic import context\",\n                \"from alembic import context\\n\" + import_statement,\n            )\n\n            # Update the target_metadata\n            env_content = env_content.replace(\n                \"target_metadata = None\", \"target_metadata = Base.metadata\"\n            )\n\n            # Write the updated env.py file\n            with open(env_path, \"w\") as f:\n                f.write(env_content)\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AsyncAlembicAdapter-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AsyncAlembicAdapter.__init__","title":"<code>__init__(connection_string, models_module=None)</code>","text":"<p>Initialize the async Alembic migration adapter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_string</code> <code>str</code> <p>Database connection string</p> required <code>models_module</code> <code>Any</code> <p>Optional module containing SQLAlchemy models</p> <code>None</code> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def __init__(self, connection_string: str, models_module: Any = None):\n    \"\"\"\n    Initialize the async Alembic migration adapter.\n\n    Args:\n        connection_string: Database connection string\n        models_module: Optional module containing SQLAlchemy models\n    \"\"\"\n    super().__init__(connection_string, models_module)\n    self.engine = create_async_engine(connection_string)\n    self.alembic_cfg = None\n</code></pre>"},{"location":"api/migrations/#registry","title":"Registry","text":""},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry","title":"<code>pydapter.migrations.registry.MigrationRegistry</code>","text":"<p>Registry for migration adapters.</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>class MigrationRegistry:\n    \"\"\"Registry for migration adapters.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._reg: dict[str, type[MigrationProtocol]] = {}\n\n    def register(self, adapter_cls: type[MigrationProtocol]) -&gt; None:\n        \"\"\"\n        Register a migration adapter.\n\n        Args:\n            adapter_cls: The adapter class to register\n\n        Raises:\n            ConfigurationError: If the adapter does not define a migration_key\n        \"\"\"\n        key = getattr(adapter_cls, \"migration_key\", None)\n        if not key:\n            raise ConfigurationError(\n                \"Migration adapter must define 'migration_key'\",\n                adapter_cls=adapter_cls.__name__,\n            )\n        self._reg[key] = adapter_cls\n\n    def get(self, migration_key: str) -&gt; type[MigrationProtocol]:\n        \"\"\"\n        Get a migration adapter by key.\n\n        Args:\n            migration_key: The key of the adapter to retrieve\n\n        Returns:\n            The adapter class\n\n        Raises:\n            AdapterNotFoundError: If no adapter is registered for the given key\n        \"\"\"\n        try:\n            return self._reg[migration_key]\n        except KeyError as exc:\n            raise AdapterNotFoundError(\n                f\"No migration adapter registered for '{migration_key}'\",\n                obj_key=migration_key,\n            ) from exc\n\n    # Convenience methods for migration operations\n\n    def init_migrations(\n        self, migration_key: str, directory: str, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        Initialize migrations for the specified adapter.\n\n        Args:\n            migration_key: The key of the adapter to use\n            directory: The directory to initialize migrations in\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationInitError: If initialization fails\n        \"\"\"\n        try:\n            adapter_cls = self.get(migration_key)\n            adapter_cls.init_migrations(directory, **kwargs)\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationInitError(\n                f\"Failed to initialize migrations for '{migration_key}'\",\n                directory=directory,\n                adapter=migration_key,\n                original_error=str(exc),\n            ) from exc\n\n    def create_migration(\n        self, migration_key: str, message: str, autogenerate: bool = True, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"\n        Create a migration for the specified adapter.\n\n        Args:\n            migration_key: The key of the adapter to use\n            message: The migration message\n            autogenerate: Whether to auto-generate the migration\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The revision identifier of the created migration\n\n        Raises:\n            MigrationCreationError: If creation fails\n        \"\"\"\n        try:\n            adapter_cls = self.get(migration_key)\n            return adapter_cls.create_migration(message, autogenerate, **kwargs)\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationCreationError(\n                f\"Failed to create migration for '{migration_key}'\",\n                message_text=message,\n                autogenerate=autogenerate,\n                adapter=migration_key,\n                original_error=str(exc),\n            ) from exc\n\n    def upgrade(\n        self, migration_key: str, revision: str = \"head\", **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        Upgrade migrations for the specified adapter.\n\n        Args:\n            migration_key: The key of the adapter to use\n            revision: The target revision to upgrade to\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationUpgradeError: If upgrade fails\n        \"\"\"\n        try:\n            adapter_cls = self.get(migration_key)\n            adapter_cls.upgrade(revision, **kwargs)\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationUpgradeError(\n                f\"Failed to upgrade migrations for '{migration_key}'\",\n                revision=revision,\n                adapter=migration_key,\n                original_error=str(exc),\n            ) from exc\n\n    def downgrade(self, migration_key: str, revision: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Downgrade migrations for the specified adapter.\n\n        Args:\n            migration_key: The key of the adapter to use\n            revision: The target revision to downgrade to\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationDowngradeError: If downgrade fails\n        \"\"\"\n        try:\n            adapter_cls = self.get(migration_key)\n            adapter_cls.downgrade(revision, **kwargs)\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationDowngradeError(\n                f\"Failed to downgrade migrations for '{migration_key}'\",\n                revision=revision,\n                adapter=migration_key,\n                original_error=str(exc),\n            ) from exc\n\n    def get_current_revision(self, migration_key: str, **kwargs: Any) -&gt; Optional[str]:\n        \"\"\"\n        Get the current revision for the specified adapter.\n\n        Args:\n            migration_key: The key of the adapter to use\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The current revision identifier, or None if no migrations have been applied\n\n        Raises:\n            MigrationError: If getting the current revision fails\n        \"\"\"\n        try:\n            adapter_cls = self.get(migration_key)\n            return adapter_cls.get_current_revision(**kwargs)\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationError(\n                f\"Failed to get current revision for '{migration_key}'\",\n                adapter=migration_key,\n                original_error=exc,\n            ) from exc\n\n    def get_migration_history(self, migration_key: str, **kwargs: Any) -&gt; list[dict]:\n        \"\"\"\n        Get the migration history for the specified adapter.\n\n        Args:\n            migration_key: The key of the adapter to use\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            A list of dictionaries containing migration information\n\n        Raises:\n            MigrationError: If getting the migration history fails\n        \"\"\"\n        try:\n            adapter_cls = self.get(migration_key)\n            return adapter_cls.get_migration_history(**kwargs)\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationError(\n                f\"Failed to get migration history for '{migration_key}'\",\n                adapter=migration_key,\n                original_error=exc,\n            ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.create_migration","title":"<code>create_migration(migration_key, message, autogenerate=True, **kwargs)</code>","text":"<p>Create a migration for the specified adapter.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to use</p> required <code>message</code> <code>str</code> <p>The migration message</p> required <code>autogenerate</code> <code>bool</code> <p>Whether to auto-generate the migration</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The revision identifier of the created migration</p> <p>Raises:</p> Type Description <code>MigrationCreationError</code> <p>If creation fails</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def create_migration(\n    self, migration_key: str, message: str, autogenerate: bool = True, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Create a migration for the specified adapter.\n\n    Args:\n        migration_key: The key of the adapter to use\n        message: The migration message\n        autogenerate: Whether to auto-generate the migration\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The revision identifier of the created migration\n\n    Raises:\n        MigrationCreationError: If creation fails\n    \"\"\"\n    try:\n        adapter_cls = self.get(migration_key)\n        return adapter_cls.create_migration(message, autogenerate, **kwargs)\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationCreationError(\n            f\"Failed to create migration for '{migration_key}'\",\n            message_text=message,\n            autogenerate=autogenerate,\n            adapter=migration_key,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.downgrade","title":"<code>downgrade(migration_key, revision, **kwargs)</code>","text":"<p>Downgrade migrations for the specified adapter.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to use</p> required <code>revision</code> <code>str</code> <p>The target revision to downgrade to</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationDowngradeError</code> <p>If downgrade fails</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def downgrade(self, migration_key: str, revision: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Downgrade migrations for the specified adapter.\n\n    Args:\n        migration_key: The key of the adapter to use\n        revision: The target revision to downgrade to\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationDowngradeError: If downgrade fails\n    \"\"\"\n    try:\n        adapter_cls = self.get(migration_key)\n        adapter_cls.downgrade(revision, **kwargs)\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationDowngradeError(\n            f\"Failed to downgrade migrations for '{migration_key}'\",\n            revision=revision,\n            adapter=migration_key,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.get","title":"<code>get(migration_key)</code>","text":"<p>Get a migration adapter by key.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to retrieve</p> required <p>Returns:</p> Type Description <code>type[MigrationProtocol]</code> <p>The adapter class</p> <p>Raises:</p> Type Description <code>AdapterNotFoundError</code> <p>If no adapter is registered for the given key</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def get(self, migration_key: str) -&gt; type[MigrationProtocol]:\n    \"\"\"\n    Get a migration adapter by key.\n\n    Args:\n        migration_key: The key of the adapter to retrieve\n\n    Returns:\n        The adapter class\n\n    Raises:\n        AdapterNotFoundError: If no adapter is registered for the given key\n    \"\"\"\n    try:\n        return self._reg[migration_key]\n    except KeyError as exc:\n        raise AdapterNotFoundError(\n            f\"No migration adapter registered for '{migration_key}'\",\n            obj_key=migration_key,\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.get_current_revision","title":"<code>get_current_revision(migration_key, **kwargs)</code>","text":"<p>Get the current revision for the specified adapter.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to use</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The current revision identifier, or None if no migrations have been applied</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the current revision fails</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def get_current_revision(self, migration_key: str, **kwargs: Any) -&gt; Optional[str]:\n    \"\"\"\n    Get the current revision for the specified adapter.\n\n    Args:\n        migration_key: The key of the adapter to use\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The current revision identifier, or None if no migrations have been applied\n\n    Raises:\n        MigrationError: If getting the current revision fails\n    \"\"\"\n    try:\n        adapter_cls = self.get(migration_key)\n        return adapter_cls.get_current_revision(**kwargs)\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationError(\n            f\"Failed to get current revision for '{migration_key}'\",\n            adapter=migration_key,\n            original_error=exc,\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.get_migration_history","title":"<code>get_migration_history(migration_key, **kwargs)</code>","text":"<p>Get the migration history for the specified adapter.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to use</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries containing migration information</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the migration history fails</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def get_migration_history(self, migration_key: str, **kwargs: Any) -&gt; list[dict]:\n    \"\"\"\n    Get the migration history for the specified adapter.\n\n    Args:\n        migration_key: The key of the adapter to use\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        A list of dictionaries containing migration information\n\n    Raises:\n        MigrationError: If getting the migration history fails\n    \"\"\"\n    try:\n        adapter_cls = self.get(migration_key)\n        return adapter_cls.get_migration_history(**kwargs)\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationError(\n            f\"Failed to get migration history for '{migration_key}'\",\n            adapter=migration_key,\n            original_error=exc,\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.init_migrations","title":"<code>init_migrations(migration_key, directory, **kwargs)</code>","text":"<p>Initialize migrations for the specified adapter.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to use</p> required <code>directory</code> <code>str</code> <p>The directory to initialize migrations in</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationInitError</code> <p>If initialization fails</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def init_migrations(\n    self, migration_key: str, directory: str, **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Initialize migrations for the specified adapter.\n\n    Args:\n        migration_key: The key of the adapter to use\n        directory: The directory to initialize migrations in\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationInitError: If initialization fails\n    \"\"\"\n    try:\n        adapter_cls = self.get(migration_key)\n        adapter_cls.init_migrations(directory, **kwargs)\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationInitError(\n            f\"Failed to initialize migrations for '{migration_key}'\",\n            directory=directory,\n            adapter=migration_key,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.register","title":"<code>register(adapter_cls)</code>","text":"<p>Register a migration adapter.</p> <p>Parameters:</p> Name Type Description Default <code>adapter_cls</code> <code>type[MigrationProtocol]</code> <p>The adapter class to register</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If the adapter does not define a migration_key</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def register(self, adapter_cls: type[MigrationProtocol]) -&gt; None:\n    \"\"\"\n    Register a migration adapter.\n\n    Args:\n        adapter_cls: The adapter class to register\n\n    Raises:\n        ConfigurationError: If the adapter does not define a migration_key\n    \"\"\"\n    key = getattr(adapter_cls, \"migration_key\", None)\n    if not key:\n        raise ConfigurationError(\n            \"Migration adapter must define 'migration_key'\",\n            adapter_cls=adapter_cls.__name__,\n        )\n    self._reg[key] = adapter_cls\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.upgrade","title":"<code>upgrade(migration_key, revision='head', **kwargs)</code>","text":"<p>Upgrade migrations for the specified adapter.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to use</p> required <code>revision</code> <code>str</code> <p>The target revision to upgrade to</p> <code>'head'</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationUpgradeError</code> <p>If upgrade fails</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def upgrade(\n    self, migration_key: str, revision: str = \"head\", **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Upgrade migrations for the specified adapter.\n\n    Args:\n        migration_key: The key of the adapter to use\n        revision: The target revision to upgrade to\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationUpgradeError: If upgrade fails\n    \"\"\"\n    try:\n        adapter_cls = self.get(migration_key)\n        adapter_cls.upgrade(revision, **kwargs)\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationUpgradeError(\n            f\"Failed to upgrade migrations for '{migration_key}'\",\n            revision=revision,\n            adapter=migration_key,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#exceptions","title":"Exceptions","text":""},{"location":"api/migrations/#migrationerror","title":"MigrationError","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationError","title":"<code>pydapter.migrations.exceptions.MigrationError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Base exception for all migration-related errors.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>class MigrationError(AdapterError):\n    \"\"\"Base exception for all migration-related errors.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        original_error: Optional[Exception] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, **context)\n        self.original_error = original_error\n        self.adapter = adapter\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the error.\"\"\"\n        result = super().__str__()\n        if hasattr(self, \"original_error\") and self.original_error is not None:\n            result += f\" (original_error='{self.original_error}')\"\n        return result\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationError-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationError.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the error.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a string representation of the error.\"\"\"\n    result = super().__str__()\n    if hasattr(self, \"original_error\") and self.original_error is not None:\n        result += f\" (original_error='{self.original_error}')\"\n    return result\n</code></pre>"},{"location":"api/migrations/#migrationiniterror","title":"MigrationInitError","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationInitError","title":"<code>pydapter.migrations.exceptions.MigrationInitError</code>","text":"<p>               Bases: <code>MigrationError</code></p> <p>Exception raised when migration initialization fails.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>class MigrationInitError(MigrationError):\n    \"\"\"Exception raised when migration initialization fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        directory: Optional[str] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, directory=directory, adapter=adapter, **context)\n        self.directory = directory\n        self.adapter = adapter\n        # Ensure original_error is set even if not passed through super().__init__\n        if \"original_error\" in context:\n            self.original_error = context[\"original_error\"]\n</code></pre>"},{"location":"api/migrations/#migrationcreationerror","title":"MigrationCreationError","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationCreationError","title":"<code>pydapter.migrations.exceptions.MigrationCreationError</code>","text":"<p>               Bases: <code>MigrationError</code></p> <p>Exception raised when migration creation fails.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>class MigrationCreationError(MigrationError):\n    \"\"\"Exception raised when migration creation fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        message_text: Optional[str] = None,\n        autogenerate: Optional[bool] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(\n            message,\n            message_text=message_text,\n            autogenerate=autogenerate,\n            adapter=adapter,\n            **context,\n        )\n        self.message_text = message_text\n        self.autogenerate = autogenerate\n        self.adapter = adapter\n        # Ensure original_error is set even if not passed through super().__init__\n        if \"original_error\" in context:\n            self.original_error = context[\"original_error\"]\n</code></pre>"},{"location":"api/migrations/#migrationupgradeerror","title":"MigrationUpgradeError","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationUpgradeError","title":"<code>pydapter.migrations.exceptions.MigrationUpgradeError</code>","text":"<p>               Bases: <code>MigrationError</code></p> <p>Exception raised when migration upgrade fails.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>class MigrationUpgradeError(MigrationError):\n    \"\"\"Exception raised when migration upgrade fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        revision: Optional[str] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, revision=revision, adapter=adapter, **context)\n        self.revision = revision\n        self.adapter = adapter\n        # Ensure original_error is set even if not passed through super().__init__\n        if \"original_error\" in context:\n            self.original_error = context[\"original_error\"]\n</code></pre>"},{"location":"api/migrations/#migrationdowngradeerror","title":"MigrationDowngradeError","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationDowngradeError","title":"<code>pydapter.migrations.exceptions.MigrationDowngradeError</code>","text":"<p>               Bases: <code>MigrationError</code></p> <p>Exception raised when migration downgrade fails.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>class MigrationDowngradeError(MigrationError):\n    \"\"\"Exception raised when migration downgrade fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        revision: Optional[str] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, revision=revision, adapter=adapter, **context)\n        self.revision = revision\n        self.adapter = adapter\n        # Ensure original_error is set even if not passed through super().__init__\n        if \"original_error\" in context:\n            self.original_error = context[\"original_error\"]\n</code></pre>"},{"location":"api/migrations/#migrationnotfounderror","title":"MigrationNotFoundError","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationNotFoundError","title":"<code>pydapter.migrations.exceptions.MigrationNotFoundError</code>","text":"<p>               Bases: <code>MigrationError</code></p> <p>Exception raised when a migration is not found.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>class MigrationNotFoundError(MigrationError):\n    \"\"\"Exception raised when a migration is not found.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        revision: Optional[str] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, revision=revision, adapter=adapter, **context)\n        self.revision = revision\n        self.adapter = adapter\n        # Ensure original_error is set even if not passed through super().__init__\n        if \"original_error\" in context:\n            self.original_error = context[\"original_error\"]\n</code></pre>"},{"location":"guides/","title":"Pydapter Development Guides","text":""},{"location":"guides/#overview","title":"Overview","text":"<p>Pydapter is a protocol-driven data transformation framework that emphasizes stateless adapters, composable protocols, type safety, and now a comprehensive field system for building robust models.</p>"},{"location":"guides/#field-system-new-in-v030","title":"\ud83c\udfd7\ufe0f Field System (New in v0.3.0)","text":"<p>The field system provides powerful tools for model creation:</p>"},{"location":"guides/#core-field-guides","title":"Core Field Guides","text":"<ul> <li>Fields Overview - Advanced field descriptors, composition   methods, and field templates</li> <li>Field Families - Pre-built field collections for   common patterns (Entity, Audit, Soft Delete)</li> <li>Best Practices - Comprehensive   patterns for using fields with protocols</li> </ul>"},{"location":"guides/#quick-example","title":"Quick Example","text":"<pre><code>from pydapter.fields import DomainModelBuilder, FieldTemplate\nfrom pydapter.protocols import (\n    create_protocol_model_class,\n    IDENTIFIABLE,\n    TEMPORAL\n)\n\n# Build models with field families\nUser = (\n    DomainModelBuilder(\"User\")\n    .with_entity_fields(timezone_aware=True)  # id, created_at, updated_at\n    .with_audit_fields()                      # created_by, updated_by, version\n    .add_field(\"email\", FieldTemplate(base_type=str))\n    .build()\n)\n\n# Or create protocol-compliant models with behaviors\nUser = create_protocol_model_class(\n    \"User\",\n    IDENTIFIABLE,\n    TEMPORAL,\n    email=FieldTemplate(base_type=str)\n)\n\nuser = User(email=\"test@example.com\")\nuser.update_timestamp()  # Method from TemporalMixin\n</code></pre>"},{"location":"guides/#protocol-system","title":"\ud83d\udd0c Protocol System","text":"<p>Enhanced with type-safe constants and factory functions:</p>"},{"location":"guides/#protocol-guides","title":"Protocol Guides","text":"<ul> <li>Protocols Overview - Deep dive into protocol system and   mixins</li> <li>Protocol Patterns - Common usage   patterns and best practices</li> </ul>"},{"location":"guides/#whats-new","title":"What's New","text":"<ul> <li>Type-Safe Constants: Use <code>IDENTIFIABLE</code>, <code>TEMPORAL</code> instead of strings</li> <li>Factory Functions: <code>create_protocol_model_class()</code> for one-step model creation</li> <li>Mixin Helpers: <code>combine_with_mixins()</code> to add behaviors to existing models</li> </ul>"},{"location":"guides/#architecture-implementation","title":"\ud83c\udfdb\ufe0f Architecture &amp; Implementation","text":""},{"location":"guides/#core-concepts","title":"Core Concepts","text":"<ul> <li>Architecture - Protocol-driven design, stateless   adapters, dual sync/async APIs</li> <li>Creating Adapters - Custom adapter patterns,   error handling, metadata integration</li> <li>Async Patterns - Async adapters, concurrency   control, resource management</li> </ul>"},{"location":"guides/#testing-examples","title":"Testing &amp; Examples","text":"<ul> <li>Testing Strategies - Protocol testing, adapter   testing, async testing patterns</li> <li>End-to-End Backend - Building backends with   PostgreSQL, MongoDB, Neo4j</li> </ul>"},{"location":"guides/#getting-started-flow","title":"Getting Started Flow","text":""},{"location":"guides/#1-understand-the-architecture","title":"1. Understand the Architecture","text":"<p>Start with Architecture to grasp pydapter's core principles:</p> <ul> <li>Protocol + Mixin pattern for composable behaviors</li> <li>Stateless class methods for transformations</li> <li>Separate sync/async implementations</li> </ul>"},{"location":"guides/#2-define-your-models","title":"2. Define Your Models","text":"<p>Use Protocols to add standardized behaviors:</p> <pre><code>class User(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID\n    created_at: datetime\n    updated_at: datetime\n    name: str\n    email: str\n</code></pre>"},{"location":"guides/#3-configure-fields","title":"3. Configure Fields","text":"<p>Leverage Fields for reusable, validated field definitions:</p> <pre><code>from pydapter.fields import ID_FROZEN, DATETIME\n\nclass User(BaseModel):\n    id: UUID = ID_FROZEN.field_info\n    created_at: datetime = DATETIME.field_info\n</code></pre>"},{"location":"guides/#4-create-adapters","title":"4. Create Adapters","text":"<p>Follow Creating Adapters for data transformations:</p> <pre><code>class YamlAdapter(Adapter[T]):\n    obj_key = \"yaml\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: str, /, *, many=False, **kw):\n        data = yaml.safe_load(obj)\n        return subj_cls.model_validate(data)\n</code></pre>"},{"location":"guides/#5-handle-async-operations","title":"5. Handle Async Operations","text":"<p>Use Async Patterns for async data sources:</p> <pre><code>class ApiAdapter(AsyncAdapter[T]):\n    obj_key = \"api\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        async with aiohttp.ClientSession() as session:\n            async with session.get(obj[\"url\"]) as response:\n                data = await response.json()\n        return subj_cls.model_validate(data)\n</code></pre>"},{"location":"guides/#6-test-your-implementation","title":"6. Test Your Implementation","text":"<p>Apply Testing Strategies for robust validation:</p> <pre><code>def test_adapter_roundtrip():\n    original = User(name=\"test\", email=\"test@example.com\")\n    external = YamlAdapter.to_obj(original)\n    restored = YamlAdapter.from_obj(User, external)\n    assert restored == original\n</code></pre>"},{"location":"guides/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"guides/#protocol-composition","title":"Protocol Composition","text":"<pre><code># Standard entity pattern\nclass Entity(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID\n    created_at: datetime\n    updated_at: datetime\n\n# ML content pattern\nclass MLContent(BaseModel, IdentifiableMixin, EmbeddableMixin):\n    id: UUID\n    content: str | None = None\n    embedding: list[float] = Field(default_factory=list)\n</code></pre>"},{"location":"guides/#adapter-registry-pattern","title":"Adapter Registry Pattern","text":"<pre><code># Create registry\nregistry = AdapterRegistry()\n\n# Register adapters\nregistry.register(JsonAdapter)\nregistry.register(YamlAdapter)\nregistry.register(CsvAdapter)\n\n# Use through registry\nuser = registry.adapt_from(User, data, obj_key=\"json\")\noutput = registry.adapt_to(user, obj_key=\"yaml\")\n</code></pre>"},{"location":"guides/#repository-pattern","title":"Repository Pattern","text":"<pre><code>class UserRepository:\n    def __init__(self, registry: AsyncAdapterRegistry):\n        self.registry = registry\n\n    async def get_by_id(self, user_id: UUID) -&gt; User | None:\n        config = {\"query\": \"SELECT * FROM users WHERE id = $1\", \"params\": [user_id]}\n        return await self.registry.adapt_from(User, config, obj_key=\"postgres\")\n</code></pre>"},{"location":"guides/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/#1-multi-format-data-processing","title":"1. Multi-Format Data Processing","text":"<p>Handle JSON, CSV, YAML, XML with unified interface:</p> <pre><code>registry = AdapterRegistry()\nregistry.register(JsonAdapter)\nregistry.register(CsvAdapter)\nregistry.register(YamlAdapter)\n\n# Process any format\ndef process_data(data: str, format: str) -&gt; list[User]:\n    return registry.adapt_from(User, data, obj_key=format, many=True)\n</code></pre>"},{"location":"guides/#2-database-abstraction","title":"2. Database Abstraction","text":"<p>Work with multiple databases through consistent interface:</p> <pre><code># PostgreSQL for ACID transactions\nusers = await postgres_registry.adapt_from(User, postgres_config, obj_key=\"postgres\")\n\n# MongoDB for analytics\nawait mongo_registry.adapt_to(users, obj_key=\"mongo\", collection=\"user_analytics\")\n\n# Neo4j for relationships\nawait neo4j_registry.adapt_to(users, obj_key=\"neo4j\", relationship=\"KNOWS\")\n</code></pre>"},{"location":"guides/#3-api-integration","title":"3. API Integration","text":"<p>Transform between internal models and external APIs:</p> <pre><code># Fetch from external API\nexternal_users = await api_adapter.from_obj(ExternalUser, {\"url\": api_url}, many=True)\n\n# Transform to internal format\ninternal_users = [InternalUser.model_validate(user.model_dump()) for user in external_users]\n\n# Store in database\nawait db_adapter.to_obj(internal_users, many=True, table=\"users\")\n</code></pre>"},{"location":"guides/#advanced-topics","title":"Advanced Topics","text":""},{"location":"guides/#field-metadata-for-adapters","title":"Field Metadata for Adapters","text":"<pre><code>VECTOR_FIELD = Field(\n    name=\"embedding\",\n    annotation=list[float],\n    json_schema_extra={\n        \"vector_dim\": 768,\n        \"distance_metric\": \"cosine\"\n    }\n)\n\n# Adapters can use this metadata\nclass VectorDBAdapter(Adapter[T]):\n    @classmethod\n    def to_obj(cls, subj: T, **kw):\n        for field_name, field_info in subj.model_fields.items():\n            extra = field_info.json_schema_extra or {}\n            if extra.get(\"vector_dim\"):\n                # Create vector column with specified dimension\n                pass\n</code></pre>"},{"location":"guides/#custom-protocol-creation","title":"Custom Protocol Creation","text":"<pre><code>@runtime_checkable\nclass Auditable(Protocol):\n    audit_log: list[str]\n\nclass AuditableMixin:\n    def add_audit_entry(self, action: str) -&gt; None:\n        if not hasattr(self, 'audit_log'):\n            self.audit_log = []\n        self.audit_log.append(f\"{datetime.now()}: {action}\")\n</code></pre>"},{"location":"guides/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Connection pooling for database adapters\nclass PooledDBAdapter(AsyncAdapter[T]):\n    _pool: asyncpg.Pool = None\n\n    @classmethod\n    async def _get_pool(cls):\n        if cls._pool is None:\n            cls._pool = await asyncpg.create_pool(connection_string)\n        return cls._pool\n\n# Concurrent processing\nasync def process_multiple_sources(sources: list[dict]) -&gt; list[T]:\n    semaphore = asyncio.Semaphore(5)  # Limit concurrency\n\n    async def process_one(source):\n        async with semaphore:\n            return await SomeAdapter.from_obj(MyModel, source)\n\n    results = await asyncio.gather(*[process_one(s) for s in sources])\n    return [r for r in results if r is not None]\n</code></pre>"},{"location":"guides/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"guides/#1-design-principles","title":"1. Design Principles","text":"<ul> <li>Use protocols for behavioral contracts</li> <li>Keep adapters stateless with class methods</li> <li>Compose behaviors through mixins</li> <li>Separate sync and async implementations</li> </ul>"},{"location":"guides/#2-error-handling","title":"2. Error Handling","text":"<ul> <li>Use specific exception types (<code>ParseError</code>, <code>ValidationError</code>)</li> <li>Provide detailed error context</li> <li>Test error paths thoroughly</li> </ul>"},{"location":"guides/#3-testing-strategy","title":"3. Testing Strategy","text":"<ul> <li>Test protocol compliance</li> <li>Verify adapter roundtrips</li> <li>Mock external dependencies</li> <li>Use property-based testing for edge cases</li> </ul>"},{"location":"guides/#4-performance","title":"4. Performance","text":"<ul> <li>Use connection pooling for databases</li> <li>Implement concurrency control with semaphores</li> <li>Add timeout and retry logic for external services</li> <li>Monitor and optimize hot paths</li> </ul> <p>For detailed information on any topic, see the specific guide linked above.</p>"},{"location":"guides/architecture/","title":"Pydapter Architecture and Design Philosophy","text":""},{"location":"guides/architecture/#core-principles","title":"Core Principles","text":"<p>Pydapter follows protocol-driven architecture with stateless transformations and composition over inheritance.</p>"},{"location":"guides/architecture/#1-protocol-mixin-pattern","title":"1. Protocol + Mixin Pattern","text":"<pre><code>@runtime_checkable\nclass Identifiable(Protocol):\n    id: UUID\n\nclass IdentifiableMixin:\n    def __hash__(self) -&gt; int:\n        return hash(self.id)\n</code></pre> <p>Key Benefits:</p> <ul> <li>Type safety without inheritance coupling</li> <li>Runtime validation when needed</li> <li>Composable behaviors</li> </ul>"},{"location":"guides/architecture/#2-stateless-class-methods","title":"2. Stateless Class Methods","text":"<pre><code>class Adapter(Protocol[T]):\n    obj_key: ClassVar[str]\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: Any, /, *, many=False, **kw) -&gt; T | list[T]: ...\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many=False, **kw) -&gt; Any: ...\n</code></pre> <p>Why Class Methods:</p> <ul> <li>Thread safety (no shared state)</li> <li>No instantiation overhead</li> <li>Simple testing</li> <li>Clear interfaces</li> </ul>"},{"location":"guides/architecture/#3-dual-syncasync-apis","title":"3. Dual Sync/Async APIs","text":"<p>Separate implementations without mixing concerns:</p> <ul> <li>Sync: <code>Adapter</code>, <code>AdapterRegistry</code>, <code>Adaptable</code></li> <li>Async: <code>AsyncAdapter</code>, <code>AsyncAdapterRegistry</code>, <code>AsyncAdaptable</code></li> </ul> <p>Benefits:</p> <ul> <li>No async overhead in sync code</li> <li>Clear separation of concerns</li> <li>Type safety in both contexts</li> </ul>"},{"location":"guides/architecture/#component-layers","title":"Component Layers","text":""},{"location":"guides/architecture/#core-layer","title":"Core Layer","text":"<ul> <li><code>Adapter</code>: Transformation protocol</li> <li><code>AdapterRegistry</code>: Adapter management</li> <li><code>Adaptable</code>: Model mixin for adapter access</li> </ul>"},{"location":"guides/architecture/#protocol-layer","title":"Protocol Layer","text":"<ul> <li><code>Identifiable</code>: UUID-based identity</li> <li><code>Temporal</code>: Timestamp management</li> <li><code>Embeddable</code>: Vector embeddings</li> <li><code>Event</code>: Event-driven patterns</li> </ul>"},{"location":"guides/architecture/#field-system","title":"Field System","text":"<ul> <li><code>Field</code>: Advanced field descriptors</li> <li>Pre-configured fields: <code>ID_FROZEN</code>, <code>DATETIME</code>, <code>EMBEDDING</code></li> <li>Composition methods: <code>as_nullable()</code>, <code>as_listable()</code></li> </ul>"},{"location":"guides/architecture/#adapter-ecosystem","title":"Adapter Ecosystem","text":"<ul> <li>Built-in: JSON, CSV, TOML</li> <li>Extended: PostgreSQL, MongoDB, Neo4j, Qdrant, Weaviate</li> </ul>"},{"location":"guides/architecture/#design-philosophy","title":"Design Philosophy","text":""},{"location":"guides/architecture/#composition-over-inheritance","title":"Composition Over Inheritance","text":"<pre><code>class Document(BaseModel, IdentifiableMixin, TemporalMixin):\n    title: str\n    content: str\n</code></pre>"},{"location":"guides/architecture/#progressive-complexity","title":"Progressive Complexity","text":"<pre><code># Simple: Direct usage\nperson = JsonAdapter.from_obj(Person, json_data)\n\n# Advanced: Registry-based\nregistry.adapt_from(Person, data, obj_key=\"json\")\n</code></pre>"},{"location":"guides/architecture/#explicit-configuration","title":"Explicit Configuration","text":"<pre><code># Clear interfaces, explicit parameters\nperson = JsonAdapter.from_obj(Person, data, many=False, strict=True)\n</code></pre>"},{"location":"guides/architecture/#extension-points","title":"Extension Points","text":"<ol> <li>Custom Adapters: Implement <code>Adapter</code>/<code>AsyncAdapter</code> protocol</li> <li>Custom Protocols: Extend existing or create new protocols</li> <li>Field Descriptors: Domain-specific fields with <code>Field</code></li> <li>Migration Adapters: Schema evolution support</li> <li>Registry Extensions: Specialized adapter collections</li> </ol> <p>This architecture enables both simple use cases and complex production systems through clear abstractions and composable components.</p>"},{"location":"guides/async-patterns/","title":"Async/Sync Patterns in Pydapter","text":""},{"location":"guides/async-patterns/#architecture-decision","title":"Architecture Decision","text":"<p>Pydapter provides separate sync and async implementations without mixing concerns:</p> <ul> <li>Sync: <code>Adapter</code>, <code>AdapterRegistry</code>, <code>Adaptable</code></li> <li>Async: <code>AsyncAdapter</code>, <code>AsyncAdapterRegistry</code>, <code>AsyncAdaptable</code></li> </ul> <p>Benefits:</p> <ul> <li>No async overhead in sync operations</li> <li>Clear separation of concerns</li> <li>Type safety in both contexts</li> <li>Simple, focused interfaces</li> </ul>"},{"location":"guides/async-patterns/#core-async-components","title":"Core Async Components","text":""},{"location":"guides/async-patterns/#asyncadapter-protocol","title":"AsyncAdapter Protocol","text":"<pre><code>from pydapter.async_core import AsyncAdapter\n\nclass MyAsyncAdapter(AsyncAdapter[T]):\n    obj_key = \"my_async_format\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: Any, /, *, many=False, **kw) -&gt; T | list[T]:\n        # Async operations (HTTP, database, etc.)\n        pass\n\n    @classmethod\n    async def to_obj(cls, subj: T | list[T], /, *, many=False, **kw) -&gt; Any:\n        # Async output operations\n        pass\n</code></pre>"},{"location":"guides/async-patterns/#asyncadapterregistry","title":"AsyncAdapterRegistry","text":"<pre><code>from pydapter.async_core import AsyncAdapterRegistry\n\nasync_registry = AsyncAdapterRegistry()\nasync_registry.register(MyAsyncAdapter)\n\nresult = await async_registry.adapt_from(MyModel, data, obj_key=\"my_async_format\")\n</code></pre>"},{"location":"guides/async-patterns/#asyncadaptable-mixin","title":"AsyncAdaptable Mixin","text":"<pre><code>from pydapter.async_core import AsyncAdaptable\n\nclass MyModel(BaseModel, AsyncAdaptable):\n    name: str\n    value: int\n\nMyModel.register_async_adapter(MyAsyncAdapter)\ninstance = await MyModel.adapt_from_async(data, obj_key=\"my_async_format\")\n</code></pre>"},{"location":"guides/async-patterns/#common-async-patterns","title":"Common Async Patterns","text":""},{"location":"guides/async-patterns/#http-api-adapter","title":"HTTP API Adapter","text":"<pre><code>class RestApiAdapter(AsyncAdapter[T]):\n    obj_key = \"rest_api\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        url = f\"{obj['base_url']}/{obj['endpoint']}\"\n\n        async with aiohttp.ClientSession() as session:\n            async with session.get(url) as response:\n                response.raise_for_status()\n                data = await response.json()\n\n        if many:\n            items = data.get(\"items\", data) if isinstance(data, dict) else data\n            return [subj_cls.model_validate(item) for item in items]\n        return subj_cls.model_validate(data)\n</code></pre>"},{"location":"guides/async-patterns/#database-adapter","title":"Database Adapter","text":"<pre><code>class AsyncPostgresAdapter(AsyncAdapter[T]):\n    obj_key = \"async_postgres\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        conn = await asyncpg.connect(obj[\"connection_string\"])\n        try:\n            if many:\n                rows = await conn.fetch(obj[\"query\"], *obj.get(\"params\", []))\n                return [subj_cls.model_validate(dict(row)) for row in rows]\n            else:\n                row = await conn.fetchrow(obj[\"query\"], *obj.get(\"params\", []))\n                return subj_cls.model_validate(dict(row)) if row else None\n        finally:\n            await conn.close()\n</code></pre>"},{"location":"guides/async-patterns/#concurrent-operations","title":"Concurrent Operations","text":"<pre><code>class ConcurrentAdapter(AsyncAdapter[T]):\n    obj_key = \"concurrent\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        sources = obj[\"sources\"]\n        max_concurrent = kw.get(\"max_concurrent\", 5)\n\n        semaphore = asyncio.Semaphore(max_concurrent)\n\n        async def fetch_one(source):\n            async with semaphore:\n                # Fetch from individual source\n                return await SomeAdapter.from_obj(subj_cls, source)\n\n        results = await asyncio.gather(\n            *[fetch_one(source) for source in sources],\n            return_exceptions=True\n        )\n\n        # Filter successful results\n        successful = [r for r in results if not isinstance(r, Exception)]\n        return successful if many else (successful[0] if successful else None)\n</code></pre>"},{"location":"guides/async-patterns/#sharing-logic-between-syncasync","title":"Sharing Logic Between Sync/Async","text":"<p>Use mixins for shared transformation logic:</p> <pre><code>class DataTransformationMixin:\n    @staticmethod\n    def normalize_data(data: dict) -&gt; dict:\n        return {\n            \"id\": data.get(\"identifier\") or data.get(\"id\"),\n            \"name\": (data.get(\"name\") or \"\").strip(),\n            \"active\": data.get(\"status\") == \"active\",\n        }\n\n# Sync adapter\nclass MySyncAdapter(Adapter[T], DataTransformationMixin):\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        normalized = cls.normalize_data(obj)\n        return subj_cls.model_validate(normalized)\n\n# Async adapter using same logic\nclass MyAsyncAdapter(AsyncAdapter[T], DataTransformationMixin):\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        data = await cls._fetch_async_data(obj[\"url\"])\n        normalized = cls.normalize_data(data)\n        return subj_cls.model_validate(normalized)\n</code></pre>"},{"location":"guides/async-patterns/#error-handling-patterns","title":"Error Handling Patterns","text":""},{"location":"guides/async-patterns/#timeout-management","title":"Timeout Management","text":"<pre><code>class TimeoutAwareAdapter(AsyncAdapter[T]):\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        timeout_seconds = kw.get(\"timeout\", 30)\n\n        try:\n            async with asyncio.timeout(timeout_seconds):\n                return await cls._fetch_data(obj, subj_cls)\n        except asyncio.TimeoutError:\n            raise ParseError(f\"Operation timed out after {timeout_seconds}s\")\n</code></pre>"},{"location":"guides/async-patterns/#retry-logic","title":"Retry Logic","text":"<pre><code>class RetryableAdapter(AsyncAdapter[T]):\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        max_retries = kw.get(\"max_retries\", 3)\n\n        for attempt in range(max_retries + 1):\n            try:\n                return await cls._attempt_fetch(subj_cls, obj)\n            except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n                if attempt &lt; max_retries:\n                    await asyncio.sleep(2 ** attempt)  # Exponential backoff\n                    continue\n                raise ParseError(f\"Failed after {max_retries + 1} attempts: {e}\")\n</code></pre>"},{"location":"guides/async-patterns/#testing-async-adapters","title":"Testing Async Adapters","text":"<pre><code>@pytest.mark.asyncio\nclass TestAsyncAdapters:\n    async def test_http_adapter(self, respx_mock):\n        respx_mock.get(\"http://api.example.com/data\").mock(\n            return_value=httpx.Response(200, json={\"name\": \"test\"})\n        )\n\n        result = await RestApiAdapter.from_obj(\n            MyModel, {\"base_url\": \"http://api.example.com\", \"endpoint\": \"data\"}\n        )\n        assert result.name == \"test\"\n\n    async def test_timeout_handling(self):\n        with pytest.raises(ParseError, match=\"timed out\"):\n            await TimeoutAwareAdapter.from_obj(MyModel, config, timeout=0.1)\n</code></pre>"},{"location":"guides/async-patterns/#key-tips-for-llm-developers","title":"Key Tips for LLM Developers","text":""},{"location":"guides/async-patterns/#1-resource-management","title":"1. Resource Management","text":"<pre><code># \u2713 Always use context managers\nasync with aiohttp.ClientSession() as session:\n    async with session.get(url) as response:\n        data = await response.json()\n\n# \u2713 Proper cleanup\nconn = await asyncpg.connect(connection_string)\ntry:\n    result = await conn.fetch(query)\nfinally:\n    await conn.close()\n</code></pre>"},{"location":"guides/async-patterns/#2-concurrency-control","title":"2. Concurrency Control","text":"<pre><code># \u2713 Use semaphore for rate limiting\nsemaphore = asyncio.Semaphore(max_concurrent)\n\nasync def process_item(item):\n    async with semaphore:\n        # Process item without overwhelming external services\n        pass\n</code></pre>"},{"location":"guides/async-patterns/#3-common-async-caveats","title":"3. Common Async Caveats","text":"<ul> <li>Context managers: Always use <code>async with</code> for resource cleanup</li> <li>Timeouts: Set reasonable timeouts for external operations</li> <li>Error handling: Distinguish between retryable and non-retryable errors</li> <li>Concurrency limits: Use semaphores to avoid overwhelming external services</li> <li>Testing: Use <code>pytest.mark.asyncio</code> and mock async dependencies</li> </ul>"},{"location":"guides/async-patterns/#4-separation-of-concerns","title":"4. Separation of Concerns","text":"<pre><code># \u2713 Keep sync and async adapters separate\nclass SyncAdapter(Adapter[T]): pass\nclass AsyncAdapter(AsyncAdapter[T]): pass\n\n# \u2717 Avoid mixing sync/async in same class\n</code></pre>"},{"location":"guides/async-patterns/#5-performance-patterns","title":"5. Performance Patterns","text":"<ul> <li>Use <code>asyncio.gather()</code> for concurrent operations</li> <li>Implement connection pooling for database adapters</li> <li>Add circuit breakers for external service calls</li> <li>Use exponential backoff for retry logic</li> </ul> <p>This dual-API approach ensures optimal performance and clear separation between synchronous and asynchronous contexts.</p>"},{"location":"guides/creating-adapters/","title":"Creating Custom Adapters","text":""},{"location":"guides/creating-adapters/#adapter-interface","title":"Adapter Interface","text":"<p>All adapters implement the <code>Adapter</code> protocol:</p> <pre><code>from pydapter.core import Adapter\nfrom typing import ClassVar, TypeVar, Any\n\nT = TypeVar(\"T\", bound=BaseModel)\n\nclass MyAdapter(Adapter[T]):\n    obj_key: ClassVar[str] = \"my_format\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: Any, /, *, many=False, **kw) -&gt; T | list[T]:\n        \"\"\"Convert from external format to model\"\"\"\n        pass\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many=False, **kw) -&gt; Any:\n        \"\"\"Convert from model to external format\"\"\"\n        pass\n</code></pre>"},{"location":"guides/creating-adapters/#basic-implementation-pattern","title":"Basic Implementation Pattern","text":"<pre><code>from pydapter.exceptions import ParseError, ValidationError as AdapterValidationError\n\nclass YamlAdapter(Adapter[T]):\n    obj_key = \"yaml\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: str | Path, /, *, many=False, **kw):\n        try:\n            # Handle input types\n            text = obj.read_text() if isinstance(obj, Path) else obj\n\n            # Parse format\n            data = yaml.safe_load(text)\n\n            # Validate and convert\n            if many:\n                return [subj_cls.model_validate(item) for item in data]\n            return subj_cls.model_validate(data)\n\n        except yaml.YAMLError as e:\n            raise ParseError(f\"Invalid YAML: {e}\", source=str(obj)[:100])\n        except ValidationError as e:\n            raise AdapterValidationError(f\"Validation failed: {e}\", errors=e.errors())\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many=False, **kw) -&gt; str:\n        items = subj if isinstance(subj, list) else [subj]\n        payload = [item.model_dump() for item in items] if many else items[0].model_dump()\n        return yaml.dump(payload, **kw)\n</code></pre>"},{"location":"guides/creating-adapters/#error-handling","title":"Error Handling","text":""},{"location":"guides/creating-adapters/#exception-hierarchy","title":"Exception Hierarchy","text":"<ul> <li><code>ParseError</code>: Invalid format/data structure</li> <li><code>ValidationError</code>: Model validation failures</li> <li><code>AdapterError</code>: General adapter issues</li> </ul>"},{"location":"guides/creating-adapters/#error-context-pattern","title":"Error Context Pattern","text":"<pre><code>try:\n    # Adapter logic\n    pass\nexcept ParseError:\n    raise  # Re-raise pydapter exceptions\nexcept ValidationError as e:\n    raise AdapterValidationError(\"Validation failed\", data=data, errors=e.errors())\nexcept Exception as e:\n    raise ParseError(f\"Unexpected error: {e}\", source=str(obj)[:100])\n</code></pre>"},{"location":"guides/creating-adapters/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/creating-adapters/#configuration-support","title":"Configuration Support","text":"<pre><code>class DatabaseAdapter(Adapter[T]):\n    DEFAULT_CONFIG = {\"batch_size\": 1000, \"timeout\": 30}\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        config = {**cls.DEFAULT_CONFIG, **kw}\n        # Use config for connection settings, timeouts, etc.\n</code></pre>"},{"location":"guides/creating-adapters/#metadata-integration","title":"Metadata Integration","text":"<pre><code>@classmethod\ndef to_obj(cls, subj: T | list[T], /, *, many=False, **kw):\n    for field_name, field_info in subj.model_fields.items():\n        extra = field_info.json_schema_extra or {}\n\n        # Use field metadata for custom formatting\n        if extra.get(\"db_column\"):\n            # Map to different column name\n        if extra.get(\"vector_dim\"):\n            # Handle vector data specially\n</code></pre>"},{"location":"guides/creating-adapters/#async-adapters","title":"Async Adapters","text":"<pre><code>from pydapter.async_core import AsyncAdapter\n\nclass HttpApiAdapter(AsyncAdapter[T]):\n    obj_key = \"http_api\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        url = obj[\"url\"]\n\n        async with aiohttp.ClientSession() as session:\n            async with session.get(url) as response:\n                data = await response.json()\n\n        if many:\n            return [subj_cls.model_validate(item) for item in data]\n        return subj_cls.model_validate(data)\n</code></pre>"},{"location":"guides/creating-adapters/#testing-strategy","title":"Testing Strategy","text":"<pre><code>class TestMyAdapter:\n    def test_roundtrip(self):\n        \"\"\"Test data survives roundtrip conversion\"\"\"\n        original = MyModel(name=\"test\", value=42)\n        external = MyAdapter.to_obj(original)\n        restored = MyAdapter.from_obj(MyModel, external)\n        assert restored == original\n\n    def test_error_handling(self):\n        with pytest.raises(ParseError):\n            MyAdapter.from_obj(MyModel, \"invalid_data\")\n</code></pre>"},{"location":"guides/creating-adapters/#registry-integration","title":"Registry Integration","text":"<pre><code>from pydapter.core import AdapterRegistry\n\nregistry = AdapterRegistry()\nregistry.register(YamlAdapter)\n\n# Use through registry\nuser = registry.adapt_from(User, yaml_data, obj_key=\"yaml\")\n</code></pre>"},{"location":"guides/creating-adapters/#key-tips-for-llm-developers","title":"Key Tips for LLM Developers","text":""},{"location":"guides/creating-adapters/#1-stateless-design","title":"1. Stateless Design","text":"<ul> <li>Use <code>@classmethod</code> for all adapter methods</li> <li>No instance variables or shared state</li> <li>Thread-safe by design</li> </ul>"},{"location":"guides/creating-adapters/#2-error-handling","title":"2. Error Handling","text":"<ul> <li>Always provide context in error messages</li> <li>Use specific exception types</li> <li>Include source data preview (truncated)</li> </ul>"},{"location":"guides/creating-adapters/#3-input-validation","title":"3. Input Validation","text":"<pre><code># Validate input type and structure\nif not isinstance(obj, expected_types):\n    raise ParseError(f\"Expected {expected_types}, got {type(obj)}\")\n\n# Handle edge cases\nif obj is None:\n    return [] if many else None\n</code></pre>"},{"location":"guides/creating-adapters/#4-configuration-patterns","title":"4. Configuration Patterns","text":"<pre><code># Merge defaults with user config\nconfig = {**cls.DEFAULT_CONFIG, **kw}\n\n# Extract specific options\ntimeout = config.pop(\"timeout\", 30)\nbatch_size = config.pop(\"batch_size\", 1000)\n</code></pre>"},{"location":"guides/creating-adapters/#5-common-caveats","title":"5. Common Caveats","text":"<ul> <li>Many parameter: Handle both single items and lists consistently</li> <li>Empty inputs: Return appropriate empty values</li> <li>Path vs string: Support both file paths and direct content</li> <li>Async context: Proper resource cleanup with context managers</li> <li>Error propagation: Re-raise pydapter exceptions, wrap others</li> </ul>"},{"location":"guides/creating-adapters/#6-field-metadata-usage","title":"6. Field Metadata Usage","text":"<pre><code># Access field metadata for custom behavior\nfor field_name, field_info in model.model_fields.items():\n    extra = field_info.json_schema_extra or {}\n    if extra.get(\"custom_format\"):\n        # Apply custom formatting\n</code></pre> <p>This pattern ensures adapters integrate seamlessly with pydapter's ecosystem while maintaining consistency and reliability.</p>"},{"location":"guides/end-to-end-backend/","title":"Building Multi-Database Backends with Pydapter","text":""},{"location":"guides/end-to-end-backend/#overview","title":"Overview","text":"<p>This guide shows how to build flexible backends that work with multiple database backends using pydapter's protocol and adapter system, with async PostgreSQL as the primary database.</p>"},{"location":"guides/end-to-end-backend/#architecture-pattern","title":"Architecture Pattern","text":"<pre><code>Models (Protocols) \u2192 Adapters (DB-specific) \u2192 Registry (Unified Interface)\n</code></pre> <p>Core Concept: Define your models with protocols, implement database-specific adapters, use registry for database abstraction.</p>"},{"location":"guides/end-to-end-backend/#1-define-protocol-based-models","title":"1. Define Protocol-Based Models","text":"<pre><code>from pydantic import BaseModel\nfrom pydapter.protocols import IdentifiableMixin, TemporalMixin\nfrom uuid import UUID\nfrom datetime import datetime\n\nclass User(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID\n    created_at: datetime\n    updated_at: datetime\n    name: str\n    email: str\n    active: bool = True\n\nclass Post(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID\n    created_at: datetime\n    updated_at: datetime\n    title: str\n    content: str\n    author_id: UUID\n    published: bool = False\n</code></pre>"},{"location":"guides/end-to-end-backend/#2-database-adapter-pattern","title":"2. Database Adapter Pattern","text":"<pre><code>from pydapter.async_core import AsyncAdapter\n\nclass AsyncPostgresUserAdapter(AsyncAdapter[User]):\n    obj_key = \"postgres_user\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[User], obj: dict, /, *, many=False, **kw):\n        conn_string = obj[\"connection_string\"]\n        # Query logic here\n        pass\n\n    @classmethod\n    async def to_obj(cls, subj: User | list[User], /, *, many=False, **kw):\n        # Insert/update logic here\n        pass\n\n# Similar adapters for other databases\nclass AsyncMongoUserAdapter(AsyncAdapter[User]):\n    obj_key = \"mongo_user\"\n    # MongoDB-specific implementation\n\nclass AsyncNeo4jUserAdapter(AsyncAdapter[User]):\n    obj_key = \"neo4j_user\"\n    # Neo4j-specific implementation\n</code></pre>"},{"location":"guides/end-to-end-backend/#3-repository-pattern-with-registry","title":"3. Repository Pattern with Registry","text":"<pre><code>from pydapter.async_core import AsyncAdapterRegistry\n\nclass UserRepository:\n    def __init__(self, registry: AsyncAdapterRegistry, default_adapter: str = \"postgres_user\"):\n        self.registry = registry\n        self.default_adapter = default_adapter\n\n    async def get_by_id(self, user_id: UUID, adapter_key: str = None) -&gt; User | None:\n        adapter_key = adapter_key or self.default_adapter\n        query_config = {\n            \"connection_string\": self._get_connection_string(adapter_key),\n            \"query\": \"SELECT * FROM users WHERE id = $1\",\n            \"params\": [user_id]\n        }\n        return await self.registry.adapt_from(User, query_config, obj_key=adapter_key)\n\n    async def create(self, user: User, adapter_key: str = None) -&gt; User:\n        adapter_key = adapter_key or self.default_adapter\n        config = {\n            \"connection_string\": self._get_connection_string(adapter_key),\n            \"table\": \"users\"\n        }\n        return await self.registry.adapt_to(user, obj_key=adapter_key, **config)\n\n    def _get_connection_string(self, adapter_key: str) -&gt; str:\n        # Configuration lookup\n        pass\n</code></pre>"},{"location":"guides/end-to-end-backend/#4-service-layer","title":"4. Service Layer","text":"<pre><code>class UserService:\n    def __init__(self, user_repo: UserRepository, post_repo: PostRepository):\n        self.user_repo = user_repo\n        self.post_repo = post_repo\n\n    async def create_user_with_welcome_post(self, name: str, email: str) -&gt; tuple[User, Post]:\n        # Create user in primary database (PostgreSQL)\n        user = User(id=uuid4(), name=name, email=email,\n                   created_at=datetime.now(), updated_at=datetime.now())\n        created_user = await self.user_repo.create(user)\n\n        # Create welcome post\n        welcome_post = Post(\n            id=uuid4(), title=\"Welcome!\", content=f\"Welcome {name}!\",\n            author_id=created_user.id, created_at=datetime.now(), updated_at=datetime.now()\n        )\n        created_post = await self.post_repo.create(welcome_post)\n\n        # Optionally replicate to other databases for analytics\n        await self._replicate_to_analytics(created_user)\n\n        return created_user, created_post\n\n    async def _replicate_to_analytics(self, user: User):\n        \"\"\"Replicate user data to analytics database (e.g., MongoDB)\"\"\"\n        try:\n            await self.user_repo.create(user, adapter_key=\"mongo_user\")\n        except Exception as e:\n            # Log error but don't fail main operation\n            logger.warning(f\"Failed to replicate user to analytics: {e}\")\n</code></pre>"},{"location":"guides/end-to-end-backend/#5-configuration-and-setup","title":"5. Configuration and Setup","text":"<pre><code># Database configuration\nDATABASE_CONFIG = {\n    \"postgres\": {\n        \"connection_string\": \"postgresql://user:pass@localhost/main\",\n        \"primary\": True\n    },\n    \"mongo\": {\n        \"connection_string\": \"mongodb://localhost:27017/analytics\",\n        \"use_for\": [\"analytics\", \"caching\"]\n    },\n    \"neo4j\": {\n        \"connection_string\": \"bolt://localhost:7687\",\n        \"use_for\": [\"relationships\", \"recommendations\"]\n    }\n}\n\n# Registry setup\nasync def create_registry() -&gt; AsyncAdapterRegistry:\n    registry = AsyncAdapterRegistry()\n\n    # Register all adapters\n    registry.register(AsyncPostgresUserAdapter)\n    registry.register(AsyncMongoUserAdapter)\n    registry.register(AsyncNeo4jUserAdapter)\n    # ... register adapters for other models\n\n    return registry\n\n# Application factory\nasync def create_app():\n    registry = await create_registry()\n\n    user_repo = UserRepository(registry, default_adapter=\"postgres_user\")\n    post_repo = PostRepository(registry, default_adapter=\"postgres_post\")\n\n    user_service = UserService(user_repo, post_repo)\n\n    return user_service\n</code></pre>"},{"location":"guides/end-to-end-backend/#6-fastapi-integration","title":"6. FastAPI Integration","text":"<pre><code>from fastapi import FastAPI, Depends\n\napp = FastAPI()\n\nasync def get_user_service() -&gt; UserService:\n    return await create_app()\n\n@app.post(\"/users\")\nasync def create_user(\n    user_data: dict,\n    service: UserService = Depends(get_user_service)\n):\n    user, welcome_post = await service.create_user_with_welcome_post(\n        name=user_data[\"name\"],\n        email=user_data[\"email\"]\n    )\n    return {\"user\": user.model_dump(), \"welcome_post\": welcome_post.model_dump()}\n\n@app.get(\"/users/{user_id}\")\nasync def get_user(\n    user_id: UUID,\n    source: str = \"postgres\",  # Allow client to specify database\n    service: UserService = Depends(get_user_service)\n):\n    adapter_map = {\n        \"postgres\": \"postgres_user\",\n        \"mongo\": \"mongo_user\",\n        \"neo4j\": \"neo4j_user\"\n    }\n\n    user = await service.user_repo.get_by_id(\n        user_id,\n        adapter_key=adapter_map.get(source, \"postgres_user\")\n    )\n\n    if not user:\n        raise HTTPException(404, \"User not found\")\n\n    return user.model_dump()\n</code></pre>"},{"location":"guides/end-to-end-backend/#key-benefits","title":"Key Benefits","text":""},{"location":"guides/end-to-end-backend/#1-database-flexibility","title":"1. Database Flexibility","text":"<ul> <li>Primary Database: PostgreSQL for ACID transactions</li> <li>Analytics Database: MongoDB for flexible schema and aggregations</li> <li>Graph Database: Neo4j for relationship queries</li> <li>Easy Migration: Change adapters without changing business logic</li> </ul>"},{"location":"guides/end-to-end-backend/#2-protocol-consistency","title":"2. Protocol Consistency","text":"<ul> <li>Models work across all databases through protocols</li> <li>Type safety maintained regardless of storage backend</li> <li>Consistent validation and serialization</li> </ul>"},{"location":"guides/end-to-end-backend/#3-incremental-adoption","title":"3. Incremental Adoption","text":"<ul> <li>Start with one database, add others as needed</li> <li>Gradual migration between databases</li> <li>A/B testing with different storage backends</li> </ul>"},{"location":"guides/end-to-end-backend/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/end-to-end-backend/#multi-database-queries","title":"Multi-Database Queries","text":"<pre><code>async def get_user_with_recommendations(self, user_id: UUID):\n    # Get user from primary database\n    user = await self.user_repo.get_by_id(user_id, \"postgres_user\")\n\n    # Get recommendations from graph database\n    recommendations = await self.recommendation_service.get_for_user(\n        user_id, adapter_key=\"neo4j_recommendation\"\n    )\n\n    return {\"user\": user, \"recommendations\": recommendations}\n</code></pre>"},{"location":"guides/end-to-end-backend/#data-synchronization","title":"Data Synchronization","text":"<pre><code>async def sync_user_across_databases(self, user: User):\n    \"\"\"Ensure user exists in all required databases\"\"\"\n    tasks = [\n        self.user_repo.create(user, \"postgres_user\"),\n        self.user_repo.create(user, \"mongo_user\"),\n        self.user_repo.create(user, \"neo4j_user\"),\n    ]\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Handle partial failures\n    for i, result in enumerate(results):\n        if isinstance(result, Exception):\n            logger.error(f\"Failed to sync user to database {i}: {result}\")\n</code></pre>"},{"location":"guides/end-to-end-backend/#deployment-considerations","title":"Deployment Considerations","text":""},{"location":"guides/end-to-end-backend/#environment-configuration","title":"Environment Configuration","text":"<pre><code># Production: Use environment variables\nPOSTGRES_URL = os.getenv(\"DATABASE_URL\")\nMONGO_URL = os.getenv(\"MONGO_URL\")\nNEO4J_URL = os.getenv(\"NEO4J_URL\")\n\n# Development: Use local databases\n# Testing: Use in-memory or test databases\n</code></pre>"},{"location":"guides/end-to-end-backend/#monitoring-and-observability","title":"Monitoring and Observability","text":"<ul> <li>Log database adapter usage</li> <li>Monitor query performance across databases</li> <li>Track replication lag and sync errors</li> <li>Use health checks for each database</li> </ul> <p>This pattern provides a robust foundation for building scalable backends that can evolve with changing requirements while maintaining clean separation of concerns.</p>"},{"location":"guides/field-families/","title":"Field Families and Common Patterns Library","text":"<p>Field Families provide predefined collections of field templates for rapid model development. This powerful feature allows you to quickly create models with standard fields while maintaining consistency across your application.</p>"},{"location":"guides/field-families/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code>from pydapter.fields import DomainModelBuilder, FieldTemplate\nfrom pydapter.protocols import (\n    create_protocol_model_class,\n    IDENTIFIABLE,\n    TEMPORAL\n)\n\n# Option 1: Build a model with field families\nUser = (\n    DomainModelBuilder(\"User\")\n    .with_entity_fields()     # id, created_at, updated_at\n    .with_soft_delete()       # deleted_at, is_deleted\n    .with_audit_fields()      # created_by, updated_by, version\n    .add_field(\"email\", FieldTemplate(base_type=str))\n    .build()\n)\n\n# Option 2: Create a protocol-compliant model with behaviors\nUser = create_protocol_model_class(\n    \"User\",\n    IDENTIFIABLE,  # Adds id field + behavior\n    TEMPORAL,      # Adds timestamps + update_timestamp() method\n    email=FieldTemplate(base_type=str)\n)\n</code></pre>"},{"location":"guides/field-families/#core-concepts","title":"\ud83d\udcda Core Concepts","text":"<p>The Field Families system includes four main components:</p> <ol> <li>FieldFamilies - Predefined collections of field templates for core    database patterns</li> <li>DomainModelBuilder - Fluent API for building models with method chaining</li> <li>Protocol Integration - Type-safe protocol compliance with behavioral    methods</li> <li>ValidationPatterns - Common validation patterns and constraints</li> </ol>"},{"location":"guides/field-families/#using-field-families","title":"Using Field Families","text":""},{"location":"guides/field-families/#basic-usage","title":"Basic Usage","text":"<pre><code>from pydapter.fields import FieldFamilies, create_field_dict, create_model\n\n# Create a model with entity fields\nfields = create_field_dict(FieldFamilies.ENTITY)\nEntityModel = create_model(\"EntityModel\", fields=fields)\n\n# Combine multiple families\nfields = create_field_dict(\n    FieldFamilies.ENTITY,\n    FieldFamilies.AUDIT,\n    FieldFamilies.SOFT_DELETE\n)\nTrackedModel = create_model(\"TrackedModel\", fields=fields)\n</code></pre>"},{"location":"guides/field-families/#available-field-families","title":"\ud83d\udccb Available Field Families","text":"Family Fields Description ENTITY <code>id</code>, <code>created_at</code>, <code>updated_at</code> Basic entity fields ENTITY_TZ <code>id</code>, <code>created_at</code>, <code>updated_at</code> Entity with timezone-aware timestamps SOFT_DELETE <code>deleted_at</code>, <code>is_deleted</code> Soft delete support SOFT_DELETE_TZ <code>deleted_at</code>, <code>is_deleted</code> Soft delete with timezone-aware timestamp AUDIT <code>created_by</code>, <code>updated_by</code>, <code>version</code> Audit/tracking fields"},{"location":"guides/field-families/#domain-model-builder","title":"Domain Model Builder","text":"<p>The <code>DomainModelBuilder</code> provides a fluent API for creating models:</p> <pre><code>from pydapter.fields import DomainModelBuilder, FieldTemplate\n\n# Create a tracked entity model\nTrackedEntity = (\n    DomainModelBuilder(\"TrackedEntity\")\n    .with_entity_fields(timezone_aware=True)\n    .with_soft_delete(timezone_aware=True)\n    .with_audit_fields()\n    .add_field(\"name\", FieldTemplate(\n        base_type=str,\n        description=\"Entity name\"\n    ))\n    .add_field(\"status\", FieldTemplate(\n        base_type=str,\n        default=\"active\",\n        description=\"Entity status\"\n    ))\n    .build(from_attributes=True)\n)\n</code></pre>"},{"location":"guides/field-families/#builder-methods","title":"Builder Methods","text":"<ul> <li><code>with_entity_fields(timezone_aware=False)</code> - Add basic entity fields</li> <li><code>with_soft_delete(timezone_aware=False)</code> - Add soft delete fields</li> <li><code>with_audit_fields()</code> - Add audit fields</li> <li><code>with_family(family)</code> - Add a custom field family</li> <li><code>add_field(name, template, replace=True)</code> - Add a single field</li> <li><code>remove_field(name)</code> - Remove a field</li> <li><code>remove_fields(*names)</code> - Remove multiple fields</li> <li><code>preview()</code> - Preview fields before building</li> <li><code>build(**config)</code> - Build the final model</li> </ul>"},{"location":"guides/field-families/#protocol-integration","title":"\ud83d\udd0c Protocol Integration","text":"<p>Create models that comply with pydapter protocols:</p>"},{"location":"guides/field-families/#type-safe-protocol-constants","title":"Type-Safe Protocol Constants","text":"<pre><code>from pydapter.protocols import IDENTIFIABLE, TEMPORAL, EMBEDDABLE\nfrom pydapter.fields import create_protocol_model, FieldTemplate\n\n# Use type-safe constants instead of strings\nTrackedEntity = create_protocol_model(\n    \"TrackedEntity\",\n    IDENTIFIABLE,  # Instead of \"identifiable\"\n    TEMPORAL,      # Instead of \"temporal\"\n)\n</code></pre>"},{"location":"guides/field-families/#one-step-protocol-models-recommended","title":"\ud83c\udfaf One-Step Protocol Models (Recommended)","text":"<p>Use <code>create_protocol_model_class</code> for models with both fields AND behaviors:</p> <pre><code>from pydapter.protocols import (\n    create_protocol_model_class,\n    IDENTIFIABLE,\n    TEMPORAL\n)\n\n# Creates a model with fields AND methods in one step\nUser = create_protocol_model_class(\n    \"User\",\n    IDENTIFIABLE,  # Adds id field + behavior\n    TEMPORAL,      # Adds timestamps + update_timestamp() method\n    email=FieldTemplate(base_type=str),\n    name=FieldTemplate(base_type=str)\n)\n\n# Use the model\nuser = User(email=\"test@example.com\", name=\"Alice\")\nuser.update_timestamp()  # Method available!\n</code></pre>"},{"location":"guides/field-families/#supported-protocols","title":"\ud83d\udccb Supported Protocols","text":"Protocol Constant Fields Added Methods Added Identifiable <code>IDENTIFIABLE</code> <code>id</code> - Temporal <code>TEMPORAL</code> <code>created_at</code>, <code>updated_at</code> <code>update_timestamp()</code> Embeddable <code>EMBEDDABLE</code> <code>embedding</code> <code>parse_embedding_response()</code> Invokable <code>INVOKABLE</code> <code>execution</code> <code>invoke()</code> Cryptographical <code>CRYPTOGRAPHICAL</code> <code>sha256</code> <code>hash_content()</code>"},{"location":"guides/field-families/#alternative-approaches","title":"Alternative Approaches","text":"<p>For more control, you can use these alternative methods:</p> <pre><code># Option 1: Structure only (no methods)\nfrom pydapter.fields import create_protocol_model\n\nUserStructure = create_protocol_model(\n    \"UserStructure\",\n    IDENTIFIABLE,\n    TEMPORAL,\n    email=FieldTemplate(base_type=str)\n)\n\n# Option 2: Add behaviors to existing model\nfrom pydapter.protocols import combine_with_mixins\n\nUser = combine_with_mixins(UserStructure, IDENTIFIABLE, TEMPORAL)\n\n# Option 3: Manual composition\nfrom pydapter.protocols import IdentifiableMixin, TemporalMixin\n\nclass User(UserStructure, IdentifiableMixin, TemporalMixin):\n    pass\n</code></pre>"},{"location":"guides/field-families/#validation-patterns","title":"Validation Patterns","text":"<p>Use pre-built validation patterns for common field types:</p> <pre><code>from pydapter.fields import (\n    ValidationPatterns,\n    create_pattern_template,\n    create_range_template\n)\n\n# Use pre-built patterns\nemail_field = create_pattern_template(\n    ValidationPatterns.EMAIL,\n    description=\"User email address\",\n    error_message=\"Please enter a valid email\"\n)\n\n# Create custom patterns\nproduct_code = create_pattern_template(\n    r\"^[A-Z]{2}\\d{4}$\",\n    description=\"Product code\",\n    error_message=\"Product code must be 2 letters followed by 4 digits\"\n)\n\n# Create range-constrained fields\nage = create_range_template(\n    int,\n    ge=0,\n    le=150,\n    description=\"Person's age\"\n)\n\npercentage = create_range_template(\n    float,\n    ge=0,\n    le=100,\n    description=\"Percentage value\",\n    default=0.0\n)\n</code></pre>"},{"location":"guides/field-families/#available-patterns","title":"Available Patterns","text":"<p>ValidationPatterns provides regex patterns for:</p> <ul> <li>Email addresses</li> <li>URLs (HTTP/HTTPS)</li> <li>Phone numbers (US and international)</li> <li>Usernames</li> <li>Passwords</li> <li>Slugs and identifiers</li> <li>Color codes</li> <li>Dates and times</li> <li>Geographic data (latitude, longitude, ZIP codes)</li> <li>Financial data (credit cards, IBAN, Bitcoin addresses)</li> <li>Social media handles</li> </ul>"},{"location":"guides/field-families/#complete-example","title":"Complete Example","text":"<p>Here's a complete example combining all features:</p> <pre><code>from pydapter.fields import (\n    DomainModelBuilder,\n    FieldTemplate,\n    ValidationPatterns,\n    create_pattern_template,\n    create_range_template,\n)\n\n# Create an audited entity with validation\nAuditedEntity = (\n    DomainModelBuilder(\"AuditedEntity\")\n    .with_entity_fields(timezone_aware=True)\n    .with_soft_delete(timezone_aware=True)\n    .with_audit_fields()\n    # Add custom fields with validation\n    .add_field(\"name\", FieldTemplate(\n        base_type=str,\n        min_length=1,\n        max_length=100,\n        description=\"Entity name\"\n    ))\n    .add_field(\"email\", create_pattern_template(\n        ValidationPatterns.EMAIL,\n        description=\"Contact email\"\n    ))\n    .add_field(\"age\", create_range_template(\n        int,\n        ge=0,\n        le=150,\n        description=\"Age in years\"\n    ))\n    .add_field(\"score\", create_range_template(\n        float,\n        ge=0,\n        le=100,\n        description=\"Score percentage\"\n    ))\n    .add_field(\"website\", create_pattern_template(\n        ValidationPatterns.HTTPS_URL,\n        description=\"Website URL\",\n    ).as_nullable())\n    # Build with configuration\n    .build(\n        from_attributes=True,\n        validate_assignment=True\n    )\n)\n\n# Create an instance\nentity = AuditedEntity(\n    name=\"Example Entity\",\n    email=\"contact@example.com\",\n    age=25,\n    score=85.5\n)\n</code></pre>"},{"location":"guides/field-families/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"guides/field-families/#1-choose-the-right-approach","title":"1. Choose the Right Approach","text":"Use Case Recommended Approach Simple models with basic fields <code>DomainModelBuilder</code> Protocol-compliant models with behaviors <code>create_protocol_model_class()</code> Structure-only models <code>create_protocol_model()</code> Adding behaviors to existing models <code>combine_with_mixins()</code>"},{"location":"guides/field-families/#2-use-type-safe-constants","title":"2. Use Type-Safe Constants","text":"<pre><code># \u2705 Good - Type-safe and IDE-friendly\nfrom pydapter.protocols import IDENTIFIABLE, TEMPORAL\nmodel = create_protocol_model(\"Model\", IDENTIFIABLE, TEMPORAL)\n\n# \u274c Avoid - String literals are error-prone\nmodel = create_protocol_model(\"Model\", \"identifiable\", \"temporal\")\n</code></pre>"},{"location":"guides/field-families/#3-start-with-core-families","title":"3. Start with Core Families","text":"<p>Begin with the standard field families for consistency:</p> <ul> <li><code>ENTITY</code> for basic models</li> <li><code>SOFT_DELETE</code> for logical deletion</li> <li><code>AUDIT</code> for tracking changes</li> </ul>"},{"location":"guides/field-families/#4-compose-for-complex-models","title":"4. Compose for Complex Models","text":"<pre><code># Combine multiple patterns\nAuditedDocument = (\n    DomainModelBuilder(\"AuditedDocument\")\n    .with_entity_fields(timezone_aware=True)\n    .with_soft_delete()\n    .with_audit_fields()\n    .add_field(\"content\", FieldTemplate(base_type=str))\n    .build()\n)\n</code></pre>"},{"location":"guides/field-families/#5-preview-before-building","title":"5. Preview Before Building","text":"<p>Always preview your model structure:</p> <pre><code>builder = DomainModelBuilder(\"MyModel\").with_entity_fields()\nprint(builder.preview())  # Check fields before building\nmodel = builder.build()\n</code></pre>"},{"location":"guides/field-families/#6-keep-it-focused","title":"6. Keep It Focused","text":"<p>These families focus on database patterns. For domain-specific logic, create custom field templates and families.</p>"},{"location":"guides/fields-and-protocols-patterns/","title":"Fields and Protocols: Best Usage Patterns","text":"<p>This guide demonstrates the best practices for using pydapter's field system with protocols to create consistent, type-safe models for database operations.</p>"},{"location":"guides/fields-and-protocols-patterns/#core-concepts","title":"Core Concepts","text":""},{"location":"guides/fields-and-protocols-patterns/#1-field-templates-vs-direct-fields","title":"1. Field Templates vs Direct Fields","text":"<ul> <li>Field Templates: Reusable definitions that can create fields with different names</li> <li>Protocol Families: Pre-defined field sets that match protocol requirements</li> <li>Field Families: Logical groupings for common database patterns</li> </ul>"},{"location":"guides/fields-and-protocols-patterns/#2-protocol-alignment","title":"2. Protocol Alignment","text":"<p>Protocols define behavioral contracts. Use protocol-specific field families to ensure your models can leverage protocol functionality.</p> <p>Important Note: The <code>create_protocol_model</code> function provides structural compliance by adding the required fields for protocols. It does NOT add behavioral methods from protocol mixins. If you need methods like <code>update_timestamp()</code> from <code>TemporalMixin</code>, you must explicitly inherit from the mixin classes (see examples below).</p>"},{"location":"guides/fields-and-protocols-patterns/#pattern-1-basic-entity-with-protocols","title":"Pattern 1: Basic Entity with Protocols","text":"<p>For most database entities, combine Identifiable and Temporal protocols:</p> <pre><code>from pydapter.fields import create_protocol_model, FieldTemplate\nfrom pydapter.protocols import IDENTIFIABLE, TEMPORAL\n\n# Simple entity (structure only)\nUser = create_protocol_model(\n    \"User\",\n    IDENTIFIABLE,\n    TEMPORAL,\n    # Add domain-specific fields\n    username=FieldTemplate(base_type=str, max_length=50),\n    email=FieldTemplate(base_type=str),\n    is_active=FieldTemplate(base_type=bool, default=True)\n)\n\n# The model now has: id, created_at, updated_at, username, email, is_active\n# But NO behavioral methods yet\n\n# For full protocol compliance with methods, use create_protocol_model_class:\nfrom pydapter.protocols import create_protocol_model_class\n\nUser = create_protocol_model_class(\n    \"User\",\n    IDENTIFIABLE,\n    TEMPORAL,\n    username=FieldTemplate(base_type=str, max_length=50),\n    email=FieldTemplate(base_type=str),\n    is_active=FieldTemplate(base_type=bool, default=True)\n)\n\n# Now has both fields AND methods like update_timestamp()\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#pattern-2-versioned-entities-with-audit-trail","title":"Pattern 2: Versioned Entities with Audit Trail","text":"<p>For entities requiring version control and audit tracking:</p> <pre><code>from pydapter.fields import DomainModelBuilder, FieldTemplate\n\nOrder = (\n    DomainModelBuilder(\"Order\")\n    .with_entity_fields(timezone_aware=True)  # id, created_at, updated_at\n    .with_audit_fields()                      # created_by, updated_by, version\n    .add_field(\"order_number\", FieldTemplate(\n        base_type=str,\n        description=\"Unique order identifier\"\n    ))\n    .add_field(\"total_amount\", FieldTemplate(\n        base_type=float,\n        ge=0,\n        description=\"Order total\"\n    ))\n    .add_field(\"status\", FieldTemplate(\n        base_type=str,\n        default=\"pending\",\n        description=\"Order status\"\n    ))\n    .build()\n)\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#pattern-3-soft-deletable-entities","title":"Pattern 3: Soft-Deletable Entities","text":"<p>For entities that should never be hard-deleted:</p> <pre><code>from pydapter.fields import DomainModelBuilder, FieldTemplate\n\nProduct = (\n    DomainModelBuilder(\"Product\")\n    .with_entity_fields()\n    .with_soft_delete()  # Adds deleted_at, is_deleted\n    .add_field(\"name\", FieldTemplate(base_type=str))\n    .add_field(\"sku\", FieldTemplate(base_type=str))\n    .add_field(\"price\", FieldTemplate(base_type=float, ge=0))\n    .build()\n)\n\n# Usage in queries\nasync def get_active_products(adapter):\n    return await adapter.find_many({\"is_deleted\": False})\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#pattern-4-embeddable-documents","title":"Pattern 4: Embeddable Documents","text":"<p>For vector search and similarity operations:</p> <pre><code>from pydapter.fields import create_protocol_model, FieldTemplate\n\nDocument = create_protocol_model(\n    \"Document\",\n    \"identifiable\",\n    \"temporal\",\n    \"embeddable\",  # Adds embedding field\n    title=FieldTemplate(base_type=str),\n    content=FieldTemplate(base_type=str),\n    metadata=FieldTemplate(\n        base_type=dict,\n        default_factory=dict,\n        description=\"Additional document metadata\"\n    )\n)\n\n# Usage with vector adapters\nasync def search_similar(adapter, query_embedding):\n    return await adapter.similarity_search(\n        embedding=query_embedding,\n        limit=10\n    )\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#pattern-5-event-sourcing","title":"Pattern 5: Event Sourcing","text":"<p>For event-driven architectures, use the Event class directly:</p> <pre><code>from pydapter.protocols import Event\nfrom pydantic import Field\n\n# Extend the Event class for custom event types\nclass UserEvent(Event):\n    user_id: str = Field(..., description=\"User identifier\")\n    action: str = Field(..., description=\"Action performed\")\n    details: dict = Field(default_factory=dict, description=\"Additional details\")\n\n# Create events\nlogin_event = UserEvent(\n    handler=lambda: None,  # Required by Event\n    handler_arg=(),\n    handler_kwargs={},\n    event_type=\"user.login\",\n    user_id=\"user123\",\n    action=\"login\",\n    content={\"ip\": \"192.168.1.1\", \"user_agent\": \"...\"},\n    request={\"endpoint\": \"/api/login\", \"method\": \"POST\"}\n)\n\n# The Event class includes all protocol behaviors:\n# - IdentifiableMixin\n# - TemporalMixin\n# - EmbeddableMixin\n# - InvokableMixin\n# - CryptographicalMixin\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#pattern-6-custom-protocol-combinations","title":"Pattern 6: Custom Protocol Combinations","text":"<p>Mix protocols based on your needs:</p> <pre><code># Cryptographically signed document\nSignedDocument = create_protocol_model(\n    \"SignedDocument\",\n    \"identifiable\",\n    \"temporal\",\n    \"cryptographical\",  # Adds sha256 field\n    document_type=FieldTemplate(base_type=str),\n    content=FieldTemplate(base_type=str),\n    signature=FieldTemplate(base_type=str)\n)\n\n# Executable task with tracking\nTask = create_protocol_model(\n    \"Task\",\n    \"identifiable\",\n    \"temporal\",\n    \"invokable\",  # Adds execution field\n    name=FieldTemplate(base_type=str),\n    parameters=FieldTemplate(base_type=dict, default_factory=dict),\n    status=FieldTemplate(base_type=str, default=\"pending\")\n)\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#pattern-7-adding-behavioral-methods-to-protocol-models","title":"Pattern 7: Adding Behavioral Methods to Protocol Models","text":"<p>To get both structural fields AND behavioral methods from protocols:</p> <pre><code>from pydapter.fields import FieldTemplate\nfrom pydapter.protocols import (\n    IDENTIFIABLE, TEMPORAL,\n    create_protocol_model_class,\n    combine_with_mixins,\n    create_protocol_model\n)\n\n# Option 1: Use create_protocol_model_class (recommended)\nUser = create_protocol_model_class(\n    \"User\",\n    IDENTIFIABLE,\n    TEMPORAL,\n    username=FieldTemplate(base_type=str, max_length=50),\n    email=FieldTemplate(base_type=str)\n)\n\n# Option 2: Use combine_with_mixins\n# First create structure\nUserStructure = create_protocol_model(\n    \"UserStructure\",\n    IDENTIFIABLE,\n    TEMPORAL,\n    username=FieldTemplate(base_type=str, max_length=50),\n    email=FieldTemplate(base_type=str)\n)\n# Then add behaviors\nUser = combine_with_mixins(UserStructure, IDENTIFIABLE, TEMPORAL, name=\"User\")\n\n# Both options give you fields AND methods\nuser = User(username=\"john_doe\", email=\"john@example.com\")\nuser.update_timestamp()  # Method from TemporalMixin\nprint(user.created_at)   # Field from protocol\nprint(user.updated_at)   # Updated by the method call\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#pattern-8-validation-patterns","title":"Pattern 8: Validation Patterns","text":"<p>Use pre-built validation patterns for common field types:</p> <pre><code>from pydapter.fields import (\n    create_pattern_template,\n    create_range_template,\n    ValidationPatterns,\n    DomainModelBuilder\n)\n\nUserProfile = (\n    DomainModelBuilder(\"UserProfile\")\n    .with_entity_fields()\n    .add_field(\"username\", create_pattern_template(\n        ValidationPatterns.USERNAME_ALPHANUMERIC,\n        description=\"Alphanumeric username\"\n    ))\n    .add_field(\"email\", create_pattern_template(\n        ValidationPatterns.EMAIL,\n        description=\"User email address\"\n    ))\n    .add_field(\"age\", create_range_template(\n        int,\n        ge=13,\n        le=120,\n        description=\"User age\"\n    ))\n    .add_field(\"website\", create_pattern_template(\n        ValidationPatterns.HTTPS_URL,\n        description=\"Personal website\",\n        default=None\n    ).as_nullable())\n    .build()\n)\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#pattern-8-adapter-specific-metadata","title":"Pattern 8: Adapter-Specific Metadata","text":"<p>Add metadata for adapter optimization:</p> <pre><code>from pydapter.fields import FieldTemplate, DomainModelBuilder\n\nSearchableProduct = (\n    DomainModelBuilder(\"SearchableProduct\")\n    .with_entity_fields()\n    .add_field(\"name\", FieldTemplate(\n        base_type=str,\n        json_schema_extra={\"db_index\": True}\n    ))\n    .add_field(\"description\", FieldTemplate(\n        base_type=str,\n        json_schema_extra={\"db_fulltext\": True}\n    ))\n    .add_field(\"tags\", FieldTemplate(\n        base_type=list[str],\n        default_factory=list,\n        json_schema_extra={\"db_index\": True}\n    ))\n    .add_field(\"embedding\", FieldTemplate(\n        base_type=list[float],\n        default_factory=list,\n        json_schema_extra={\n            \"vector_dim\": 1536,\n            \"index_type\": \"hnsw\",\n            \"distance_metric\": \"cosine\"\n        }\n    ))\n    .build()\n)\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#best-practices","title":"Best Practices","text":""},{"location":"guides/fields-and-protocols-patterns/#1-choose-the-right-pattern","title":"1. Choose the Right Pattern","text":"<ul> <li>Simple CRUD: Use <code>create_protocol_model</code> with \"identifiable\" and \"temporal\"</li> <li>Audit Requirements: Add audit fields with <code>.with_audit_fields()</code></li> <li>Soft Delete: Use <code>.with_soft_delete()</code> for logical deletion</li> <li>Event Sourcing: Use the \"event\" protocol for complete event tracking</li> </ul>"},{"location":"guides/fields-and-protocols-patterns/#2-protocol-selection-guidelines","title":"2. Protocol Selection Guidelines","text":"<pre><code># Minimal entity\nModel = create_protocol_model(\"Model\", \"identifiable\")\n\n# Standard entity with timestamps\nModel = create_protocol_model(\"Model\", \"identifiable\", \"temporal\")\n\n# Searchable content\nModel = create_protocol_model(\"Model\", \"identifiable\", \"temporal\", \"embeddable\")\n\n# Event with all tracking\nModel = create_protocol_model(\"Model\", \"event\")\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#3-field-template-reuse","title":"3. Field Template Reuse","text":"<pre><code># Define common templates once\nfrom pydapter.fields import FieldTemplate\n\n# Reusable templates\nMONEY = FieldTemplate(\n    base_type=float,\n    ge=0,\n    description=\"Monetary amount\",\n    json_schema_extra={\"format\": \"money\", \"decimal_places\": 2}\n)\n\nSTATUS = FieldTemplate(\n    base_type=str,\n    description=\"Status field\",\n    default=\"active\",\n    json_schema_extra={\"enum\": [\"active\", \"inactive\", \"pending\"]}\n)\n\n# Use across models\nInvoice = create_protocol_model(\n    \"Invoice\",\n    \"identifiable\",\n    \"temporal\",\n    amount=MONEY,\n    status=STATUS\n)\n\nPayment = create_protocol_model(\n    \"Payment\",\n    \"identifiable\",\n    \"temporal\",\n    amount=MONEY,\n    status=STATUS.copy(default=\"pending\")\n)\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#4-timezone-awareness","title":"4. Timezone Awareness","text":"<p>Always specify timezone awareness explicitly:</p> <pre><code># Timezone-aware (recommended for distributed systems)\nModel = create_protocol_model(\"Model\", \"temporal\", timezone_aware=True)\n\n# Naive datetime (only for single-timezone applications)\nModel = create_protocol_model(\"Model\", \"temporal\", timezone_aware=False)\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#5-composing-complex-models","title":"5. Composing Complex Models","text":"<p>For complex requirements, combine approaches:</p> <pre><code># Start with protocol model, then enhance\nBaseOrder = create_protocol_model(\n    \"BaseOrder\",\n    \"identifiable\",\n    \"temporal\",\n    order_number=FieldTemplate(base_type=str)\n)\n\n# Extend with builder pattern\nCompleteOrder = (\n    DomainModelBuilder(\"CompleteOrder\")\n    .with_family({\n        \"id\": BaseOrder.model_fields[\"id\"],\n        \"created_at\": BaseOrder.model_fields[\"created_at\"],\n        \"updated_at\": BaseOrder.model_fields[\"updated_at\"],\n        \"order_number\": BaseOrder.model_fields[\"order_number\"]\n    })\n    .with_soft_delete()\n    .with_audit_fields()\n    .add_field(\"items\", FieldTemplate(\n        base_type=list[dict],\n        default_factory=list\n    ))\n    .build()\n)\n</code></pre>"},{"location":"guides/fields-and-protocols-patterns/#migration-strategy","title":"Migration Strategy","text":"<p>When migrating existing models:</p> <ol> <li>Identify which protocols your model should implement</li> <li>Use <code>create_protocol_model</code> to ensure protocol compliance</li> <li>Add domain-specific fields as extra parameters</li> <li>Gradually refactor to use field families for consistency</li> </ol> <pre><code># Before\nclass User(BaseModel):\n    id: UUID = Field(default_factory=uuid4)\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    username: str\n    email: str\n\n# After\nUser = create_protocol_model(\n    \"User\",\n    \"identifiable\",\n    \"temporal\",\n    username=FieldTemplate(base_type=str),\n    email=FieldTemplate(base_type=str)\n)\n</code></pre> <p>This approach ensures protocol compliance while maintaining flexibility for domain-specific needs.</p>"},{"location":"guides/fields/","title":"Working with Pydapter Fields","text":""},{"location":"guides/fields/#field-system-overview","title":"Field System Overview","text":"<p>Pydapter provides a comprehensive field system built on top of Pydantic v2 that includes:</p> <ul> <li>Field Templates: Reusable field definitions with flexible naming</li> <li>Common Templates: Pre-configured templates for common field types   (IDs, timestamps, etc.)</li> <li>Field Families: Logical groupings of fields for database patterns</li> <li>Protocol Families: Field sets that match pydapter protocol requirements</li> <li>Validation Patterns: Common regex patterns and constraint builders</li> <li>Domain Model Builder: Fluent API for creating models</li> </ul>"},{"location":"guides/fields/#field-templates","title":"Field Templates","text":"<p>Field templates are reusable field definitions that can be customized for different contexts:</p> <pre><code>from pydapter.fields import FieldTemplate\n\n# Define a reusable template\nname_template = FieldTemplate(\n    base_type=str,\n    description=\"Name field\",\n    min_length=1,\n    max_length=100\n)\n\n# Use in models with different field names\nfrom pydapter.fields import create_model\n\nUser = create_model(\n    \"User\",\n    fields={\n        \"username\": name_template.create_field(\"username\"),\n        \"full_name\": name_template.create_field(\"full_name\")\n    }\n)\n</code></pre>"},{"location":"guides/fields/#common-field-templates","title":"Common Field Templates","text":"<p>Pydapter provides pre-configured templates for common field types:</p> <pre><code>from pydapter.fields import (\n    ID_TEMPLATE,\n    CREATED_AT_TEMPLATE,\n    UPDATED_AT_TEMPLATE,\n    EMAIL_TEMPLATE,\n    URL_TEMPLATE,\n    JSON_TEMPLATE,\n    TAGS_TEMPLATE\n)\n\n# Using templates in model creation\nfields = {\n    \"id\": ID_TEMPLATE.create_field(\"id\"),\n    \"email\": EMAIL_TEMPLATE.create_field(\"email\"),\n    \"website\": URL_TEMPLATE.create_field(\"website\"),\n    \"tags\": TAGS_TEMPLATE.create_field(\"tags\")\n}\n\nUser = create_model(\"User\", fields=fields)\n</code></pre>"},{"location":"guides/fields/#field-families","title":"Field Families","text":"<p>Field families are logical groupings of fields for common database patterns:</p> <pre><code>from pydapter.fields import FieldFamilies, create_field_dict, create_model\n\n# Core families available:\n# - ENTITY: id, created_at, updated_at\n# - ENTITY_TZ: Same but with timezone-aware timestamps\n# - SOFT_DELETE: deleted_at, is_deleted\n# - AUDIT: created_by, updated_by, version\n\n# Combine families to create models\nfields = create_field_dict(\n    FieldFamilies.ENTITY,\n    FieldFamilies.AUDIT,\n    FieldFamilies.SOFT_DELETE\n)\n\nAuditedEntity = create_model(\"AuditedEntity\", fields=fields)\n</code></pre>"},{"location":"guides/fields/#domain-model-builder","title":"Domain Model Builder","text":"<p>The DomainModelBuilder provides a fluent API for creating models:</p> <pre><code>from pydapter.fields import DomainModelBuilder, FieldTemplate\n\n# Build a model with method chaining\nTrackedEntity = (\n    DomainModelBuilder(\"TrackedEntity\")\n    .with_entity_fields(timezone_aware=True)\n    .with_soft_delete(timezone_aware=True)\n    .with_audit_fields()\n    .add_field(\"name\", FieldTemplate(\n        base_type=str,\n        description=\"Entity name\",\n        max_length=100\n    ))\n    .add_field(\"status\", FieldTemplate(\n        base_type=str,\n        default=\"active\",\n        description=\"Entity status\"\n    ))\n    .build()\n)\n</code></pre>"},{"location":"guides/fields/#protocol-field-families","title":"Protocol Field Families","text":"<p>For models that need to implement pydapter protocols:</p> <pre><code>from pydapter.fields import create_protocol_model, FieldTemplate\n\n# Create a model implementing multiple protocols\nDocument = create_protocol_model(\n    \"Document\",\n    \"identifiable\",    # Adds id field\n    \"temporal\",        # Adds created_at, updated_at\n    \"embeddable\",      # Adds embedding field\n    \"cryptographical\", # Adds sha256 field\n    timezone_aware=True,\n    # Add custom fields\n    title=FieldTemplate(base_type=str, description=\"Document title\"),\n    content=FieldTemplate(base_type=str, description=\"Document content\")\n)\n</code></pre>"},{"location":"guides/fields/#validation-patterns","title":"Validation Patterns","text":"<p>Use pre-built validation patterns for common field types:</p> <pre><code>from pydapter.fields import (\n    ValidationPatterns,\n    create_pattern_template,\n    create_range_template\n)\n\n# Use pre-defined patterns\nslug_field = create_pattern_template(\n    ValidationPatterns.SLUG,\n    description=\"URL-friendly slug\",\n    error_message=\"Must contain only lowercase letters, numbers, and hyphens\"\n)\n\nphone_field = create_pattern_template(\n    ValidationPatterns.US_PHONE,\n    description=\"US phone number\"\n)\n\n# Create range-constrained fields\npercentage = create_range_template(\n    float,\n    ge=0,\n    le=100,\n    description=\"Percentage value\"\n)\n\nage = create_range_template(\n    int,\n    ge=0,\n    le=150,\n    description=\"Person's age\"\n)\n</code></pre>"},{"location":"guides/fields/#field-template-modifiers","title":"Field Template Modifiers","text":"<p>Field templates support transformation methods:</p> <pre><code>from pydapter.fields import ID_TEMPLATE, EMAIL_TEMPLATE\n\n# Make fields nullable\nnullable_id = ID_TEMPLATE.as_nullable()\noptional_email = EMAIL_TEMPLATE.as_nullable()\n\n# Copy with modifications\ncustom_id = ID_TEMPLATE.copy(\n    description=\"Custom identifier\",\n    frozen=False  # Make it mutable\n)\n\n# Change field properties\nlong_description = FieldTemplate(\n    base_type=str,\n    max_length=1000\n).copy(max_length=2000)\n</code></pre>"},{"location":"guides/fields/#complete-example","title":"Complete Example","text":"<p>Here's a comprehensive example combining the field system features:</p> <pre><code>from pydapter.fields import (\n    DomainModelBuilder,\n    FieldTemplate,\n    create_protocol_model,\n    create_pattern_template,\n    ValidationPatterns\n)\n\n# 1. Using DomainModelBuilder\nBlogPost = (\n    DomainModelBuilder(\"BlogPost\")\n    .with_entity_fields(timezone_aware=True)\n    .with_soft_delete()\n    .with_audit_fields()\n    .add_field(\"title\", FieldTemplate(\n        base_type=str,\n        description=\"Post title\",\n        max_length=200\n    ))\n    .add_field(\"slug\", create_pattern_template(\n        ValidationPatterns.SLUG,\n        description=\"URL slug\"\n    ))\n    .add_field(\"content\", FieldTemplate(\n        base_type=str,\n        description=\"Post content\"\n    ))\n    .build()\n)\n\n# 2. Using Protocol Models\nEmbeddableDocument = create_protocol_model(\n    \"EmbeddableDocument\",\n    \"identifiable\",\n    \"temporal\",\n    \"embeddable\",\n    title=FieldTemplate(base_type=str),\n    content=FieldTemplate(base_type=str),\n    tags=FieldTemplate(\n        base_type=list[str],\n        default_factory=list\n    )\n)\n\n# 3. Custom field family\ncustom_family = {\n    \"name\": FieldTemplate(base_type=str, max_length=100),\n    \"email\": EMAIL_TEMPLATE,\n    \"is_active\": FieldTemplate(base_type=bool, default=True)\n}\n\nCustomModel = (\n    DomainModelBuilder(\"CustomModel\")\n    .with_entity_fields()\n    .with_family(custom_family)\n    .build()\n)\n</code></pre>"},{"location":"guides/fields/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Templates over instances: Field templates can be reused across multiple fields</li> <li>Composition over inheritance: Build complex models by combining families</li> <li>Protocol alignment: Use protocol families for models that implement    pydapter protocols</li> <li>Validation patterns: Leverage pre-built patterns for common validation needs</li> <li>Fluent API: Use method chaining for readable model construction</li> </ol> <p>The field system provides a foundation for creating consistent, validated data models that integrate seamlessly with pydapter's adapter ecosystem.</p>"},{"location":"guides/protocols/","title":"Working with Pydapter Protocols","text":""},{"location":"guides/protocols/#protocol-mixin-pattern","title":"Protocol + Mixin Pattern","text":"<p>Each protocol provides:</p> <ol> <li>Protocol: Interface for type checking</li> <li>Mixin: Implementation with behavior</li> </ol> <pre><code>@runtime_checkable\nclass Identifiable(Protocol):\n    id: UUID\n\nclass IdentifiableMixin:\n    def __hash__(self) -&gt; int:\n        return hash(self.id)\n</code></pre>"},{"location":"guides/protocols/#available-protocols","title":"Available Protocols","text":""},{"location":"guides/protocols/#identifiable","title":"Identifiable","text":"<ul> <li>Purpose: UUID-based identity management</li> <li>Fields: <code>id: UUID</code></li> <li>Methods: <code>__hash__()</code>, UUID serialization</li> <li>Usage: Base for all tracked entities</li> </ul>"},{"location":"guides/protocols/#temporal","title":"Temporal","text":"<ul> <li>Purpose: Timestamp management</li> <li>Fields: <code>created_at: datetime</code>, <code>updated_at: datetime</code></li> <li>Methods: <code>update_timestamp()</code>, ISO datetime serialization</li> <li>Usage: Audit trails, versioning</li> </ul>"},{"location":"guides/protocols/#embeddable","title":"Embeddable","text":"<ul> <li>Purpose: Vector embeddings for ML/AI</li> <li>Fields: <code>content: str | None</code>, <code>embedding: list[float]</code></li> <li>Methods: Content processing, embedding validation</li> <li>Usage: RAG systems, semantic search</li> </ul>"},{"location":"guides/protocols/#event","title":"Event","text":"<ul> <li>Purpose: Comprehensive event tracking</li> <li>Inherits: Combines Identifiable, Temporal, Embeddable</li> <li>Usage: Event sourcing, audit logs</li> </ul>"},{"location":"guides/protocols/#composition-patterns","title":"Composition Patterns","text":""},{"location":"guides/protocols/#basic-composition","title":"Basic Composition","text":"<pre><code>class User(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID\n    created_at: datetime\n    updated_at: datetime\n    name: str\n    email: str\n</code></pre>"},{"location":"guides/protocols/#inheritance-order-matters","title":"Inheritance Order Matters","text":"<pre><code># \u2713 Correct: BaseModel first, dependency order for mixins\nclass Document(BaseModel, IdentifiableMixin, TemporalMixin, EmbeddableMixin):\n    # Protocol fields first\n    id: UUID\n    created_at: datetime\n    updated_at: datetime\n    content: str | None = None\n    embedding: list[float] = Field(default_factory=list)\n\n    # Domain fields\n    title: str\n</code></pre>"},{"location":"guides/protocols/#custom-protocol-creation","title":"Custom Protocol Creation","text":"<pre><code>@runtime_checkable\nclass Versionable(Protocol):\n    version: int\n    version_history: list[int]\n\nclass VersionableMixin:\n    def increment_version(self) -&gt; None:\n        if hasattr(self, 'version'):\n            self.version_history.append(self.version)\n            self.version += 1\n</code></pre>"},{"location":"guides/protocols/#type-checking","title":"Type Checking","text":""},{"location":"guides/protocols/#static-type-checking","title":"Static Type Checking","text":"<pre><code>def process_identifiable_items(items: list[Identifiable]) -&gt; list[UUID]:\n    return [item.id for item in items]\n</code></pre>"},{"location":"guides/protocols/#runtime-type-checking","title":"Runtime Type Checking","text":"<pre><code>def safe_get_id(obj: object) -&gt; UUID | None:\n    if isinstance(obj, Identifiable):\n        return obj.id\n    return None\n</code></pre>"},{"location":"guides/protocols/#integration-with-fields","title":"Integration with Fields","text":"<pre><code>from pydapter.fields import ID_FROZEN, DATETIME\n\nclass AdvancedModel(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID = ID_FROZEN.field_info\n    created_at: datetime = DATETIME.field_info\n    updated_at: datetime = DATETIME.field_info\n</code></pre>"},{"location":"guides/protocols/#key-tips-for-llm-developers","title":"Key Tips for LLM Developers","text":""},{"location":"guides/protocols/#1-protocol-contract-compliance","title":"1. Protocol Contract Compliance","text":"<ul> <li>Always implement all required protocol fields</li> <li>Use proper type annotations</li> <li>Test protocol compliance with <code>isinstance()</code></li> </ul>"},{"location":"guides/protocols/#2-mixin-order","title":"2. Mixin Order","text":"<ul> <li><code>BaseModel</code> first</li> <li>Protocol mixins in dependency order</li> <li>Custom mixins last</li> </ul>"},{"location":"guides/protocols/#3-automatic-serialization","title":"3. Automatic Serialization","text":"<ul> <li><code>IdentifiableMixin</code>: UUID \u2192 string</li> <li><code>TemporalMixin</code>: datetime \u2192 ISO string</li> <li>Use <code>model_dump_json()</code> for proper serialization</li> </ul>"},{"location":"guides/protocols/#4-common-patterns","title":"4. Common Patterns","text":"<pre><code># Standard composition for entities\nclass Entity(BaseModel, IdentifiableMixin, TemporalMixin):\n    pass\n\n# Standard composition for ML content\nclass MLContent(BaseModel, IdentifiableMixin, EmbeddableMixin):\n    pass\n\n# Standard composition for events\nclass EventRecord(BaseModel, Event):  # Event includes all protocols\n    pass\n</code></pre>"},{"location":"guides/protocols/#5-testing-protocol-implementation","title":"5. Testing Protocol Implementation","text":"<pre><code>def test_protocol_compliance(model_instance):\n    assert isinstance(model_instance, Identifiable)\n    assert hasattr(model_instance, 'id')\n    assert callable(getattr(model_instance, '__hash__'))\n</code></pre> <p>This protocol system enables consistent, type-safe behavior composition across your models while maintaining clean separation of concerns.</p>"},{"location":"guides/testing-strategies/","title":"Testing Strategies for Pydapter","text":""},{"location":"guides/testing-strategies/#protocol-testing","title":"Protocol Testing","text":""},{"location":"guides/testing-strategies/#protocol-compliance","title":"Protocol Compliance","text":"<pre><code>from pydapter.protocols import Identifiable, Temporal\n\ndef test_protocol_compliance():\n    model = MyModel(id=uuid4(), created_at=datetime.now(), updated_at=datetime.now())\n\n    # Runtime protocol checks\n    assert isinstance(model, Identifiable)\n    assert isinstance(model, Temporal)\n\n    # Test mixin functionality\n    original_updated = model.updated_at\n    model.update_timestamp()\n    assert model.updated_at &gt; original_updated\n</code></pre>"},{"location":"guides/testing-strategies/#adapter-testing","title":"Adapter Testing","text":""},{"location":"guides/testing-strategies/#roundtrip-testing","title":"Roundtrip Testing","text":"<pre><code>def test_adapter_roundtrip():\n    \"\"\"Test data survives roundtrip conversion\"\"\"\n    original = MyModel(name=\"test\", value=42)\n\n    external = MyAdapter.to_obj(original)\n    restored = MyAdapter.from_obj(MyModel, external)\n\n    assert restored.name == original.name\n    assert restored.value == original.value\n</code></pre>"},{"location":"guides/testing-strategies/#error-handling","title":"Error Handling","text":"<pre><code>def test_adapter_error_handling():\n    \"\"\"Test error scenarios\"\"\"\n    with pytest.raises(ParseError, match=\"Invalid format\"):\n        MyAdapter.from_obj(MyModel, \"invalid_data\")\n\n    with pytest.raises(ValidationError):\n        MyAdapter.from_obj(MyModel, {\"missing\": \"required_fields\"})\n</code></pre>"},{"location":"guides/testing-strategies/#async-testing","title":"Async Testing","text":"<pre><code>@pytest.mark.asyncio\nasync def test_async_adapter(respx_mock):\n    \"\"\"Test async adapters with mocked HTTP\"\"\"\n    respx_mock.get(\"http://api.example.com/data\").mock(\n        return_value=httpx.Response(200, json={\"name\": \"test\"})\n    )\n\n    result = await MyAsyncAdapter.from_obj(MyModel, {\"url\": \"http://api.example.com/data\"})\n    assert result.name == \"test\"\n</code></pre>"},{"location":"guides/testing-strategies/#registry-testing","title":"Registry Testing","text":"<pre><code>def test_registry_operations():\n    \"\"\"Test adapter registry functionality\"\"\"\n    registry = AdapterRegistry()\n    registry.register(MyAdapter)\n\n    # Test retrieval\n    adapter = registry.get(\"my_adapter\")\n    assert adapter == MyAdapter\n\n    # Test missing adapter\n    with pytest.raises(AdapterNotFoundError):\n        registry.get(\"nonexistent\")\n</code></pre>"},{"location":"guides/testing-strategies/#property-based-testing","title":"Property-Based Testing","text":"<pre><code>from hypothesis import given, strategies as st\n\n@given(st.text(min_size=1))\ndef test_field_validation_robustness(text_value):\n    \"\"\"Test field validators with random data\"\"\"\n    field = Field(name=\"test\", validator=lambda cls, v: v.strip())\n    # Test edge cases with generated data\n</code></pre>"},{"location":"guides/testing-strategies/#key-testing-patterns-for-llm-developers","title":"Key Testing Patterns for LLM Developers","text":""},{"location":"guides/testing-strategies/#1-test-fixtures","title":"1. Test Fixtures","text":"<pre><code>@pytest.fixture\ndef sample_user():\n    return User(\n        id=uuid4(),\n        created_at=datetime.now(),\n        updated_at=datetime.now(),\n        name=\"Test User\",\n        email=\"test@example.com\"\n    )\n\n@pytest.fixture\ndef user_registry():\n    registry = AdapterRegistry()\n    registry.register(JsonAdapter)\n    registry.register(CsvAdapter)\n    return registry\n</code></pre>"},{"location":"guides/testing-strategies/#2-mock-external-dependencies","title":"2. Mock External Dependencies","text":"<pre><code># HTTP APIs\n@pytest.fixture\ndef mock_api(respx_mock):\n    respx_mock.get(\"http://api.example.com/users\").mock(\n        return_value=httpx.Response(200, json=[{\"name\": \"John\", \"age\": 30}])\n    )\n\n# Database connections\n@pytest.fixture\ndef mock_db():\n    with patch('asyncpg.connect') as mock_connect:\n        mock_conn = AsyncMock()\n        mock_connect.return_value = mock_conn\n        yield mock_conn\n</code></pre>"},{"location":"guides/testing-strategies/#3-error-path-testing","title":"3. Error Path Testing","text":"<pre><code>def test_all_error_scenarios():\n    \"\"\"Comprehensive error testing\"\"\"\n    # Empty input\n    with pytest.raises(ParseError, match=\"Empty.*content\"):\n        MyAdapter.from_obj(MyModel, \"\")\n\n    # Invalid format\n    with pytest.raises(ParseError, match=\"Invalid.*format\"):\n        MyAdapter.from_obj(MyModel, \"invalid_format\")\n\n    # Validation failure\n    with pytest.raises(ValidationError):\n        MyAdapter.from_obj(MyModel, {\"missing_required_field\": True})\n</code></pre>"},{"location":"guides/testing-strategies/#4-async-testing-patterns","title":"4. Async Testing Patterns","text":"<pre><code>@pytest.mark.asyncio\nclass TestAsyncOperations:\n    async def test_concurrent_operations(self):\n        \"\"\"Test concurrent adapter operations\"\"\"\n        tasks = [\n            MyAsyncAdapter.from_obj(MyModel, {\"id\": i})\n            for i in range(10)\n        ]\n        results = await asyncio.gather(*tasks)\n        assert len(results) == 10\n\n    async def test_timeout_handling(self):\n        \"\"\"Test timeout scenarios\"\"\"\n        with pytest.raises(ParseError, match=\"timed out\"):\n            await MyAsyncAdapter.from_obj(MyModel, config, timeout=0.01)\n</code></pre>"},{"location":"guides/testing-strategies/#common-testing-caveats","title":"Common Testing Caveats","text":""},{"location":"guides/testing-strategies/#1-async-context","title":"1. Async Context","text":"<ul> <li>Use <code>pytest.mark.asyncio</code> for async tests</li> <li>Mock external services (HTTP, database)</li> <li>Test timeout and retry logic</li> </ul>"},{"location":"guides/testing-strategies/#2-protocol-mixins","title":"2. Protocol Mixins","text":"<ul> <li>Test both interface and implementation</li> <li>Verify field serializers</li> <li>Check inheritance order effects</li> </ul>"},{"location":"guides/testing-strategies/#3-registry-isolation","title":"3. Registry Isolation","text":"<ul> <li>Use fresh registries per test</li> <li>Clean up registered adapters</li> <li>Test adapter precedence</li> </ul>"},{"location":"guides/testing-strategies/#4-error-context","title":"4. Error Context","text":"<ul> <li>Verify specific exception types</li> <li>Check error message content</li> <li>Test error data preservation</li> </ul>"},{"location":"guides/testing-strategies/#testing-tips","title":"Testing Tips","text":"<ul> <li>Fixtures: Use pytest fixtures for common setups</li> <li>Mocking: Mock external dependencies consistently</li> <li>Error paths: Test failures as thoroughly as success</li> <li>Property-based: Use Hypothesis for edge case discovery</li> <li>Type safety: Run mypy in CI to catch type errors</li> <li>Isolation: Ensure tests don't affect each other</li> </ul>"},{"location":"tutorials/using_migrations/","title":"Tutorial: Managing Database Schema Evolution with Migrations","text":"<p>This tutorial demonstrates how to use pydapter's migrations module to manage database schema changes in a SQLAlchemy-based application. We'll create a simple user management system and evolve its schema over time using migrations.</p>"},{"location":"tutorials/using_migrations/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have installed pydapter with the migrations-sql extension:</p> <pre><code>pip install pydapter[migrations-sql]\n</code></pre> <p>This will install the required dependencies, including SQLAlchemy and Alembic.</p>"},{"location":"tutorials/using_migrations/#step-1-set-up-the-project-structure","title":"Step 1: Set Up the Project Structure","text":"<p>First, let's create a basic project structure:</p> <pre><code>user_management/\n\u251c\u2500\u2500 migrations/        # Will be created by the migration tool\n\u251c\u2500\u2500 models.py          # SQLAlchemy models\n\u251c\u2500\u2500 database.py        # Database connection setup\n\u2514\u2500\u2500 main.py            # Application entry point\n</code></pre>"},{"location":"tutorials/using_migrations/#step-2-define-the-initial-database-models","title":"Step 2: Define the Initial Database Models","text":"<p>Let's create our initial database models in <code>models.py</code>:</p> <pre><code># models.py\nfrom sqlalchemy import Column, Integer, String, DateTime, func\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = \"users\"\n\n    id = Column(Integer, primary_key=True)\n    username = Column(String(50), unique=True, nullable=False)\n    email = Column(String(100), unique=True, nullable=False)\n    created_at = Column(DateTime, default=func.now())\n</code></pre> <p>Next, let's set up the database connection in <code>database.py</code>:</p> <pre><code># database.py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\n# Use SQLite for simplicity in this tutorial\nDATABASE_URL = \"sqlite:///./user_management.db\"\n\nengine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n</code></pre>"},{"location":"tutorials/using_migrations/#step-3-initialize-migrations","title":"Step 3: Initialize Migrations","text":"<p>Now, let's initialize the migrations environment. Create a simple script in <code>main.py</code>:</p> <pre><code># main.py\nimport os\nfrom pydapter.migrations import AlembicAdapter\nimport models\n\ndef init_migrations():\n    \"\"\"Initialize the migrations environment.\"\"\"\n    os.makedirs(\"migrations\", exist_ok=True)\n\n    AlembicAdapter.init_migrations(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\",\n        models_module=models\n    )\n    print(\"Migrations initialized successfully!\")\n\nif __name__ == \"__main__\":\n    init_migrations()\n</code></pre> <p>Run this script to initialize the migrations environment:</p> <pre><code>python main.py\n</code></pre> <p>This will create the necessary directory structure and configuration files for Alembic in the <code>migrations</code> directory.</p>"},{"location":"tutorials/using_migrations/#step-4-create-the-initial-migration","title":"Step 4: Create the Initial Migration","text":"<p>Now, let's create our first migration to set up the initial database schema:</p> <pre><code># main.py (add this function)\ndef create_initial_migration():\n    \"\"\"Create the initial migration.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Create users table\",\n        autogenerate=True,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created initial migration: {revision}\")\n\nif __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    create_initial_migration()\n</code></pre> <p>Run the script again to create the initial migration:</p> <pre><code>python main.py\n</code></pre> <p>This will create a new migration file in the <code>migrations/versions/</code> directory with a unique revision ID.</p>"},{"location":"tutorials/using_migrations/#step-5-apply-the-migration","title":"Step 5: Apply the Migration","text":"<p>Now, let's apply the migration to create the database schema:</p> <pre><code># main.py (add this function)\ndef apply_migrations():\n    \"\"\"Apply all pending migrations.\"\"\"\n    AlembicAdapter.upgrade(\n        revision=\"head\",\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(\"Migrations applied successfully!\")\n\nif __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    apply_migrations()\n</code></pre> <p>Run the script to apply the migration:</p> <pre><code>python main.py\n</code></pre> <p>This will create the <code>users</code> table in the database according to our model definition.</p>"},{"location":"tutorials/using_migrations/#step-6-evolve-the-schema","title":"Step 6: Evolve the Schema","text":"<p>Now, let's evolve our schema by adding new fields to the <code>User</code> model:</p> <pre><code># models.py (updated)\nfrom sqlalchemy import Column, Integer, String, DateTime, Boolean, func\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = \"users\"\n\n    id = Column(Integer, primary_key=True)\n    username = Column(String(50), unique=True, nullable=False)\n    email = Column(String(100), unique=True, nullable=False)\n    full_name = Column(String(100))  # New field\n    is_active = Column(Boolean, default=True)  # New field\n    created_at = Column(DateTime, default=func.now())\n    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())  # New field\n</code></pre> <p>Now, let's create a new migration to reflect these changes:</p> <pre><code># main.py (add this function)\ndef create_schema_update_migration():\n    \"\"\"Create a migration for schema updates.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Add user profile fields\",\n        autogenerate=True,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created schema update migration: {revision}\")\n\nif __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # apply_migrations()  # Comment this out after first run\n    create_schema_update_migration()\n</code></pre> <p>Run the script to create the new migration:</p> <pre><code>python main.py\n</code></pre> <p>Then apply the migration:</p> <pre><code>if __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # create_schema_update_migration()  # Comment this out after first run\n    apply_migrations()\n</code></pre> <pre><code>python main.py\n</code></pre>"},{"location":"tutorials/using_migrations/#step-7-add-a-new-model","title":"Step 7: Add a New Model","text":"<p>Let's add a new model to our application for user roles:</p> <pre><code># models.py (updated)\nfrom sqlalchemy import Column, Integer, String, DateTime, Boolean, ForeignKey, func\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = \"users\"\n\n    id = Column(Integer, primary_key=True)\n    username = Column(String(50), unique=True, nullable=False)\n    email = Column(String(100), unique=True, nullable=False)\n    full_name = Column(String(100))\n    is_active = Column(Boolean, default=True)\n    created_at = Column(DateTime, default=func.now())\n    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())\n\n    # Add relationship to roles\n    roles = relationship(\"UserRole\", back_populates=\"user\")\n\n\nclass UserRole(Base):\n    __tablename__ = \"user_roles\"\n\n    id = Column(Integer, primary_key=True)\n    user_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n    role_name = Column(String(50), nullable=False)\n    created_at = Column(DateTime, default=func.now())\n\n    # Add relationship to user\n    user = relationship(\"User\", back_populates=\"roles\")\n</code></pre> <p>Now, let's create a migration for this new model:</p> <pre><code># main.py (add this function)\ndef create_roles_migration():\n    \"\"\"Create a migration for the roles model.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Add user roles table\",\n        autogenerate=True,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created roles migration: {revision}\")\n\nif __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # apply_migrations()  # Comment this out after first run\n    # create_schema_update_migration()  # Comment this out after first run\n    create_roles_migration()\n</code></pre> <p>Run the script to create the new migration:</p> <pre><code>python main.py\n</code></pre> <p>Then apply the migration:</p> <pre><code>if __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # create_schema_update_migration()  # Comment this out after first run\n    # create_roles_migration()  # Comment this out after first run\n    apply_migrations()\n</code></pre> <pre><code>python main.py\n</code></pre>"},{"location":"tutorials/using_migrations/#step-8-check-migration-status","title":"Step 8: Check Migration Status","text":"<p>Let's add functionality to check the current migration status:</p> <pre><code># main.py (add this function)\ndef check_migration_status():\n    \"\"\"Check the current migration status.\"\"\"\n    current = AlembicAdapter.get_current_revision(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Current migration revision: {current}\")\n\n    history = AlembicAdapter.get_migration_history(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(\"\\nMigration history:\")\n    for migration in history:\n        print(f\"- {migration['revision']}: {migration.get('description', 'No description')}\")\n\nif __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # apply_migrations()  # Comment this out after first run\n    # create_schema_update_migration()  # Comment this out after first run\n    # create_roles_migration()  # Comment this out after first run\n    check_migration_status()\n</code></pre> <p>Run the script to check the migration status:</p> <pre><code>python main.py\n</code></pre>"},{"location":"tutorials/using_migrations/#step-9-downgrade-to-a-previous-version","title":"Step 9: Downgrade to a Previous Version","text":"<p>Sometimes you might need to revert to a previous version of your schema. Let's add functionality to downgrade:</p> <pre><code># main.py (add this function)\ndef downgrade_migration(revision):\n    \"\"\"Downgrade to a specific revision.\"\"\"\n    AlembicAdapter.downgrade(\n        revision=revision,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Downgraded to revision: {revision}\")\n\nif __name__ == \"__main__\":\n    # Get the second-to-last revision from history\n    history = AlembicAdapter.get_migration_history(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    if len(history) &gt;= 2:\n        previous_revision = history[-2]['revision']\n        downgrade_migration(previous_revision)\n    else:\n        print(\"Not enough migrations to downgrade\")\n</code></pre> <p>Run the script to downgrade to the previous migration:</p> <pre><code>python main.py\n</code></pre>"},{"location":"tutorials/using_migrations/#step-10-create-a-custom-migration","title":"Step 10: Create a Custom Migration","text":"<p>Sometimes you need to create custom migrations that aren't just schema changes. Let's create a data migration:</p> <pre><code># main.py (add this function)\ndef create_custom_migration():\n    \"\"\"Create a custom data migration.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Add default admin user\",\n        autogenerate=False,  # Don't auto-generate from models\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created custom migration: {revision}\")\n    print(f\"Edit the migration file in migrations/versions/{revision}_add_default_admin_user.py\")\n\nif __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # apply_migrations()  # Comment this out after first run\n    # create_schema_update_migration()  # Comment this out after first run\n    # create_roles_migration()  # Comment this out after first run\n    # check_migration_status()  # Comment this out after first run\n    create_custom_migration()\n</code></pre> <p>Run the script to create the custom migration:</p> <pre><code>python main.py\n</code></pre> <p>Now, edit the generated migration file in <code>migrations/versions/</code> to add custom SQL operations:</p> <pre><code>\"\"\"Add default admin user\n\nRevision ID: &lt;your_revision_id&gt;\nRevises: &lt;previous_revision_id&gt;\nCreate Date: 2025-05-16 12:00:00.000000\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n# revision identifiers\nrevision = '&lt;your_revision_id&gt;'\ndown_revision = '&lt;previous_revision_id&gt;'\nbranch_labels = None\ndepends_on = None\n\ndef upgrade():\n    # Add a default admin user\n    op.execute(\"\"\"\n    INSERT INTO users (username, email, full_name, is_active)\n    VALUES ('admin', 'admin@example.com', 'System Administrator', 1)\n    \"\"\")\n\n    # Get the user ID\n    conn = op.get_bind()\n    result = conn.execute(\"SELECT id FROM users WHERE username = 'admin'\").fetchone()\n    if result:\n        user_id = result[0]\n        # Add admin role\n        op.execute(f\"\"\"\n        INSERT INTO user_roles (user_id, role_name)\n        VALUES ({user_id}, 'admin')\n        \"\"\")\n\ndef downgrade():\n    # Remove the admin role and user\n    op.execute(\"DELETE FROM user_roles WHERE role_name = 'admin'\")\n    op.execute(\"DELETE FROM users WHERE username = 'admin'\")\n</code></pre> <p>Then apply the migration:</p> <pre><code>if __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # create_schema_update_migration()  # Comment this out after first run\n    # create_roles_migration()  # Comment this out after first run\n    # check_migration_status()  # Comment this out after first run\n    # create_custom_migration()  # Comment this out after first run\n    apply_migrations()\n</code></pre> <pre><code>python main.py\n</code></pre>"},{"location":"tutorials/using_migrations/#step-11-using-async-migrations","title":"Step 11: Using Async Migrations","text":"<p>If your application uses asynchronous database connections, you can use the async migration adapter. Let's modify our code to use async migrations:</p> <pre><code># database.py (updated for async)\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.orm import sessionmaker\n\n# Use SQLite for simplicity in this tutorial\nDATABASE_URL = \"sqlite+aiosqlite:///./user_management_async.db\"\n\nengine = create_async_engine(DATABASE_URL)\nAsyncSessionLocal = sessionmaker(\n    class_=AsyncSession,\n    autocommit=False,\n    autoflush=False,\n    bind=engine\n)\n\nasync def get_db():\n    async with AsyncSessionLocal() as db:\n        yield db\n</code></pre> <pre><code># main_async.py\nimport asyncio\nimport os\nfrom pydapter.migrations import AsyncAlembicAdapter\nimport models\n\nasync def init_migrations():\n    \"\"\"Initialize the migrations environment.\"\"\"\n    os.makedirs(\"migrations_async\", exist_ok=True)\n\n    await AsyncAlembicAdapter.init_migrations(\n        directory=\"migrations_async\",\n        connection_string=\"sqlite+aiosqlite:///./user_management_async.db\",\n        models_module=models\n    )\n    print(\"Async migrations initialized successfully!\")\n\nasync def create_initial_migration():\n    \"\"\"Create the initial migration.\"\"\"\n    revision = await AsyncAlembicAdapter.create_migration(\n        message=\"Create users table\",\n        autogenerate=True,\n        directory=\"migrations_async\",\n        connection_string=\"sqlite+aiosqlite:///./user_management_async.db\"\n    )\n    print(f\"Created initial async migration: {revision}\")\n\nasync def apply_migrations():\n    \"\"\"Apply all pending migrations.\"\"\"\n    await AsyncAlembicAdapter.upgrade(\n        revision=\"head\",\n        directory=\"migrations_async\",\n        connection_string=\"sqlite+aiosqlite:///./user_management_async.db\"\n    )\n    print(\"Async migrations applied successfully!\")\n\nasync def main():\n    await init_migrations()\n    await create_initial_migration()\n    await apply_migrations()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Run the async script:</p> <pre><code>python main_async.py\n</code></pre>"},{"location":"tutorials/using_migrations/#complete-example","title":"Complete Example","text":"<p>Here's a complete example of the main.py file that includes all the migration operations:</p> <pre><code># main.py\nimport os\nfrom pydapter.migrations import AlembicAdapter\nimport models\n\ndef init_migrations():\n    \"\"\"Initialize the migrations environment.\"\"\"\n    os.makedirs(\"migrations\", exist_ok=True)\n\n    AlembicAdapter.init_migrations(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\",\n        models_module=models\n    )\n    print(\"Migrations initialized successfully!\")\n\ndef create_initial_migration():\n    \"\"\"Create the initial migration.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Create users table\",\n        autogenerate=True,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created initial migration: {revision}\")\n\ndef apply_migrations():\n    \"\"\"Apply all pending migrations.\"\"\"\n    AlembicAdapter.upgrade(\n        revision=\"head\",\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(\"Migrations applied successfully!\")\n\ndef create_schema_update_migration():\n    \"\"\"Create a migration for schema updates.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Add user profile fields\",\n        autogenerate=True,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created schema update migration: {revision}\")\n\ndef create_roles_migration():\n    \"\"\"Create a migration for the roles model.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Add user roles table\",\n        autogenerate=True,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created roles migration: {revision}\")\n\ndef check_migration_status():\n    \"\"\"Check the current migration status.\"\"\"\n    current = AlembicAdapter.get_current_revision(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Current migration revision: {current}\")\n\n    history = AlembicAdapter.get_migration_history(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(\"\\nMigration history:\")\n    for migration in history:\n        print(f\"- {migration['revision']}: {migration.get('description', 'No description')}\")\n\ndef downgrade_migration(revision):\n    \"\"\"Downgrade to a specific revision.\"\"\"\n    AlembicAdapter.downgrade(\n        revision=revision,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Downgraded to revision: {revision}\")\n\ndef create_custom_migration():\n    \"\"\"Create a custom data migration.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Add default admin user\",\n        autogenerate=False,  # Don't auto-generate from models\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created custom migration: {revision}\")\n    print(f\"Edit the migration file in migrations/versions/{revision}_add_default_admin_user.py\")\n\nif __name__ == \"__main__\":\n    # Uncomment the function you want to run\n    # init_migrations()\n    # create_initial_migration()\n    # apply_migrations()\n    # create_schema_update_migration()\n    # create_roles_migration()\n    # check_migration_status()\n\n    # Downgrade example\n    # history = AlembicAdapter.get_migration_history(\n    #     directory=\"migrations\",\n    #     connection_string=\"sqlite:///./user_management.db\"\n    # )\n    # if len(history) &gt;= 2:\n    #     previous_revision = history[-2]['revision']\n    #     downgrade_migration(previous_revision)\n    # else:\n    #     print(\"Not enough migrations to downgrade\")\n\n    # create_custom_migration()\n\n    # Final upgrade to latest\n    apply_migrations()\n</code></pre>"},{"location":"tutorials/using_migrations/#summary","title":"Summary","text":"<p>In this tutorial, we've demonstrated how to use pydapter's migrations module to manage database schema evolution. We've covered:</p> <ol> <li>Setting up a migrations environment</li> <li>Creating and applying initial migrations</li> <li>Evolving the schema with new fields</li> <li>Adding new models</li> <li>Checking migration status</li> <li>Downgrading to previous versions</li> <li>Creating custom data migrations</li> <li>Using async migrations</li> </ol> <p>The migrations module provides a powerful way to manage database schema changes in a controlled, versioned manner, making it easier to evolve your application's data model over time.</p>"},{"location":"tutorials/using_migrations/#best-practices","title":"Best Practices","text":"<p>Here are some best practices to follow when working with migrations:</p> <ol> <li>Always back up your database before applying migrations in production</li> <li>Keep migrations small and focused on specific changes</li> <li>Test migrations thoroughly in development before applying to production</li> <li>Include descriptive messages for each migration</li> <li>Use version control for your migration files</li> <li>Consider using separate migration environments for different deployment    stages</li> <li>Document complex migrations with comments</li> <li>Include both upgrade and downgrade operations when possible</li> </ol>"},{"location":"tutorials/using_protocols/","title":"Tutorial: Using Protocols to Create Standardized Models","text":"<p>This tutorial demonstrates how to use the pydapter protocols module to create models with standardized capabilities. We'll build a simple document management system that leverages the protocol interfaces to provide consistent behavior across different types of documents.</p>"},{"location":"tutorials/using_protocols/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have installed pydapter with the protocols extension:</p> <pre><code>pip install pydapter[protocols]\n</code></pre>"},{"location":"tutorials/using_protocols/#step-1-define-base-document-models","title":"Step 1: Define Base Document Models","text":"<p>First, let's define our base document models using the protocols:</p> <pre><code>from datetime import datetime\nfrom uuid import UUID\n\nfrom pydapter.protocols import Identifiable, Temporal, Embedable\n\nclass BaseDocument(Identifiable, Temporal):\n    \"\"\"Base document class with ID and timestamp tracking.\"\"\"\n\n    title: str\n    author: str\n\n    def __str__(self) -&gt; str:\n        return f\"{self.title} by {self.author}\"\n\n\nclass EmbeddableDocument(BaseDocument, Embedable):\n    \"\"\"Document that supports vector embeddings.\"\"\"\n\n    content: str\n\n    def create_content(self) -&gt; str:\n        \"\"\"Create content for embedding from document metadata and content.\"\"\"\n        return f\"{self.title}\\n{self.author}\\n{self.content}\"\n</code></pre>"},{"location":"tutorials/using_protocols/#step-2-create-specific-document-types","title":"Step 2: Create Specific Document Types","text":"<p>Now, let's create specific document types that inherit from our base classes:</p> <pre><code>class TextDocument(EmbeddableDocument):\n    \"\"\"A simple text document.\"\"\"\n\n    format: str = \"text\"\n\n\nclass PDFDocument(EmbeddableDocument):\n    \"\"\"A PDF document with additional metadata.\"\"\"\n\n    format: str = \"pdf\"\n    page_count: int\n\n\nclass ImageDocument(BaseDocument):\n    \"\"\"An image document that doesn't need text embedding.\"\"\"\n\n    format: str = \"image\"\n    width: int\n    height: int\n    file_path: str\n</code></pre>"},{"location":"tutorials/using_protocols/#step-3-create-a-document-repository","title":"Step 3: Create a Document Repository","text":"<p>Let's create a simple repository to manage our documents:</p> <pre><code>from typing import Dict, List, Optional, Type, TypeVar\n\nT = TypeVar('T', bound=BaseDocument)\n\nclass DocumentRepository:\n    \"\"\"Repository for managing documents.\"\"\"\n\n    def __init__(self):\n        self.documents: Dict[UUID, BaseDocument] = {}\n\n    def add(self, document: BaseDocument) -&gt; None:\n        \"\"\"Add a document to the repository.\"\"\"\n        self.documents[document.id] = document\n\n    def get(self, document_id: UUID) -&gt; Optional[BaseDocument]:\n        \"\"\"Get a document by ID.\"\"\"\n        return self.documents.get(document_id)\n\n    def list_all(self) -&gt; List[BaseDocument]:\n        \"\"\"List all documents.\"\"\"\n        return list(self.documents.values())\n\n    def find_by_type(self, doc_type: Type[T]) -&gt; List[T]:\n        \"\"\"Find documents by type.\"\"\"\n        return [doc for doc in self.documents.values() if isinstance(doc, doc_type)]\n\n    def find_by_author(self, author: str) -&gt; List[BaseDocument]:\n        \"\"\"Find documents by author.\"\"\"\n        return [doc for doc in self.documents.values() if doc.author == author]\n\n    def update(self, document: BaseDocument) -&gt; None:\n        \"\"\"Update a document.\"\"\"\n        if document.id in self.documents:\n            # Update the timestamp\n            document.update_timestamp()\n            self.documents[document.id] = document\n</code></pre>"},{"location":"tutorials/using_protocols/#step-4-working-with-documents","title":"Step 4: Working with Documents","text":"<p>Now let's use our document models and repository:</p> <pre><code># Create a repository\nrepo = DocumentRepository()\n\n# Create some documents\ntext_doc = TextDocument(\n    title=\"Getting Started with Protocols\",\n    author=\"Jane Smith\",\n    content=\"This document explains how to use protocols effectively.\"\n)\n\npdf_doc = PDFDocument(\n    title=\"Advanced Protocol Patterns\",\n    author=\"John Doe\",\n    content=\"Detailed explanation of advanced protocol usage patterns.\",\n    page_count=42\n)\n\nimage_doc = ImageDocument(\n    title=\"Protocol Architecture Diagram\",\n    author=\"Jane Smith\",\n    width=1920,\n    height=1080,\n    file_path=\"/images/protocol_diagram.png\"\n)\n\n# Add documents to the repository\nrepo.add(text_doc)\nrepo.add(pdf_doc)\nrepo.add(image_doc)\n\n# List all documents\nprint(\"All documents:\")\nfor doc in repo.list_all():\n    print(f\"- {doc}\")\n\n# Find documents by author\nprint(\"\\nDocuments by Jane Smith:\")\nfor doc in repo.find_by_author(\"Jane Smith\"):\n    print(f\"- {doc}\")\n\n# Find documents by type\nprint(\"\\nText documents:\")\nfor doc in repo.find_by_type(TextDocument):\n    print(f\"- {doc}\")\n\n# Update a document\ntext_doc.title = \"Updated: Getting Started with Protocols\"\nrepo.update(text_doc)\nprint(f\"\\nUpdated document timestamp: {text_doc.updated_at}\")\n</code></pre>"},{"location":"tutorials/using_protocols/#step-5-working-with-embeddings","title":"Step 5: Working with Embeddings","text":"<p>Let's extend our example to work with embeddings:</p> <pre><code>import numpy as np\nfrom typing import List, Tuple\n\ndef generate_mock_embedding(text: str) -&gt; List[float]:\n    \"\"\"Generate a mock embedding for demonstration purposes.\"\"\"\n    # In a real application, you would use a proper embedding model\n    # This is just a simple hash-based approach for demonstration\n    np_array = np.array([ord(c) for c in text], dtype=np.float32)\n    return (np_array / np.linalg.norm(np_array)).tolist()[:10]  # Normalize and take first 10 dims\n\ndef cosine_similarity(a: List[float], b: List[float]) -&gt; float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    dot_product = sum(x * y for x, y in zip(a, b))\n    norm_a = sum(x * x for x in a) ** 0.5\n    norm_b = sum(y * y for y in b) ** 0.5\n    return dot_product / (norm_a * norm_b)\n\n# Generate embeddings for our documents\nfor doc in repo.find_by_type(EmbeddableDocument):\n    content = doc.create_content()\n    doc.embedding = generate_mock_embedding(content)\n    print(f\"Generated embedding for '{doc.title}' with {len(doc.embedding)} dimensions\")\n\n# Find similar documents\ndef find_similar_documents(\n    query_doc: EmbeddableDocument,\n    candidates: List[EmbeddableDocument],\n    threshold: float = 0.7\n) -&gt; List[Tuple[EmbeddableDocument, float]]:\n    \"\"\"Find documents similar to the query document.\"\"\"\n    results = []\n    for doc in candidates:\n        if doc.id != query_doc.id:  # Skip the query document itself\n            similarity = cosine_similarity(query_doc.embedding, doc.embedding)\n            if similarity &gt;= threshold:\n                results.append((doc, similarity))\n    return sorted(results, key=lambda x: x[1], reverse=True)\n\n# Find documents similar to our text document\nembedable_docs = repo.find_by_type(EmbeddableDocument)\nsimilar_docs = find_similar_documents(text_doc, embedable_docs)\n\nprint(\"\\nDocuments similar to 'Getting Started with Protocols':\")\nfor doc, similarity in similar_docs:\n    print(f\"- {doc.title} (similarity: {similarity:.2f})\")\n</code></pre>"},{"location":"tutorials/using_protocols/#step-6-adding-event-tracking","title":"Step 6: Adding Event Tracking","text":"<p>Let's extend our system to track document events using the <code>Invokable</code> protocol:</p> <pre><code>import asyncio\nfrom pydapter.protocols import Invokable, Event\nfrom datetime import datetime\n\nclass DocumentEvent(Event):\n    \"\"\"Event for tracking document operations.\"\"\"\n\n    event_type: str\n    document_id: UUID\n    user_id: str\n\n    async def process(self):\n        \"\"\"Process the event.\"\"\"\n        # In a real application, this might log to a database or message queue\n        print(f\"Processing event: {self.event_type} for document {self.document_id}\")\n        return {\"processed\": True, \"timestamp\": datetime.now().isoformat()}\n\nasync def track_document_event(\n    event_type: str,\n    document: BaseDocument,\n    user_id: str\n) -&gt; DocumentEvent:\n    \"\"\"Track a document event.\"\"\"\n    event = DocumentEvent(\n        event_type=event_type,\n        document_id=document.id,\n        user_id=user_id,\n        content=f\"{event_type} operation on {document.title} by user {user_id}\"\n    )\n    event._invoke_function = event.process\n    await event.invoke()\n    return event\n\n# Example usage in an async context\nasync def main():\n    # Track a view event\n    view_event = await track_document_event(\"view\", text_doc, \"user123\")\n    print(f\"Event status: {view_event.execution.status}\")\n    print(f\"Event duration: {view_event.execution.duration:.6f} seconds\")\n    print(f\"Event response: {view_event.execution.response}\")\n\n    # Track an edit event\n    edit_event = await track_document_event(\"edit\", text_doc, \"user123\")\n    print(f\"Event status: {edit_event.execution.status}\")\n\n# Run the async example\nasyncio.run(main())\n</code></pre>"},{"location":"tutorials/using_protocols/#complete-example","title":"Complete Example","text":"<p>Here's the complete example combining all the steps:</p> <pre><code>import asyncio\nimport numpy as np\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Type, TypeVar\nfrom uuid import UUID\n\nfrom pydapter.protocols import Identifiable, Temporal, Embedable, Invokable, Event\n\n# Step 1: Define Base Document Models\nclass BaseDocument(Identifiable, Temporal):\n    \"\"\"Base document class with ID and timestamp tracking.\"\"\"\n\n    title: str\n    author: str\n\n    def __str__(self) -&gt; str:\n        return f\"{self.title} by {self.author}\"\n\n\nclass EmbeddableDocument(BaseDocument, Embedable):\n    \"\"\"Document that supports vector embeddings.\"\"\"\n\n    content: str\n\n    def create_content(self) -&gt; str:\n        \"\"\"Create content for embedding from document metadata and content.\"\"\"\n        return f\"{self.title}\\n{self.author}\\n{self.content}\"\n\n\n# Step 2: Create Specific Document Types\nclass TextDocument(EmbeddableDocument):\n    \"\"\"A simple text document.\"\"\"\n\n    format: str = \"text\"\n\n\nclass PDFDocument(EmbeddableDocument):\n    \"\"\"A PDF document with additional metadata.\"\"\"\n\n    format: str = \"pdf\"\n    page_count: int\n\n\nclass ImageDocument(BaseDocument):\n    \"\"\"An image document that doesn't need text embedding.\"\"\"\n\n    format: str = \"image\"\n    width: int\n    height: int\n    file_path: str\n\n\n# Step 3: Create a Document Repository\nT = TypeVar('T', bound=BaseDocument)\n\nclass DocumentRepository:\n    \"\"\"Repository for managing documents.\"\"\"\n\n    def __init__(self):\n        self.documents: Dict[UUID, BaseDocument] = {}\n\n    def add(self, document: BaseDocument) -&gt; None:\n        \"\"\"Add a document to the repository.\"\"\"\n        self.documents[document.id] = document\n\n    def get(self, document_id: UUID) -&gt; Optional[BaseDocument]:\n        \"\"\"Get a document by ID.\"\"\"\n        return self.documents.get(document_id)\n\n    def list_all(self) -&gt; List[BaseDocument]:\n        \"\"\"List all documents.\"\"\"\n        return list(self.documents.values())\n\n    def find_by_type(self, doc_type: Type[T]) -&gt; List[T]:\n        \"\"\"Find documents by type.\"\"\"\n        return [doc for doc in self.documents.values() if isinstance(doc, doc_type)]\n\n    def find_by_author(self, author: str) -&gt; List[BaseDocument]:\n        \"\"\"Find documents by author.\"\"\"\n        return [doc for doc in self.documents.values() if doc.author == author]\n\n    def update(self, document: BaseDocument) -&gt; None:\n        \"\"\"Update a document.\"\"\"\n        if document.id in self.documents:\n            # Update the timestamp\n            document.update_timestamp()\n            self.documents[document.id] = document\n\n\n# Step 6: Define Document Event\nclass DocumentEvent(Event):\n    \"\"\"Event for tracking document operations.\"\"\"\n\n    event_type: str\n    document_id: UUID\n    user_id: str\n\n    async def process(self):\n        \"\"\"Process the event.\"\"\"\n        # In a real application, this might log to a database or message queue\n        print(f\"Processing event: {self.event_type} for document {self.document_id}\")\n        return {\"processed\": True, \"timestamp\": datetime.now().isoformat()}\n\n\n# Helper functions\ndef generate_mock_embedding(text: str) -&gt; List[float]:\n    \"\"\"Generate a mock embedding for demonstration purposes.\"\"\"\n    # In a real application, you would use a proper embedding model\n    # This is just a simple hash-based approach for demonstration\n    np_array = np.array([ord(c) for c in text], dtype=np.float32)\n    return (np_array / np.linalg.norm(np_array)).tolist()[:10]  # Normalize and take first 10 dims\n\n\ndef cosine_similarity(a: List[float], b: List[float]) -&gt; float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    dot_product = sum(x * y for x, y in zip(a, b))\n    norm_a = sum(x * x for x in a) ** 0.5\n    norm_b = sum(y * y for y in b) ** 0.5\n    return dot_product / (norm_a * norm_b)\n\n\ndef find_similar_documents(\n    query_doc: EmbeddableDocument,\n    candidates: List[EmbeddableDocument],\n    threshold: float = 0.7\n) -&gt; List[Tuple[EmbeddableDocument, float]]:\n    \"\"\"Find documents similar to the query document.\"\"\"\n    results = []\n    for doc in candidates:\n        if doc.id != query_doc.id:  # Skip the query document itself\n            similarity = cosine_similarity(query_doc.embedding, doc.embedding)\n            if similarity &gt;= threshold:\n                results.append((doc, similarity))\n    return sorted(results, key=lambda x: x[1], reverse=True)\n\n\nasync def track_document_event(\n    event_type: str,\n    document: BaseDocument,\n    user_id: str\n) -&gt; DocumentEvent:\n    \"\"\"Track a document event.\"\"\"\n    event = DocumentEvent(\n        event_type=event_type,\n        document_id=document.id,\n        user_id=user_id,\n        content=f\"{event_type} operation on {document.title} by user {user_id}\"\n    )\n    event._invoke_function = event.process\n    await event.invoke()\n    return event\n\n\n# Main function to demonstrate usage\nasync def main():\n    # Create a repository\n    repo = DocumentRepository()\n\n    # Create some documents\n    text_doc = TextDocument(\n        title=\"Getting Started with Protocols\",\n        author=\"Jane Smith\",\n        content=\"This document explains how to use protocols effectively.\"\n    )\n\n    pdf_doc = PDFDocument(\n        title=\"Advanced Protocol Patterns\",\n        author=\"John Doe\",\n        content=\"Detailed explanation of advanced protocol usage patterns.\",\n        page_count=42\n    )\n\n    image_doc = ImageDocument(\n        title=\"Protocol Architecture Diagram\",\n        author=\"Jane Smith\",\n        width=1920,\n        height=1080,\n        file_path=\"/images/protocol_diagram.png\"\n    )\n\n    # Add documents to the repository\n    repo.add(text_doc)\n    repo.add(pdf_doc)\n    repo.add(image_doc)\n\n    # List all documents\n    print(\"All documents:\")\n    for doc in repo.list_all():\n        print(f\"- {doc}\")\n\n    # Find documents by author\n    print(\"\\nDocuments by Jane Smith:\")\n    for doc in repo.find_by_author(\"Jane Smith\"):\n        print(f\"- {doc}\")\n\n    # Find documents by type\n    print(\"\\nText documents:\")\n    for doc in repo.find_by_type(TextDocument):\n        print(f\"- {doc}\")\n\n    # Update a document\n    text_doc.title = \"Updated: Getting Started with Protocols\"\n    repo.update(text_doc)\n    print(f\"\\nUpdated document timestamp: {text_doc.updated_at}\")\n\n    # Generate embeddings for our documents\n    for doc in repo.find_by_type(EmbeddableDocument):\n        content = doc.create_content()\n        doc.embedding = generate_mock_embedding(content)\n        print(f\"Generated embedding for '{doc.title}' with {len(doc.embedding)} dimensions\")\n\n    # Find similar documents\n    embedable_docs = repo.find_by_type(EmbeddableDocument)\n    similar_docs = find_similar_documents(text_doc, embedable_docs)\n\n    print(\"\\nDocuments similar to 'Updated: Getting Started with Protocols':\")\n    for doc, similarity in similar_docs:\n        print(f\"- {doc.title} (similarity: {similarity:.2f})\")\n\n    # Track document events\n    view_event = await track_document_event(\"view\", text_doc, \"user123\")\n    print(f\"\\nEvent status: {view_event.execution.status}\")\n    print(f\"Event duration: {view_event.execution.duration:.6f} seconds\")\n    print(f\"Event response: {view_event.execution.response}\")\n\n    edit_event = await track_document_event(\"edit\", text_doc, \"user123\")\n    print(f\"Event status: {edit_event.execution.status}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"tutorials/using_protocols/#summary","title":"Summary","text":"<p>In this tutorial, we've demonstrated how to use pydapter's protocols to create standardized models with consistent behavior. We've covered:</p> <ol> <li>Creating base document models with <code>Identifiable</code> and <code>Temporal</code> protocols</li> <li>Adding embedding support with the <code>Embedable</code> protocol</li> <li>Building a document repository to manage our models</li> <li>Working with document embeddings for similarity search</li> <li>Tracking document events with the <code>Invokable</code> and <code>Event</code> protocols</li> </ol> <p>The protocols module provides a powerful way to add standardized capabilities to your models, making your code more consistent and easier to maintain.</p>"}]}
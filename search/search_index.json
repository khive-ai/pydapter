{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pydapter","text":"<p>pydapter is a tiny trait + adapter toolkit for pydantic models.</p>"},{"location":"#overview","title":"Overview","text":"<p>pydapter provides a lightweight, flexible way to adapt Pydantic models to various data sources and sinks. It enables seamless data transfer between different formats and storage systems while maintaining the type safety and validation that Pydantic provides.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Unified Interface: Consistent API for working with different data sources</li> <li>Type Safety: Leverages Pydantic's validation system</li> <li>Extensible: Easy to add new adapters for different data sources</li> <li>Async Support: Both synchronous and asynchronous interfaces</li> <li>Minimal Dependencies: Core functionality has minimal requirements</li> <li>Protocols: Optional standardized interfaces for models</li> <li>Migrations: Optional database schema migration tools</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pydapter\n</code></pre> <p>With optional dependencies:</p> <pre><code># Database adapters\npip install \"pydapter[postgres]\"\npip install \"pydapter[mongo]\"\npip install \"pydapter[neo4j]\"\n\n# File formats\npip install \"pydapter[excel]\"\n\n# New modules\npip install \"pydapter[protocols]\"      # Standardized model interfaces\npip install \"pydapter[migrations-sql]\" # Database schema migrations with SQLAlchemy/Alembic\n\n# Combined packages\npip install \"pydapter[migrations]\"     # All migration components\npip install \"pydapter[migrations-all]\" # Migrations with protocols support\n\n# For all extras\npip install \"pydapter[all]\"\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from pydantic import BaseModel\nfrom typing import List\nfrom pydapter.adapters.json_ import JsonAdapter\n\n# Define your model\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\n# Create an adapter\nadapter = JsonAdapter[User](path=\"users.json\")\n\n# Read data\nusers: List[User] = adapter.read_all()\n\n# Write data\nnew_user = User(id=3, name=\"Alice\", email=\"alice@example.com\")\nadapter.write_one(new_user)\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Check out the Getting Started guide</li> <li>Learn about Error Handling</li> <li>Explore specific adapters like PostgreSQL,   Neo4j, or Qdrant</li> <li>Use Protocols to add standardized capabilities to your models</li> <li>Manage database schema changes with Migrations</li> <li>Follow our tutorials:</li> <li>Using Protocols</li> <li>Using Migrations</li> <li>If you're transitioning from the dev/ directory, see the   Migration Guide</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#020-2025-05-24","title":"0.2.0 - 2025-05-24","text":""},{"location":"changelog/#highlights","title":"Highlights","text":"<p>This release introduces two major foundational modules: Fields and Protocols. These modules provide a robust and extensible framework for defining data structures and behaviors within <code>pydapter</code>.</p> <ul> <li>Fields Module (<code>pydapter.fields</code>): A powerful system for defining typed,   validated, and serializable fields. It includes pre-defined field types for   common use cases like IDs, datetimes, embeddings, and execution tracking,   along with a flexible <code>Field</code> class for custom definitions and a   <code>create_model</code> utility for dynamic Pydantic model creation.</li> <li>Protocols Module (<code>pydapter.protocols</code>): A set of composable interfaces   (e.g., <code>Identifiable</code>, <code>Temporal</code>, <code>Embeddable</code>, <code>Invokable</code>,   <code>Cryptographical</code>) that define standard behaviors for Pydantic models. The   <code>Event</code> protocol combines these to offer comprehensive event tracking   capabilities, enhanced by the <code>@as_event</code> decorator for easily instrumenting   functions.</li> </ul> <p>These additions significantly enhance <code>pydapter</code>'s ability to model complex data interactions and workflows in a standardized and maintainable way.</p>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li>New <code>pydapter.fields</code> module: Introduced a robust system for defining   typed, validated, and serializable fields (e.g., IDs, datetimes, embeddings,   execution tracking) and a <code>create_model</code> utility for dynamic Pydantic model   creation. (Related to Issue #100, PR #99)</li> <li>New <code>pydapter.protocols</code> module: Added composable protocol interfaces   (<code>Identifiable</code>, <code>Temporal</code>, <code>Embeddable</code>, <code>Invokable</code>, <code>Cryptographical</code>) and   an <code>Event</code> protocol with an <code>@as_event</code> decorator for comprehensive event   modeling and function instrumentation. (Related to Issue #100, PR #99)</li> <li>Hybrid Documentation System: Implemented a new documentation system   combining auto-generated API skeletons with rich manual content and automated   validation (markdown linting, link checking). (Issue #103, PR #104)</li> <li>Updated CI to install documentation validation tools. (Issue #105)</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Resolved Python 3.10 compatibility issues related to <code>datetime.timezone.utc</code>.   (Part of PR #99 fixes)</li> <li>Addressed various <code>mkdocs</code> build warnings and broken links in the   documentation. (Part of PR #104 fixes)</li> </ul>"},{"location":"changelog/#015-2025-05-14","title":"0.1.5 - 2025-05-14","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>New adapter implementations:</li> <li><code>AsyncNeo4jAdapter</code> - Asynchronous adapter for Neo4j graph database with     comprehensive error handling</li> <li><code>WeaviateAdapter</code> - Synchronous adapter for Weaviate vector database with     vector search capabilities</li> <li><code>AsyncWeaviateAdapter</code> - Asynchronous adapter for Weaviate vector database     using aiohttp for REST API calls</li> </ul>"},{"location":"changelog/#011-2025-05-04","title":"0.1.1 - 2025-05-04","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Integration tests for database adapters using TestContainers</li> <li>PostgreSQL integration tests</li> <li>MongoDB integration tests</li> <li>Neo4j integration tests</li> <li>Qdrant vector database integration tests</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Neo4j adapter now supports authentication</li> <li>Qdrant adapter improved connection error handling</li> <li>SQL adapter enhanced error handling for connection issues</li> <li>Improved error handling in core adapter classes</li> </ul>"},{"location":"changelog/#010-2025-05-03","title":"0.1.0 - 2025-05-03","text":"<ul> <li>Initial public release.</li> <li><code>core.Adapter</code>, <code>AdapterRegistry</code>, <code>Adaptable</code></li> <li>Built-in JSON adapter</li> </ul>"},{"location":"ci/","title":"Continuous Integration","text":"<p>This document describes the continuous integration (CI) setup for the pydapter project and how to use it locally.</p>"},{"location":"ci/#overview","title":"Overview","text":"<p>The pydapter project uses a comprehensive CI system that runs:</p> <ul> <li>Linting checks (using ruff)</li> <li>Code formatting checks (using ruff format)</li> <li>Type checking (using mypy)</li> <li>Unit tests (using pytest)</li> <li>Integration tests (using pytest)</li> <li>Coverage reporting</li> </ul> <p>The CI system is implemented as a Python script that can be run both locally and in GitHub Actions workflows, ensuring consistency between local development and CI environments.</p>"},{"location":"ci/#prerequisites","title":"Prerequisites","text":"<p>Before running the CI script, you need to ensure all dependencies are installed. The easiest way to do this is to use the <code>uv sync</code> command with the <code>--extra all</code> option:</p> <pre><code># Install all dependencies including those needed for testing\nuv sync --extra all\n</code></pre> <p>This will install all the dependencies defined in the <code>pyproject.toml</code> file, including those needed for testing and integration with external services.</p>"},{"location":"ci/#running-ci-locally","title":"Running CI Locally","text":"<p>The CI script is located at <code>scripts/ci.py</code> and can be run with various options to customize the checks performed.</p>"},{"location":"ci/#basic-usage","title":"Basic Usage","text":"<p>To run all CI checks:</p> <pre><code>python scripts/ci.py\n</code></pre> <p>This will run all linting, type checking, unit tests, integration tests, and coverage reporting.</p>"},{"location":"ci/#running-specific-components","title":"Running Specific Components","text":"<p>You can run only specific components using the <code>--only</code> option:</p> <pre><code># Run only linting checks\npython scripts/ci.py --only lint\n\n# Run only type checking\npython scripts/ci.py --only type\n\n# Run only unit tests\npython scripts/ci.py --only unit\n\n# Run only integration tests\npython scripts/ci.py --only integration\n\n# Run only coverage report\npython scripts/ci.py --only coverage\n</code></pre>"},{"location":"ci/#skipping-specific-checks","title":"Skipping Specific Checks","text":"<p>You can skip specific checks using the following options:</p> <pre><code># Skip linting checks\npython scripts/ci.py --skip-lint\n\n# Skip type checking\npython scripts/ci.py --skip-type-check\n\n# Skip unit tests\npython scripts/ci.py --skip-unit\n\n# Skip integration tests\npython scripts/ci.py --skip-integration\n\n# Skip coverage reporting\npython scripts/ci.py --skip-coverage\n</code></pre> <p>You can combine these options to run only the checks you're interested in:</p> <pre><code># Run only unit tests\npython scripts/ci.py --skip-lint --skip-type-check --skip-integration --skip-coverage\n</code></pre>"},{"location":"ci/#handling-external-dependencies","title":"Handling External Dependencies","text":"<p>Some tests require external dependencies like databases (MongoDB, Neo4j, Qdrant, etc.). You can skip these tests using the <code>--skip-external-deps</code> option:</p> <pre><code>python scripts/ci.py --skip-external-deps\n</code></pre> <p>This is useful when you want to run the tests locally without setting up all the external dependencies. The script will automatically exclude tests that require external services.</p>"},{"location":"ci/#parallel-test-execution","title":"Parallel Test Execution","text":"<p>To speed up test execution, you can run tests in parallel:</p> <pre><code>python scripts/ci.py --parallel 4\n</code></pre> <p>This will use 4 processes to run the tests.</p>"},{"location":"ci/#python-version","title":"Python Version","text":"<p>You can specify a Python version to use:</p> <pre><code>python scripts/ci.py --python-version 3.10\n</code></pre> <p>Or specify a path to a Python executable:</p> <pre><code>python scripts/ci.py --python-path /path/to/python\n</code></pre>"},{"location":"ci/#dry-run","title":"Dry Run","text":"<p>To see what commands would be executed without actually running them:</p> <pre><code>python scripts/ci.py --dry-run\n</code></pre>"},{"location":"ci/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<p>The CI script is also used in GitHub Actions workflows, defined in <code>.github/workflows/ci.yml</code>. The workflow runs on push to main, pull requests to main, and can be triggered manually.</p> <p>The workflow includes the following jobs:</p> <ul> <li>test: Runs the full CI script on multiple Python versions (3.10, 3.11,   3.12)</li> <li>lint: Runs only linting and formatting checks</li> <li>type-check: Runs only type checking</li> <li>integration: Runs only integration tests</li> <li>coverage: Runs tests with coverage reporting and uploads to Codecov</li> </ul>"},{"location":"ci/#adding-new-checks","title":"Adding New Checks","text":"<p>To add new checks to the CI script:</p> <ol> <li>Add a new method to the <code>CIRunner</code> class in <code>scripts/ci.py</code></li> <li>Call the method from the <code>run_all</code> method</li> <li>Add the result to the <code>results</code> list</li> <li>Add any necessary command-line options to the <code>parse_args</code> function</li> <li>Update the <code>REQUIRED_DEPS</code> dictionary with any new dependencies</li> <li>If the check requires external dependencies, update the <code>EXTERNAL_DEPS_FILES</code>    list</li> </ol>"},{"location":"ci/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with the CI script:</p> <ul> <li>Make sure you have all the required dependencies installed:   <code>uv sync --extra all</code></li> <li>For integration tests, you may need to have Docker running if you're using   testcontainers</li> <li>Try running specific checks individually to isolate the issue</li> <li>Use the <code>--skip-external-deps</code> option to skip tests that require external   dependencies</li> <li>Check the output for missing dependencies and install them as needed</li> </ul>"},{"location":"ci/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Missing Dependencies: The script will attempt to install missing    dependencies automatically, but if it fails, you'll need to install them    manually.</p> </li> <li> <p>External Dependencies: Tests that require external services like    databases will fail if those services are not available. Use    <code>--skip-external-deps</code> to skip these tests.</p> </li> <li> <p>Type Checking Errors: The type checking step may fail due to missing type    stubs for dependencies. You can install these with    <code>uv pip install types-&lt;package&gt;</code>.</p> </li> <li> <p>Docker Not Running: If you're running integration tests that use    testcontainers, make sure Docker is running on your system.</p> </li> </ol>"},{"location":"contributing/","title":"Contributing to Pydapter","text":"<p>Thank you for your interest in contributing to Pydapter! This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"contributing/#development-environment-setup","title":"Development Environment Setup","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:</li> </ol> <pre><code>git clone https://github.com/your-username/pydapter.git\ncd pydapter\n</code></pre> <ol> <li>Set up a development environment:</li> </ol> <pre><code># Using uv (recommended)\nuv pip install -e \".[dev,all]\"\n\n# Or using pip\npip install -e \".[dev,all]\"\n</code></pre> <ol> <li>Install pre-commit hooks:</li> </ol> <pre><code>pre-commit install\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li>Create a new branch for your feature or bugfix:</li> </ol> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <ol> <li> <p>Make your changes, following the project's coding standards</p> </li> <li> <p>Run the CI script locally to ensure all tests pass:</p> </li> </ol> <pre><code>python scripts/ci.py\n</code></pre> <ol> <li>Commit your changes using conventional commit messages:</li> </ol> <pre><code>git commit -m \"feat: add new feature\"\n</code></pre> <ol> <li>Push your branch to your fork:</li> </ol> <pre><code>git push origin feature/your-feature-name\n</code></pre> <ol> <li>Open a pull request on GitHub</li> </ol>"},{"location":"contributing/#continuous-integration","title":"Continuous Integration","text":"<p>The project uses a comprehensive CI system that runs:</p> <ul> <li>Linting checks (using ruff)</li> <li>Code formatting checks (using ruff format)</li> <li>Type checking (using mypy)</li> <li>Unit tests (using pytest)</li> <li>Integration tests (using pytest)</li> <li>Coverage reporting</li> <li>Documentation validation (using markdownlint and markdown-link-check)</li> </ul> <p>You can run the CI script locally with various options:</p> <pre><code># Run all checks\npython scripts/ci.py\n\n# Skip integration tests (which require Docker)\npython scripts/ci.py --skip-integration\n\n# Run only documentation validation\npython scripts/ci.py --only docs\n\n# Run only linting and formatting checks\npython scripts/ci.py --skip-unit --skip-integration --skip-coverage --skip-docs\n\n# Run tests in parallel\npython scripts/ci.py --parallel 4\n</code></pre> <p>For more information, see the CI documentation.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>This project follows these coding standards:</p> <ul> <li>Code formatting with ruff format</li> <li>Linting with ruff</li> <li>Type annotations for all functions and classes</li> <li>Comprehensive docstrings in   Google style</li> <li>Test coverage for all new features</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<p>All new features and bug fixes should include tests. The project uses pytest for testing:</p> <pre><code># Run all tests\nuv run pytest\n\n# Run specific tests\nuv run pytest tests/test_specific_file.py\n\n# Run with coverage\nuv run pytest --cov=pydapter\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Documentation is written in Markdown and built with MkDocs using a hybrid approach that combines auto-generated API references with enhanced manual content.</p>"},{"location":"contributing/#documentation-standards","title":"Documentation Standards","text":"<p>All documentation must follow these standards:</p> <ol> <li>Markdown Quality: All markdown files must pass <code>markdownlint</code> validation</li> <li>Link Integrity: All internal and external links must be valid</li> <li>API Documentation: Use the hybrid approach with enhanced manual content</li> <li>Code Examples: Include working code examples with proper syntax    highlighting</li> <li>Cross-References: Link related concepts and maintain navigation    consistency</li> </ol>"},{"location":"contributing/#validation-tools","title":"Validation Tools","text":"<p>The project uses automated validation tools:</p> <ul> <li>markdownlint: Ensures consistent markdown formatting</li> <li>markdown-link-check: Validates all links in documentation</li> <li>Pre-commit hooks: Automatic validation before commits</li> </ul>"},{"location":"contributing/#writing-documentation","title":"Writing Documentation","text":"<p>When contributing documentation:</p> <ol> <li>API Reference: Follow the pattern established in <code>docs/api/protocols.md</code>    and <code>docs/api/core.md</code></li> <li>Manual Enhancement: Add examples, best practices, and cross-references    beyond basic API extraction</li> <li>User Personas: Consider different user needs (new users, API users,    contributors)</li> <li>Code Examples: Provide complete, runnable examples</li> <li>Navigation: Ensure proper cross-linking between related sections</li> </ol>"},{"location":"contributing/#documentation-workflow","title":"Documentation Workflow","text":"<pre><code># Preview documentation locally\nuv run mkdocs serve\n\n# Validate documentation\npython scripts/ci.py --only docs\n\n# Check specific files\nmarkdownlint docs/**/*.md\nmarkdown-link-check docs/api/core.md --config .markdownlinkcheck.json\n\n# Fix common issues automatically (when possible)\nmarkdownlint --fix docs/**/*.md\n</code></pre>"},{"location":"contributing/#documentation-structure","title":"Documentation Structure","text":"<ul> <li><code>docs/api/</code>: API reference documentation (hybrid approach)</li> <li><code>docs/tutorials/</code>: Step-by-step guides</li> <li><code>docs/</code>: General guides and concepts</li> <li>Examples should be complete and testable</li> <li>Cross-references should use relative links</li> </ul> <p>Then open http://127.0.0.1:8000/ in your browser to preview changes.</p>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure your code passes all CI checks</li> <li>Update documentation if necessary</li> <li>Add tests for new features</li> <li>Make sure your PR description clearly describes the changes and their purpose</li> <li>Wait for review and address any feedback</li> </ol>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to Pydapter, you agree that your contributions will be licensed under the project's MIT License.</p>"},{"location":"error_handling/","title":"Error Handling in pydapter","text":"<p>pydapter provides a comprehensive error handling system to help you diagnose and resolve issues when working with adapters. This document explains the exception hierarchy and how to handle errors effectively in your applications.</p>"},{"location":"error_handling/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>All pydapter exceptions inherit from the base <code>AdapterError</code> class, which provides context-rich error messages and a consistent interface for error handling.</p> <pre><code>AdapterError\n\u251c\u2500\u2500 ValidationError\n\u251c\u2500\u2500 ParseError\n\u251c\u2500\u2500 ConnectionError\n\u251c\u2500\u2500 QueryError\n\u251c\u2500\u2500 ResourceError\n\u251c\u2500\u2500 ConfigurationError\n\u2514\u2500\u2500 AdapterNotFoundError\n</code></pre>"},{"location":"error_handling/#adaptererror","title":"AdapterError","text":"<p>The base exception class for all pydapter errors. It provides a mechanism to attach context information to errors.</p> <pre><code>try:\n    # Some adapter operation\nexcept AdapterError as e:\n    print(f\"Error message: {e.message}\")\n    print(f\"Error context: {e.context}\")\n</code></pre>"},{"location":"error_handling/#validationerror","title":"ValidationError","text":"<p>Raised when data validation fails, such as when required fields are missing or have incorrect types.</p> <pre><code>try:\n    model = MyModel.adapt_from(invalid_data, obj_key=\"json\")\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\n    print(f\"Invalid data: {e.data}\")\n</code></pre>"},{"location":"error_handling/#parseerror","title":"ParseError","text":"<p>Raised when data parsing fails, such as when trying to parse invalid JSON, CSV, or TOML.</p> <pre><code>try:\n    model = MyModel.adapt_from('{\"invalid\": json', obj_key=\"json\")\nexcept ParseError as e:\n    print(f\"Parse error: {e}\")\n    print(f\"Source: {e.source}\")\n</code></pre>"},{"location":"error_handling/#connectionerror","title":"ConnectionError","text":"<p>Raised when a connection to a data source fails, such as when a database is unavailable.</p> <pre><code>try:\n    model = MyModel.adapt_from({\"engine_url\": \"invalid://url\", \"table\": \"test\"}, obj_key=\"sql\")\nexcept ConnectionError as e:\n    print(f\"Connection failed: {e}\")\n    print(f\"Adapter: {e.adapter}\")\n    print(f\"URL: {e.url}\")\n</code></pre>"},{"location":"error_handling/#queryerror","title":"QueryError","text":"<p>Raised when a query to a data source fails, such as when an SQL query contains errors.</p> <pre><code>try:\n    model = MyModel.adapt_from({\"engine_url\": \"sqlite://\", \"table\": \"test\", \"query\": \"INVALID SQL\"}, obj_key=\"sql\")\nexcept QueryError as e:\n    print(f\"Query failed: {e}\")\n    print(f\"Query: {e.query}\")\n    print(f\"Adapter: {e.adapter}\")\n</code></pre>"},{"location":"error_handling/#resourceerror","title":"ResourceError","text":"<p>Raised when a resource (file, database table, etc.) cannot be accessed.</p> <pre><code>try:\n    model = MyModel.adapt_from(Path(\"nonexistent.json\"), obj_key=\"json\")\nexcept ResourceError as e:\n    print(f\"Resource error: {e}\")\n    print(f\"Resource: {e.resource}\")\n</code></pre>"},{"location":"error_handling/#configurationerror","title":"ConfigurationError","text":"<p>Raised when adapter configuration is invalid, such as when required parameters are missing.</p> <pre><code>try:\n    model = MyModel.adapt_from({\"missing\": \"required_params\"}, obj_key=\"sql\")\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\n    print(f\"Config: {e.config}\")\n</code></pre>"},{"location":"error_handling/#adapternotfounderror","title":"AdapterNotFoundError","text":"<p>Raised when an adapter is not found for a given <code>obj_key</code>.</p> <pre><code>try:\n    model = MyModel.adapt_from({}, obj_key=\"nonexistent\")\nexcept AdapterNotFoundError as e:\n    print(f\"Adapter not found: {e}\")\n    print(f\"Object key: {e.obj_key}\")\n</code></pre>"},{"location":"error_handling/#error-handling-best-practices","title":"Error Handling Best Practices","text":""},{"location":"error_handling/#catch-specific-exceptions","title":"Catch Specific Exceptions","text":"<p>Catch the most specific exception type that applies to your situation:</p> <pre><code>try:\n    model = MyModel.adapt_from(data, obj_key=\"json\")\nexcept ParseError:\n    # Handle parsing errors\nexcept ValidationError:\n    # Handle validation errors\nexcept AdapterError:\n    # Handle any other adapter errors\n</code></pre>"},{"location":"error_handling/#provide-context-in-error-messages","title":"Provide Context in Error Messages","text":"<p>When raising custom exceptions, provide as much context as possible:</p> <pre><code>raise ConnectionError(\n    \"Failed to connect to database\",\n    adapter=\"postgres\",\n    url=\"postgresql://localhost:5432/mydb\",\n    timeout=30\n)\n</code></pre>"},{"location":"error_handling/#handle-asynchronous-errors","title":"Handle Asynchronous Errors","text":"<p>For asynchronous adapters, use try/except blocks within async functions:</p> <pre><code>async def fetch_data():\n    try:\n        return await MyModel.adapt_from_async(data, obj_key=\"async_mongo\")\n    except ConnectionError as e:\n        logger.error(f\"Connection failed: {e}\")\n        # Handle connection error\n    except AdapterError as e:\n        logger.error(f\"Adapter error: {e}\")\n        # Handle other adapter errors\n</code></pre>"},{"location":"error_handling/#resource-cleanup","title":"Resource Cleanup","text":"<p>Ensure resources are properly cleaned up, even in error scenarios:</p> <pre><code>try:\n    # Some operation that acquires resources\n    result = perform_operation()\n    return result\nexcept AdapterError:\n    # Handle the error\n    raise\nfinally:\n    # Clean up resources\n    cleanup_resources()\n</code></pre>"},{"location":"error_handling/#common-error-scenarios-and-solutions","title":"Common Error Scenarios and Solutions","text":""},{"location":"error_handling/#json-parsing-errors","title":"JSON Parsing Errors","text":"<pre><code>try:\n    model = MyModel.adapt_from(json_data, obj_key=\"json\")\nexcept ParseError as e:\n    if \"Expecting property name\" in str(e):\n        # Handle malformed JSON\n    elif \"Expecting value\" in str(e):\n        # Handle empty JSON\n</code></pre>"},{"location":"error_handling/#database-connection-errors","title":"Database Connection Errors","text":"<pre><code>try:\n    model = MyModel.adapt_from(db_config, obj_key=\"postgres\")\nexcept ConnectionError as e:\n    if \"authentication failed\" in str(e):\n        # Handle authentication issues\n    elif \"connection refused\" in str(e):\n        # Handle server unavailable\n    elif \"database does not exist\" in str(e):\n        # Handle missing database\n</code></pre>"},{"location":"error_handling/#empty-result-sets","title":"Empty Result Sets","text":"<pre><code>try:\n    model = MyModel.adapt_from(query_params, obj_key=\"mongo\", many=False)\nexcept ResourceError as e:\n    if \"No documents found\" in str(e):\n        # Handle empty result\n        return default_value\n</code></pre>"},{"location":"error_handling/#conclusion","title":"Conclusion","text":"<p>Proper error handling is essential for building robust applications with pydapter. By understanding the exception hierarchy and following best practices, you can create more resilient code that gracefully handles failure scenarios and provides clear feedback to users.</p>"},{"location":"getting_started/","title":"Exploring Pydapter: A Python Adapter Library","text":"<p>Pydapter is a powerful adapter library that lets you easily convert between Pydantic models and various data formats. This guide will help you get started with the non-database adapters.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>First, let's install pydapter and its dependencies:</p> <pre><code># Create a virtual environment (optional but recommended)\npython -m venv pydapter-demo\nsource pydapter-demo/bin/activate  # On Windows: pydapter-demo\\Scripts\\activate\n\n# Install pydapter and dependencies\nuv pip install pydapter\nuv pip install pandas  # For DataFrameAdapter and SeriesAdapter\nuv pip install xlsxwriter  # For ExcelAdapter\nuv pip install openpyxl  # Also needed for Excel support\n\n# Install optional modules\nuv pip install \"pydapter[protocols]\"      # For standardized model interfaces\nuv pip install \"pydapter[migrations-sql]\" # For database schema migrations\n\n# or install all adapters at once\nuv pip install \"pydapter[all]\"\n</code></pre>"},{"location":"getting_started/#basic-example-using-jsonadapter","title":"Basic Example: Using JsonAdapter","text":"<p>Let's start with a simple example using the JsonAdapter:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\nfrom pydapter.adapters.json_ import JsonAdapter\n\n# Define a Pydantic model\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n    active: bool = True\n    tags: List[str] = []\n\n# Create some test data\nusers = [\n    User(id=1, name=\"Alice\", email=\"alice@example.com\", tags=[\"admin\", \"staff\"]),\n    User(id=2, name=\"Bob\", email=\"bob@example.com\", active=False),\n    User(id=3, name=\"Charlie\", email=\"charlie@example.com\", tags=[\"staff\"]),\n]\n\n# Convert models to JSON\njson_data = JsonAdapter.to_obj(users, many=True)\nprint(\"JSON Output:\")\nprint(json_data)\n\n# Convert JSON back to models\nloaded_users = JsonAdapter.from_obj(User, json_data, many=True)\nprint(\"\\nLoaded users:\")\nfor user in loaded_users:\n    print(f\"{user.name} ({user.email}): Active={user.active}, Tags={user.tags}\")\n</code></pre>"},{"location":"getting_started/#using-the-adaptable-mixin-for-better-ergonomics","title":"Using the Adaptable Mixin for Better Ergonomics","text":"<p>Pydapter provides an <code>Adaptable</code> mixin that makes the API more ergonomic:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List\nfrom pydapter.core import Adaptable\nfrom pydapter.adapters.json_ import JsonAdapter\n\n# Define a model with the Adaptable mixin\nclass Product(BaseModel, Adaptable):\n    id: int\n    name: str\n    price: float\n    in_stock: bool = True\n\n# Register the JSON adapter\nProduct.register_adapter(JsonAdapter)\n\n# Create a product\nproduct = Product(id=101, name=\"Laptop\", price=999.99)\n\n# Convert to JSON using the mixin method\njson_data = product.adapt_to(obj_key=\"json\")\nprint(\"JSON Output:\")\nprint(json_data)\n\n# Convert back to a model\nloaded_product = Product.adapt_from(json_data, obj_key=\"json\")\nprint(f\"\\nLoaded product: {loaded_product.name} (${loaded_product.price})\")\n</code></pre>"},{"location":"getting_started/#working-with-csv","title":"Working with CSV","text":"<p>Here's how to use the CSV adapter:</p> <pre><code>from pydantic import BaseModel\nfrom pydapter.adapters.csv_ import CsvAdapter\n\n# Define a Pydantic model\nclass Employee(Adaptable, BaseModel):\n    id: int\n    name: str\n    department: str\n    salary: float\n    hire_date: str\n\n# Create some sample data\nemployees = [\n    Employee(id=1, name=\"Alice\", department=\"Engineering\", salary=85000, hire_date=\"2020-01-15\"),\n    Employee(id=2, name=\"Bob\", department=\"Marketing\", salary=75000, hire_date=\"2021-03-20\"),\n    Employee(id=3, name=\"Charlie\", department=\"Finance\", salary=95000, hire_date=\"2019-11-01\"),\n]\n\ncsv_data = CsvAdapter.to_obj(employees, many=True)\nprint(\"CSV Output:\")\nprint(csv_data)\n\n# Convert CSV back to models\nloaded_employees = CsvAdapter.from_obj(Employee, csv_data, many=True)\nprint(\"\\nLoaded employees:\")\nfor employee in loaded_employees:\n    print(f\"{employee.name} - {employee.department} (${employee.salary})\")\n\n# You can also save to a file and read from a file\nfrom pathlib import Path\n\n# Save to file\nPath(\"employees.csv\").write_text(csv_data)\n\n# Read from file\nfile_employees = CsvAdapter.from_obj(Employee, Path(\"employees.csv\"), many=True)\n</code></pre>"},{"location":"getting_started/#working-with-toml","title":"Working with TOML","text":"<p>Here's how to use the TOML adapter:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Dict, Optional\nfrom pydapter.adapters.toml_ import TomlAdapter\n\n# Define a Pydantic model\nclass AppConfig(BaseModel):\n    app_name: str\n    version: str\n    debug: bool = False\n    database: Dict[str, str] = {}\n    allowed_hosts: List[str] = []\n\n# Create a config\nconfig = AppConfig(\n    app_name=\"MyApp\",\n    version=\"1.0.0\",\n    debug=True,\n    database={\"host\": \"localhost\", \"port\": \"5432\", \"name\": \"myapp\"},\n    allowed_hosts=[\"localhost\", \"example.com\"]\n)\n\n# Convert to TOML\ntoml_data = TomlAdapter.to_obj(config)\nprint(\"TOML Output:\")\nprint(toml_data)\n\n# Convert TOML back to model\nloaded_config = TomlAdapter.from_obj(AppConfig, toml_data)\nprint(\"\\nLoaded config:\")\nprint(f\"App: {loaded_config.app_name} v{loaded_config.version}\")\nprint(f\"Debug mode: {loaded_config.debug}\")\nprint(f\"Database: {loaded_config.database}\")\nprint(f\"Allowed hosts: {loaded_config.allowed_hosts}\")\n\n# Save to file\nPath(\"config.toml\").write_text(toml_data)\n\n# Read from file\nfile_config = TomlAdapter.from_obj(AppConfig, Path(\"config.toml\"))\n</code></pre>"},{"location":"getting_started/#working-with-pandas-dataframe","title":"Working with Pandas DataFrame","text":"<p>Here's how to use the DataFrame adapter:</p> <pre><code>import pandas as pd\nfrom pydantic import BaseModel\nfrom pydapter.extras.pandas_ import DataFrameAdapter\n\n# Define a Pydantic model\nclass SalesRecord(BaseModel):\n    id: int\n    product: str\n    quantity: int\n    price: float\n    date: str\n\n# Create a sample DataFrame\ndf = pd.DataFrame([\n    {\"id\": 1, \"product\": \"Laptop\", \"quantity\": 2, \"price\": 999.99, \"date\": \"2023-01-15\"},\n    {\"id\": 2, \"product\": \"Monitor\", \"quantity\": 3, \"price\": 249.99, \"date\": \"2023-01-20\"},\n    {\"id\": 3, \"product\": \"Mouse\", \"quantity\": 5, \"price\": 29.99, \"date\": \"2023-01-25\"}\n])\n\n# Convert DataFrame to models\nsales_records = DataFrameAdapter.from_obj(SalesRecord, df, many=True)\nprint(\"DataFrame to Models:\")\nfor record in sales_records:\n    print(f\"{record.id}: {record.quantity} x {record.product} at ${record.price}\")\n\n# Convert models back to DataFrame\nnew_df = DataFrameAdapter.to_obj(sales_records, many=True)\nprint(\"\\nModels to DataFrame:\")\nprint(new_df)\n</code></pre>"},{"location":"getting_started/#working-with-excel-files","title":"Working with Excel Files","text":"<p>Here's how to use the Excel adapter:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\nfrom pydapter.extras.excel_ import ExcelAdapter\nfrom pathlib import Path\n\n# Define a Pydantic model\nclass Student(BaseModel):\n    id: int\n    name: str\n    grade: str\n    score: float\n\n# Create some sample data\nstudents = [\n    Student(id=1, name=\"Alice\", grade=\"A\", score=92.5),\n    Student(id=2, name=\"Bob\", grade=\"B\", score=85.0),\n    Student(id=3, name=\"Charlie\", grade=\"A-\", score=90.0),\n]\n\n# Convert to Excel and save to file\nexcel_data = ExcelAdapter.to_obj(students, many=True, sheet_name=\"Students\")\nwith open(\"students.xlsx\", \"wb\") as f:\n    f.write(excel_data)\n\nprint(\"Excel file saved as 'students.xlsx'\")\n\n# Read from Excel file\nloaded_students = ExcelAdapter.from_obj(Student, Path(\"students.xlsx\"), many=True)\nprint(\"\\nLoaded students:\")\nfor student in loaded_students:\n    print(f\"{student.name}: {student.grade} ({student.score})\")\n</code></pre>"},{"location":"getting_started/#error-handling","title":"Error Handling","text":"<p>Let's demonstrate proper error handling:</p> <pre><code>from pydantic import BaseModel, Field\nfrom pydapter.adapters.json_ import JsonAdapter\nfrom pydapter.exceptions import ParseError, ValidationError as AdapterValidationError\n\n# Define a model with validation constraints\nclass Product(BaseModel):\n    id: int = Field(gt=0)  # Must be greater than 0\n    name: str = Field(min_length=3)  # Must be at least 3 characters\n    price: float = Field(gt=0.0)  # Must be greater than 0\n\n# Handle parsing errors\ntry:\n    # Try to parse invalid JSON\n    invalid_json = \"{ 'id': 1, 'name': 'Laptop', price: 999.99 }\"  # Note the missing quotes around 'price'\n    product = JsonAdapter.from_obj(Product, invalid_json)\nexcept ParseError as e:\n    print(f\"Parsing error: {e}\")\n\n# Handle validation errors\ntry:\n    # Try to create a model with invalid data\n    valid_json = '{\"id\": 0, \"name\": \"A\", \"price\": -10.0}'  # All fields violate constraints\n    product = JsonAdapter.from_obj(Product, valid_json)\nexcept AdapterValidationError as e:\n    print(f\"Validation error: {e}\")\n    if hasattr(e, 'errors') and callable(e.errors):\n        for error in e.errors():\n            print(f\"  - {error['loc']}: {error['msg']}\")\n</code></pre>"},{"location":"getting_started/#using-protocols","title":"Using Protocols","text":"<p>Pydapter provides a set of standardized interfaces through the protocols module. These protocols allow you to add common capabilities to your models:</p> <pre><code>from pydapter.protocols import Identifiable, Temporal\n\n# Define a model with standardized interfaces\nclass User(Identifiable, Temporal):\n    name: str\n    email: str\n\n# Create a user\nuser = User(name=\"Alice\", email=\"alice@example.com\")\n\n# Access standardized properties\nprint(f\"User ID: {user.id}\")  # Automatically generated UUID\nprint(f\"Created at: {user.created_at}\")  # Automatically set timestamp\n\n# Update the timestamp\nuser.name = \"Alicia\"\nuser.update_timestamp()\nprint(f\"Updated at: {user.updated_at}\")\n</code></pre> <p>For more details, see the Protocols documentation and the Using Protocols tutorial.</p>"},{"location":"getting_started/#using-migrations","title":"Using Migrations","text":"<p>Pydapter provides tools for managing database schema changes through the migrations module:</p> <pre><code>from pydapter.migrations import AlembicAdapter\nimport mymodels  # Module containing your SQLAlchemy models\n\n# Initialize migrations\nAlembicAdapter.init_migrations(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\",\n    models_module=mymodels\n)\n\n# Create a migration\nrevision = AlembicAdapter.create_migration(\n    message=\"Create users table\",\n    autogenerate=True,\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\n\n# Apply migrations\nAlembicAdapter.upgrade(\n    revision=\"head\",\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\n</code></pre> <p>For more details, see the Migrations documentation and the Using Migrations tutorial.</p>"},{"location":"migration_guide/","title":"Migration Guide: Transitioning from dev/ Directory","text":"<p>This guide is for users who have been using the experimental protocols and migrations modules from the <code>dev/</code> directory. These modules have now been integrated into the main package as optional dependencies.</p>"},{"location":"migration_guide/#overview-of-changes","title":"Overview of Changes","text":"<p>The protocols and migrations modules have been moved from the <code>dev/</code> directory to the main package structure:</p> <ul> <li><code>dev/protocols/</code> \u2192 <code>pydapter.protocols</code></li> <li><code>dev/migrations/</code> \u2192 <code>pydapter.migrations</code></li> </ul> <p>The functionality remains largely the same, but there are some important changes to be aware of:</p> <ol> <li>The modules are now optional dependencies</li> <li>Import paths have changed</li> <li>Backward compatibility is maintained temporarily</li> <li>Type checking works regardless of installed dependencies</li> </ol>"},{"location":"migration_guide/#installation","title":"Installation","text":"<p>To use the new modules, you need to install them as optional dependencies:</p> <pre><code># For protocols module\npip install \"pydapter[protocols]\"\n\n# For migrations core functionality\npip install \"pydapter[migrations-core]\"\n\n# For SQL migrations with Alembic\npip install \"pydapter[migrations-sql]\"\n\n# For all migrations components\npip install \"pydapter[migrations]\"\n\n# For both protocols and migrations\npip install \"pydapter[migrations-all]\"\n\n# For all pydapter features\npip install \"pydapter[all]\"\n</code></pre>"},{"location":"migration_guide/#core-system","title":"Core System","text":"<p>The core adapter system remains unchanged in its API and functionality. The main changes are related to import paths and enhanced error handling. If you're using the core system directly, no code changes are required beyond updating import statements.</p>"},{"location":"migration_guide/#fields-system","title":"Fields System","text":"<p>The fields system has been integrated from the experimental <code>dev/</code> directory with enhanced validation and protocol integration. Field definitions and usage patterns remain the same, with improved type safety and documentation.</p>"},{"location":"migration_guide/#protocols-and-fields","title":"Protocols and Fields","text":"<p>The protocols and fields systems now work together seamlessly, with protocols leveraging pre-defined field definitions for consistency and reusability.</p>"},{"location":"migration_guide/#updating-import-statements","title":"Updating Import Statements","text":""},{"location":"migration_guide/#protocols-module","title":"Protocols Module","text":"<p>Old imports:</p> <pre><code>from dev.protocols import Identifiable, Temporal, Embedable, Invokable, Event\nfrom dev.protocols.types import Embedding, ExecutionStatus, Execution, Log\n</code></pre> <p>New imports:</p> <pre><code>from pydapter.protocols import Identifiable, Temporal, Embedable, Invokable, Event\nfrom pydapter.protocols import Embedding, ExecutionStatus, Execution, Log\n</code></pre>"},{"location":"migration_guide/#migrations-module","title":"Migrations Module","text":"<p>Old imports:</p> <pre><code>from dev.migrations import BaseMigrationAdapter, SyncMigrationAdapter, AsyncMigrationAdapter\nfrom dev.migrations.protocols import MigrationProtocol, AsyncMigrationProtocol\nfrom dev.migrations.exceptions import MigrationError\nfrom dev.migrations.sql.alembic_adapter import AlembicAdapter, AsyncAlembicAdapter\n</code></pre> <p>New imports:</p> <pre><code>from pydapter.migrations import BaseMigrationAdapter, SyncMigrationAdapter, AsyncMigrationAdapter\nfrom pydapter.migrations import MigrationProtocol, AsyncMigrationProtocol\nfrom pydapter.migrations import MigrationError\nfrom pydapter.migrations import AlembicAdapter, AsyncAlembicAdapter\n</code></pre>"},{"location":"migration_guide/#backward-compatibility","title":"Backward Compatibility","text":"<p>For a transitional period, the modules in the <code>dev/</code> directory will continue to work by re-exporting from the new locations. However, you will see deprecation warnings:</p> <pre><code>DeprecationWarning: Importing from dev.protocols is deprecated and will be\nremoved in a future version. Please use pydapter.protocols instead.\n</code></pre> <p>It's recommended to update your import statements as soon as possible to avoid issues when the backward compatibility is removed in a future version.</p>"},{"location":"migration_guide/#handling-missing-dependencies","title":"Handling Missing Dependencies","text":"<p>If you try to import from the new modules without installing the required dependencies, you'll get a clear error message:</p> <pre><code>ImportError: The 'protocols' feature requires the 'typing_extensions' package.\nInstall it with: pip install pydapter[protocols]\n</code></pre> <p>This helps guide you to install the correct dependencies.</p>"},{"location":"migration_guide/#type-checking","title":"Type Checking","text":"<p>The new modules are designed to work well with static type checkers like mypy, even if the optional dependencies are not installed. This is achieved through conditional imports with <code>TYPE_CHECKING</code>.</p> <pre><code>from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from pydapter.protocols import Identifiable, Temporal\n</code></pre>"},{"location":"migration_guide/#examples","title":"Examples","text":""},{"location":"migration_guide/#using-protocols","title":"Using Protocols","text":"<pre><code># Old code\nfrom dev.protocols import Identifiable, Temporal\n\nclass User(Identifiable, Temporal):\n    name: str\n    email: str\n\n# New code\nfrom pydapter.protocols import Identifiable, Temporal\n\nclass User(Identifiable, Temporal):\n    name: str\n    email: str\n</code></pre>"},{"location":"migration_guide/#using-migrations","title":"Using Migrations","text":"<pre><code># Old code\nfrom dev.migrations.sql.alembic_adapter import AlembicAdapter\n\nAlembicAdapter.init_migrations(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\",\n    models_module=models\n)\n\n# New code\nfrom pydapter.migrations import AlembicAdapter\n\nAlembicAdapter.init_migrations(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\",\n    models_module=models\n)\n</code></pre>"},{"location":"migration_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"migration_guide/#missing-dependencies","title":"Missing Dependencies","text":"<p>If you encounter import errors, make sure you've installed the required dependencies:</p> <pre><code>pip install \"pydapter[protocols]\"\npip install \"pydapter[migrations-sql]\"\n</code></pre>"},{"location":"migration_guide/#type-checking-issues","title":"Type Checking Issues","text":"<p>If you encounter type checking issues, make sure your type checker is configured to handle conditional imports with <code>TYPE_CHECKING</code>. For mypy, this should work out of the box.</p>"},{"location":"migration_guide/#circular-imports","title":"Circular Imports","text":"<p>If you encounter circular import errors, try using relative imports within your modules:</p> <pre><code># Instead of\nfrom pydapter.protocols import Identifiable\n\n# Use\nfrom ..protocols import Identifiable\n</code></pre>"},{"location":"migration_guide/#timeline-for-deprecation","title":"Timeline for Deprecation","text":"<p>The backward compatibility with the <code>dev/</code> directory will be maintained for at least one minor version release. After that, the modules in the <code>dev/</code> directory will be removed, and you'll need to use the new import paths.</p>"},{"location":"migration_guide/#additional-resources","title":"Additional Resources","text":"<ul> <li>Protocols Documentation</li> <li>Migrations Documentation</li> <li>Using Protocols Tutorial</li> <li>Using Migrations Tutorial</li> </ul>"},{"location":"migrations/","title":"Migrations Module","text":"<p>The Migrations module provides a framework for managing database schema changes in a controlled, versioned manner. It follows pydapter's adapter pattern philosophy, offering both synchronous and asynchronous interfaces for different database backends.</p>"},{"location":"migrations/#installation","title":"Installation","text":"<p>The Migrations module is available as an optional dependency with different components:</p> <pre><code># Core migrations functionality (minimal dependencies)\npip install pydapter[migrations-core]\n\n# SQL migrations with Alembic support\npip install pydapter[migrations-sql]\n\n# All migrations components\npip install pydapter[migrations]\n\n# Migrations with protocols support\npip install pydapter[migrations-all]\n</code></pre>"},{"location":"migrations/#key-concepts","title":"Key Concepts","text":""},{"location":"migrations/#migration-adapters","title":"Migration Adapters","text":"<p>Migration adapters implement the migration protocols and provide concrete functionality for specific database backends. The base module includes:</p> <ul> <li><code>BaseMigrationAdapter</code>: Abstract base class for all migration adapters</li> <li><code>SyncMigrationAdapter</code>: Base class for synchronous migration adapters</li> <li><code>AsyncMigrationAdapter</code>: Base class for asynchronous migration adapters</li> </ul>"},{"location":"migrations/#migration-protocols","title":"Migration Protocols","text":"<p>The module defines protocols that specify the interface for migration operations:</p> <ul> <li><code>MigrationProtocol</code>: Protocol for synchronous migration operations</li> <li><code>AsyncMigrationProtocol</code>: Protocol for asynchronous migration operations</li> </ul>"},{"location":"migrations/#sql-migrations-with-alembic","title":"SQL Migrations with Alembic","text":"<p>The SQL migrations implementation uses Alembic, a database migration tool for SQLAlchemy. It provides:</p> <ul> <li><code>AlembicAdapter</code>: Synchronous Alembic-based migration adapter</li> <li><code>AsyncAlembicAdapter</code>: Asynchronous Alembic-based migration adapter</li> </ul>"},{"location":"migrations/#basic-usage","title":"Basic Usage","text":""},{"location":"migrations/#initializing-migrations","title":"Initializing Migrations","text":"<p>Before you can create and apply migrations, you need to initialize the migration environment:</p> <pre><code>from pydapter.migrations import AlembicAdapter\nimport mymodels  # Module containing your SQLAlchemy models\n\n# Initialize migrations\nAlembicAdapter.init_migrations(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\",\n    models_module=mymodels\n)\n</code></pre> <p>This creates the necessary directory structure and configuration files for Alembic.</p>"},{"location":"migrations/#creating-migrations","title":"Creating Migrations","text":"<p>You can create migrations manually or automatically based on model changes:</p> <pre><code># Create a migration with auto-generation based on model changes\nrevision = AlembicAdapter.create_migration(\n    message=\"Add users table\",\n    autogenerate=True,\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\nprint(f\"Created migration: {revision}\")\n</code></pre> <p>The <code>autogenerate</code> parameter tells Alembic to compare your models with the current database schema and generate the necessary changes.</p>"},{"location":"migrations/#applying-migrations","title":"Applying Migrations","text":"<p>To apply migrations and update your database schema:</p> <pre><code># Upgrade to the latest version\nAlembicAdapter.upgrade(\n    revision=\"head\",  # \"head\" means the latest version\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\n</code></pre>"},{"location":"migrations/#reverting-migrations","title":"Reverting Migrations","text":"<p>If you need to revert to a previous version:</p> <pre><code># Downgrade to a specific revision\nAlembicAdapter.downgrade(\n    revision=\"ae1027a6acf\",  # Specific revision identifier\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\n</code></pre>"},{"location":"migrations/#checking-migration-status","title":"Checking Migration Status","text":"<p>You can check the current migration status:</p> <pre><code># Get the current revision\ncurrent = AlembicAdapter.get_current_revision(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\nprint(f\"Current revision: {current}\")\n\n# Get the full migration history\nhistory = AlembicAdapter.get_migration_history(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\nfor migration in history:\n    print(f\"{migration['revision']}: {migration['description']}\")\n</code></pre>"},{"location":"migrations/#asynchronous-migrations","title":"Asynchronous Migrations","text":"<p>For applications using asynchronous database connections, you can use the async migration adapter:</p> <pre><code>from pydapter.migrations import AsyncAlembicAdapter\nimport mymodels\n\n# Initialize migrations\nawait AsyncAlembicAdapter.init_migrations(\n    directory=\"migrations\",\n    connection_string=\"postgresql+asyncpg://user:pass@localhost/mydb\",\n    models_module=mymodels\n)\n\n# Create a migration\nrevision = await AsyncAlembicAdapter.create_migration(\n    message=\"Add users table\",\n    autogenerate=True,\n    directory=\"migrations\",\n    connection_string=\"postgresql+asyncpg://user:pass@localhost/mydb\"\n)\n\n# Apply migrations\nawait AsyncAlembicAdapter.upgrade(\n    revision=\"head\",\n    directory=\"migrations\",\n    connection_string=\"postgresql+asyncpg://user:pass@localhost/mydb\"\n)\n</code></pre>"},{"location":"migrations/#error-handling","title":"Error Handling","text":"<p>The migrations module provides a comprehensive error hierarchy:</p> <ul> <li><code>MigrationError</code>: Base exception for all migration errors</li> <li><code>MigrationInitError</code>: Raised when initialization fails</li> <li><code>MigrationCreationError</code>: Raised when migration creation fails</li> <li><code>MigrationUpgradeError</code>: Raised when upgrade fails</li> <li><code>MigrationDowngradeError</code>: Raised when downgrade fails</li> <li><code>MigrationNotFoundError</code>: Raised when a specified revision is not found</li> </ul> <p>Example of handling migration errors:</p> <pre><code>from pydapter.migrations import AlembicAdapter, MigrationError, MigrationUpgradeError\n\ntry:\n    AlembicAdapter.upgrade(\n        revision=\"head\",\n        directory=\"migrations\",\n        connection_string=\"postgresql://user:pass@localhost/mydb\"\n    )\nexcept MigrationUpgradeError as e:\n    print(f\"Failed to upgrade: {e}\")\n    # Handle specific upgrade error\nexcept MigrationError as e:\n    print(f\"Migration error: {e}\")\n    # Handle general migration error\n</code></pre>"},{"location":"migrations/#advanced-usage","title":"Advanced Usage","text":""},{"location":"migrations/#custom-migration-scripts","title":"Custom Migration Scripts","text":"<p>While auto-generated migrations work for many cases, you might need to write custom migration scripts for complex changes:</p> <ol> <li>Create a migration without auto-generation:</li> </ol> <pre><code>revision = AlembicAdapter.create_migration(\n    message=\"Custom data migration\",\n    autogenerate=False,\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\"\n)\n</code></pre> <ol> <li>Edit the generated script in the <code>migrations/versions/</code> directory:</li> </ol> <pre><code>\"\"\"Custom data migration\n\nRevision ID: ae1027a6acf\nRevises: 1a2b3c4d5e6f\nCreate Date: 2025-05-16 10:30:00.000000\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n# revision identifiers\nrevision = 'ae1027a6acf'\ndown_revision = '1a2b3c4d5e6f'\nbranch_labels = None\ndepends_on = None\n\ndef upgrade():\n    # Custom upgrade operations\n    op.execute(\"\"\"\n        UPDATE users\n        SET status = 'active'\n        WHERE status = 'pending' AND created_at &lt; NOW() - INTERVAL '7 days'\n    \"\"\")\n\ndef downgrade():\n    # Custom downgrade operations\n    # Note: Data migrations are often not reversible\n    pass\n</code></pre>"},{"location":"migrations/#working-with-multiple-databases","title":"Working with Multiple Databases","text":"<p>If your application uses multiple databases, you can create separate migration directories for each:</p> <pre><code># Initialize migrations for the main database\nAlembicAdapter.init_migrations(\n    directory=\"migrations/main\",\n    connection_string=\"postgresql://user:pass@localhost/main_db\",\n    models_module=main_models\n)\n\n# Initialize migrations for the analytics database\nAlembicAdapter.init_migrations(\n    directory=\"migrations/analytics\",\n    connection_string=\"postgresql://user:pass@localhost/analytics_db\",\n    models_module=analytics_models\n)\n</code></pre> <p>Then apply migrations to each database separately:</p> <pre><code># Upgrade main database\nAlembicAdapter.upgrade(\n    revision=\"head\",\n    directory=\"migrations/main\",\n    connection_string=\"postgresql://user:pass@localhost/main_db\"\n)\n\n# Upgrade analytics database\nAlembicAdapter.upgrade(\n    revision=\"head\",\n    directory=\"migrations/analytics\",\n    connection_string=\"postgresql://user:pass@localhost/analytics_db\"\n)\n</code></pre>"},{"location":"migrations/#integration-with-sqlalchemy-models","title":"Integration with SQLAlchemy Models","text":"<p>The migrations module works best with SQLAlchemy models that follow the declarative base pattern:</p> <pre><code>from sqlalchemy import Column, Integer, String, create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = \"users\"\n\n    id = Column(Integer, primary_key=True)\n    username = Column(String, unique=True, nullable=False)\n    email = Column(String, unique=True, nullable=False)\n\n# Later, when initializing migrations:\nAlembicAdapter.init_migrations(\n    directory=\"migrations\",\n    connection_string=\"postgresql://user:pass@localhost/mydb\",\n    models_module=sys.modules[__name__]  # Current module containing models\n)\n</code></pre>"},{"location":"migrations/#best-practices","title":"Best Practices","text":""},{"location":"migrations/#migration-workflow","title":"Migration Workflow","text":"<p>Follow these best practices for a smooth migration workflow:</p> <ol> <li>Always back up your database before applying migrations</li> <li>Test migrations in a development environment first</li> <li>Keep migrations small and focused on specific changes</li> <li>Include descriptive messages for each migration</li> <li>Ensure both upgrade and downgrade functions are implemented</li> <li>Use transactions for data safety</li> </ol>"},{"location":"migrations/#organizing-migrations","title":"Organizing Migrations","text":"<p>For larger projects, consider these organizational tips:</p> <ol> <li>Use a consistent naming convention for migration files</li> <li>Group related migrations in branches when appropriate</li> <li>Document complex migrations with comments</li> <li>Include the related issue or ticket number in migration messages</li> </ol>"},{"location":"migrations/#deployment-considerations","title":"Deployment Considerations","text":"<p>When deploying migrations to production:</p> <ol> <li>Include migrations in your CI/CD pipeline</li> <li>Apply migrations during maintenance windows when possible</li> <li>Have a rollback plan for each migration</li> <li>Monitor database performance during and after migrations</li> <li>Consider using a separate deployment step for migrations</li> </ol>"},{"location":"migrations/#extending-the-migrations-framework","title":"Extending the Migrations Framework","text":"<p>You can extend the migrations framework by creating custom adapters for other database systems:</p> <pre><code>from pydapter.migrations.base import SyncMigrationAdapter\nfrom typing import ClassVar, Optional, List, Dict, Any\n\nclass CustomDatabaseAdapter(SyncMigrationAdapter):\n    \"\"\"Custom migration adapter for a specific database system.\"\"\"\n\n    migration_key: ClassVar[str] = \"custom_db\"\n\n    @classmethod\n    def init_migrations(cls, directory: str, **kwargs) -&gt; None:\n        # Implementation for initializing migrations\n        pass\n\n    @classmethod\n    def create_migration(cls, message: str, autogenerate: bool = True, **kwargs) -&gt; str:\n        # Implementation for creating migrations\n        pass\n\n    @classmethod\n    def upgrade(cls, revision: str = \"head\", **kwargs) -&gt; None:\n        # Implementation for upgrading\n        pass\n\n    @classmethod\n    def downgrade(cls, revision: str, **kwargs) -&gt; None:\n        # Implementation for downgrading\n        pass\n\n    @classmethod\n    def get_current_revision(cls, **kwargs) -&gt; Optional[str]:\n        # Implementation for getting current revision\n        pass\n\n    @classmethod\n    def get_migration_history(cls, **kwargs) -&gt; List[Dict[str, Any]]:\n        # Implementation for getting migration history\n        pass\n</code></pre>"},{"location":"migrations/#troubleshooting","title":"Troubleshooting","text":""},{"location":"migrations/#common-issues","title":"Common Issues","text":"<ol> <li>Missing dependencies:</li> </ol> <pre><code>ImportError: The 'migrations-sql' feature requires the 'sqlalchemy' package.\n</code></pre> <p>Solution: Install the required dependencies with    <code>pip install pydapter[migrations-sql]</code></p> <ol> <li>Alembic command not found:</li> </ol> <pre><code>ModuleNotFoundError: No module named 'alembic'\n</code></pre> <p>Solution: Install Alembic with <code>pip install alembic</code></p> <ol> <li> <p>Autogeneration not detecting changes: Solution: Ensure your models are    imported and accessible in the environment where migrations are created</p> </li> <li> <p>Conflicts between migrations: Solution: Ensure you're working with the    latest revision before creating new migrations</p> </li> </ol>"},{"location":"migrations/#getting-help","title":"Getting Help","text":"<p>If you encounter issues with migrations, check:</p> <ol> <li>The Alembic documentation: https://alembic.sqlalchemy.org/</li> <li>SQLAlchemy documentation: https://docs.sqlalchemy.org/</li> <li>pydapter GitHub issues: https://github.com/yourusername/pydapter/issues</li> </ol>"},{"location":"neo4j_adapter/","title":"Neo4j Adapter Tutorial for Pydapter","text":"<p>This tutorial will show you how to use pydapter's Neo4j adapter to seamlessly convert between Pydantic models and Neo4j graph databases. You'll learn how to model, store, and query graph data using Pydantic's validation capabilities.</p>"},{"location":"neo4j_adapter/#prerequisites","title":"Prerequisites","text":""},{"location":"neo4j_adapter/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code># Create a virtual environment if you haven't already\npython -m venv pydapter-demo\nsource pydapter-demo/bin/activate  # On Windows: pydapter-demo\\Scripts\\activate\n\n# Install dependencies\npip install pydantic neo4j\n\n# Install pydapter (if you haven't done so already)\n# Either from PyPI when available:\n# pip install pydapter\n# Or from the repository:\ngit clone https://github.com/ohdearquant/pydapter.git\ncd pydapter\npip install -e .\n</code></pre>"},{"location":"neo4j_adapter/#2-set-up-neo4j","title":"2. Set Up Neo4j","text":"<p>The easiest way to set up Neo4j is using Docker:</p> <pre><code># Run Neo4j in Docker with a password\ndocker run \\\n    --name neo4j-pydapter \\\n    -p 7474:7474 -p 7687:7687 \\\n    -e NEO4J_AUTH=neo4j/password \\\n    -d neo4j:latest\n</code></pre> <p>Alternatively, you can:</p> <ul> <li>Download and install Neo4j Desktop from   Neo4j's website</li> <li>Use Neo4j AuraDB cloud service</li> <li>Install Neo4j directly on your system</li> </ul> <p>With Docker, you can access:</p> <ul> <li>Neo4j Browser UI at http://localhost:7474</li> <li>Bolt protocol at bolt://localhost:7687</li> </ul>"},{"location":"neo4j_adapter/#basic-example-person-management-system","title":"Basic Example: Person Management System","text":"<p>Let's build a simple person management system using Neo4j and pydapter:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\n\n# Neo4j connection settings\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_AUTH = (\"neo4j\", \"password\")  # Default credentials, change if different\n\n# Define a Pydantic model\nclass Person(BaseModel):\n    id: str\n    name: str\n    age: int\n    email: Optional[str] = None\n    interests: List[str] = []\n\n# Create some test data\npeople = [\n    Person(id=\"p1\", name=\"Alice\", age=30, email=\"alice@example.com\", interests=[\"coding\", \"hiking\"]),\n    Person(id=\"p2\", name=\"Bob\", age=25, email=\"bob@example.com\", interests=[\"gaming\", \"cooking\"]),\n    Person(id=\"p3\", name=\"Charlie\", age=35, email=\"charlie@example.com\", interests=[\"reading\", \"travel\"])\n]\n\n# Store data in Neo4j\ndef store_people(people_list):\n    print(f\"Storing {len(people_list)} people in Neo4j...\")\n\n    for person in people_list:\n        result = Neo4jAdapter.to_obj(\n            person,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Person\",  # Node label in Neo4j\n            merge_on=\"id\"    # Property to use for MERGE operation\n        )\n        print(f\"Stored {person.name}: {result}\")\n\n# Retrieve all people\ndef get_all_people():\n    print(\"Retrieving all people from Neo4j...\")\n\n    people = Neo4jAdapter.from_obj(\n        Person,\n        {\n            \"url\": NEO4J_URI,\n            \"auth\": NEO4J_AUTH,\n            \"label\": \"Person\"\n        },\n        many=True\n    )\n\n    print(f\"Found {len(people)} people:\")\n    for person in people:\n        print(f\"  - {person.name} (Age: {person.age}, Email: {person.email})\")\n        if person.interests:\n            print(f\"    Interests: {', '.join(person.interests)}\")\n\n    return people\n\n# Find people by property\ndef find_people_by_property(property_name, property_value):\n    print(f\"Finding people with {property_name}={property_value}...\")\n\n    where_clause = f\"n.{property_name} = '{property_value}'\"\n\n    people = Neo4jAdapter.from_obj(\n        Person,\n        {\n            \"url\": NEO4J_URI,\n            \"auth\": NEO4J_AUTH,\n            \"label\": \"Person\",\n            \"where\": where_clause\n        },\n        many=True\n    )\n\n    print(f\"Found {len(people)} matching people:\")\n    for person in people:\n        print(f\"  - {person.name} (Age: {person.age}, Email: {person.email})\")\n\n    return people\n\n# Main function to demo the adapter\ndef main():\n    # First, store people\n    store_people(people)\n\n    # Retrieve all people\n    all_people = get_all_people()\n\n    # Find people with specific properties\n    young_people = find_people_by_property(\"age\", \"25\")\n\n    # Find by email domain (using ENDS WITH in Cypher)\n    print(\"\\nFinding people with example.com email addresses...\")\n    example_emails = Neo4jAdapter.from_obj(\n        Person,\n        {\n            \"url\": NEO4J_URI,\n            \"auth\": NEO4J_AUTH,\n            \"label\": \"Person\",\n            \"where\": \"n.email ENDS WITH 'example.com'\"\n        },\n        many=True\n    )\n\n    print(f\"Found {len(example_emails)} people with example.com emails:\")\n    for person in example_emails:\n        print(f\"  - {person.name}: {person.email}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"neo4j_adapter/#working-with-relationships","title":"Working with Relationships","text":"<p>One of Neo4j's key features is its ability to model relationships between nodes. Let's expand our example to include relationships:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\nfrom neo4j import GraphDatabase\n\n# Neo4j connection settings\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_AUTH = (\"neo4j\", \"password\")\n\n# Define models\nclass Person(BaseModel):\n    id: str\n    name: str\n    age: int\n    email: Optional[str] = None\n\nclass Hobby(BaseModel):\n    id: str\n    name: str\n    category: Optional[str] = None\n\n# Custom function to create relationships\n# (Since pydapter doesn't directly handle relationships yet)\ndef create_relationship(person_id, hobby_id, relationship_type=\"ENJOYS\"):\n    \"\"\"Create a relationship between a Person and a Hobby\"\"\"\n    driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n\n    with driver.session() as session:\n        result = session.run(\n            f\"\"\"\n            MATCH (p:Person {{id: $person_id}})\n            MATCH (h:Hobby {{id: $hobby_id}})\n            MERGE (p)-[r:{relationship_type}]-&gt;(h)\n            RETURN p.name, h.name\n            \"\"\",\n            person_id=person_id,\n            hobby_id=hobby_id\n        )\n\n        for record in result:\n            print(f\"Created relationship: {record['p.name']} {relationship_type} {record['h.name']}\")\n\n    driver.close()\n\n# Function to find people who enjoy a specific hobby\ndef find_people_by_hobby(hobby_name):\n    \"\"\"Find all people who enjoy a specific hobby\"\"\"\n    driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n\n    people_list = []\n\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (p:Person)-[:ENJOYS]-&gt;(h:Hobby {name: $hobby_name})\n            RETURN p\n            \"\"\",\n            hobby_name=hobby_name\n        )\n\n        for record in result:\n            # Convert Neo4j node properties to dict\n            person_data = dict(record[\"p\"].items())\n            # Create Pydantic model from data\n            person = Person(**person_data)\n            people_list.append(person)\n\n    driver.close()\n    return people_list\n\n# Function to find hobbies for a specific person\ndef find_hobbies_for_person(person_id):\n    \"\"\"Find all hobbies for a specific person\"\"\"\n    driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n\n    hobbies_list = []\n\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (p:Person {id: $person_id})-[:ENJOYS]-&gt;(h:Hobby)\n            RETURN h\n            \"\"\",\n            person_id=person_id\n        )\n\n        for record in result:\n            hobby_data = dict(record[\"h\"].items())\n            hobby = Hobby(**hobby_data)\n            hobbies_list.append(hobby)\n\n    driver.close()\n    return hobbies_list\n\n# Main function to demo relationships\ndef main():\n    # Create people\n    people = [\n        Person(id=\"p1\", name=\"Alice\", age=30, email=\"alice@example.com\"),\n        Person(id=\"p2\", name=\"Bob\", age=25, email=\"bob@example.com\"),\n        Person(id=\"p3\", name=\"Charlie\", age=35, email=\"charlie@example.com\")\n    ]\n\n    # Create hobbies\n    hobbies = [\n        Hobby(id=\"h1\", name=\"Coding\", category=\"Technical\"),\n        Hobby(id=\"h2\", name=\"Hiking\", category=\"Outdoor\"),\n        Hobby(id=\"h3\", name=\"Reading\", category=\"Indoor\"),\n        Hobby(id=\"h4\", name=\"Cooking\", category=\"Indoor\"),\n        Hobby(id=\"h5\", name=\"Gaming\", category=\"Entertainment\")\n    ]\n\n    # Store people in Neo4j\n    print(\"Storing people...\")\n    for person in people:\n        Neo4jAdapter.to_obj(\n            person,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Person\",\n            merge_on=\"id\"\n        )\n\n    # Store hobbies in Neo4j\n    print(\"\\nStoring hobbies...\")\n    for hobby in hobbies:\n        Neo4jAdapter.to_obj(\n            hobby,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Hobby\",\n            merge_on=\"id\"\n        )\n\n    # Create relationships\n    print(\"\\nCreating relationships...\")\n    # Alice enjoys Coding, Hiking, and Reading\n    create_relationship(\"p1\", \"h1\")\n    create_relationship(\"p1\", \"h2\")\n    create_relationship(\"p1\", \"h3\")\n\n    # Bob enjoys Gaming and Cooking\n    create_relationship(\"p2\", \"h4\")\n    create_relationship(\"p2\", \"h5\")\n\n    # Charlie enjoys Reading and Hiking\n    create_relationship(\"p3\", \"h2\")\n    create_relationship(\"p3\", \"h3\")\n\n    # Find people who enjoy Hiking\n    print(\"\\nPeople who enjoy Hiking:\")\n    hikers = find_people_by_hobby(\"Hiking\")\n    for person in hikers:\n        print(f\"  - {person.name} (Age: {person.age})\")\n\n    # Find hobbies for Alice\n    print(\"\\nAlice's hobbies:\")\n    alice_hobbies = find_hobbies_for_person(\"p1\")\n    for hobby in alice_hobbies:\n        print(f\"  - {hobby.name} (Category: {hobby.category})\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"neo4j_adapter/#advanced-example-movie-recommendation-system","title":"Advanced Example: Movie Recommendation System","text":"<p>Let's build a more complex example - a movie recommendation system that demonstrates advanced Neo4j features and pydapter integration:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional, Dict, Any\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\nfrom neo4j import GraphDatabase\nimport random\n\n# Neo4j connection settings\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_AUTH = (\"neo4j\", \"password\")\n\n# Define our models\nclass Person(BaseModel):\n    id: str\n    name: str\n    age: Optional[int] = None\n\nclass Movie(BaseModel):\n    id: str\n    title: str\n    year: int\n    genre: List[str] = []\n    rating: Optional[float] = None\n\nclass Actor(Person):\n    roles: List[str] = []\n\nclass Director(Person):\n    movies_directed: int = 0\n\n# Helper function to create Neo4j driver\ndef get_driver():\n    return GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n\n# Initialize the database with schema and constraints\ndef initialize_database():\n    driver = get_driver()\n\n    with driver.session() as session:\n        # Create constraints to ensure uniqueness\n        session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (p:Person) REQUIRE p.id IS UNIQUE\")\n        session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (m:Movie) REQUIRE m.id IS UNIQUE\")\n\n    driver.close()\n    print(\"Database initialized with constraints\")\n\n# Helper function to create relationships\ndef create_relationship(start_id, end_id, start_label, end_label, rel_type, properties=None):\n    driver = get_driver()\n\n    props_str = \"\"\n    if properties:\n        props_list = [f\"{k}: ${k}\" for k in properties.keys()]\n        props_str = \"{\" + \", \".join(props_list) + \"}\"\n\n    with driver.session() as session:\n        query = f\"\"\"\n        MATCH (a:{start_label} {{id: $start_id}})\n        MATCH (b:{end_label} {{id: $end_id}})\n        MERGE (a)-[r:{rel_type} {props_str}]-&gt;(b)\n        RETURN a.name, b.title\n        \"\"\"\n\n        params = {\"start_id\": start_id, \"end_id\": end_id}\n        if properties:\n            params.update(properties)\n\n        result = session.run(query, params)\n        data = result.single()\n        if data:\n            print(f\"Created relationship: {data[0]} {rel_type} {data[1]}\")\n\n    driver.close()\n\n# Populate the database with sample data\ndef populate_database():\n    # Create some movies\n    movies = [\n        Movie(id=\"m1\", title=\"The Matrix\", year=1999,\n              genre=[\"Sci-Fi\", \"Action\"], rating=8.7),\n        Movie(id=\"m2\", title=\"Inception\", year=2010,\n              genre=[\"Sci-Fi\", \"Action\", \"Thriller\"], rating=8.8),\n        Movie(id=\"m3\", title=\"The Shawshank Redemption\", year=1994,\n              genre=[\"Drama\"], rating=9.3),\n        Movie(id=\"m4\", title=\"Pulp Fiction\", year=1994,\n              genre=[\"Crime\", \"Drama\"], rating=8.9),\n        Movie(id=\"m5\", title=\"The Dark Knight\", year=2008,\n              genre=[\"Action\", \"Crime\", \"Drama\"], rating=9.0),\n    ]\n\n    # Create some actors\n    actors = [\n        Actor(id=\"a1\", name=\"Keanu Reeves\", age=57, roles=[\"Neo\", \"John Wick\"]),\n        Actor(id=\"a2\", name=\"Leonardo DiCaprio\", age=46, roles=[\"Dom Cobb\", \"Jack Dawson\"]),\n        Actor(id=\"a3\", name=\"Morgan Freeman\", age=84, roles=[\"Ellis Boyd 'Red' Redding\"]),\n        Actor(id=\"a4\", name=\"Tim Robbins\", age=62, roles=[\"Andy Dufresne\"]),\n        Actor(id=\"a5\", name=\"John Travolta\", age=67, roles=[\"Vincent Vega\"]),\n        Actor(id=\"a6\", name=\"Samuel L. Jackson\", age=72, roles=[\"Jules Winnfield\"]),\n        Actor(id=\"a7\", name=\"Christian Bale\", age=47, roles=[\"Bruce Wayne\"]),\n    ]\n\n    # Create some directors\n    directors = [\n        Director(id=\"d1\", name=\"Lana Wachowski\", age=56, movies_directed=5),\n        Director(id=\"d2\", name=\"Christopher Nolan\", age=51, movies_directed=11),\n        Director(id=\"d3\", name=\"Frank Darabont\", age=62, movies_directed=4),\n        Director(id=\"d4\", name=\"Quentin Tarantino\", age=58, movies_directed=9),\n    ]\n\n    # Store movies in Neo4j\n    print(\"Storing movies...\")\n    for movie in movies:\n        Neo4jAdapter.to_obj(\n            movie,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Movie\",\n            merge_on=\"id\"\n        )\n\n    # Store actors in Neo4j\n    print(\"\\nStoring actors...\")\n    for actor in actors:\n        # Convert to dict and add label\n        actor_dict = actor.model_dump()\n\n        # Store using Neo4jAdapter\n        Neo4jAdapter.to_obj(\n            actor,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Actor\",  # Use Actor label\n            merge_on=\"id\"\n        )\n\n    # Store directors in Neo4j\n    print(\"\\nStoring directors...\")\n    for director in directors:\n        Neo4jAdapter.to_obj(\n            director,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Director\",  # Use Director label\n            merge_on=\"id\"\n        )\n\n    # Create relationships\n    print(\"\\nCreating relationships...\")\n\n    # Matrix relationships\n    create_relationship(\"a1\", \"m1\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Neo\"})\n    create_relationship(\"d1\", \"m1\", \"Director\", \"Movie\", \"DIRECTED\")\n\n    # Inception relationships\n    create_relationship(\"a2\", \"m2\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Dom Cobb\"})\n    create_relationship(\"d2\", \"m2\", \"Director\", \"Movie\", \"DIRECTED\")\n\n    # Shawshank Redemption relationships\n    create_relationship(\"a3\", \"m3\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Ellis Boyd 'Red' Redding\"})\n    create_relationship(\"a4\", \"m3\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Andy Dufresne\"})\n    create_relationship(\"d3\", \"m3\", \"Director\", \"Movie\", \"DIRECTED\")\n\n    # Pulp Fiction relationships\n    create_relationship(\"a5\", \"m4\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Vincent Vega\"})\n    create_relationship(\"a6\", \"m4\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Jules Winnfield\"})\n    create_relationship(\"d4\", \"m4\", \"Director\", \"Movie\", \"DIRECTED\")\n\n    # Dark Knight relationships\n    create_relationship(\"a7\", \"m5\", \"Actor\", \"Movie\", \"ACTED_IN\", {\"role\": \"Bruce Wayne\"})\n    create_relationship(\"d2\", \"m5\", \"Director\", \"Movie\", \"DIRECTED\")\n\n    # Create user ratings\n    create_user_ratings()\n\n    print(\"Database populated with sample data\")\n\n# Create some users and their ratings\ndef create_user_ratings():\n    # Create users\n    users = [\n        Person(id=\"u1\", name=\"User One\", age=25),\n        Person(id=\"u2\", name=\"User Two\", age=35),\n        Person(id=\"u3\", name=\"User Three\", age=45),\n    ]\n\n    # Store users\n    print(\"\\nStoring users...\")\n    for user in users:\n        Neo4jAdapter.to_obj(\n            user,\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"User\",\n            merge_on=\"id\"\n        )\n\n    # Create rating relationships\n    driver = get_driver()\n\n    with driver.session() as session:\n        # Get all movie IDs\n        result = session.run(\"MATCH (m:Movie) RETURN m.id AS id\")\n        movie_ids = [record[\"id\"] for record in result]\n\n        # For each user, create some random ratings\n        for user_id in [\"u1\", \"u2\", \"u3\"]:\n            for movie_id in movie_ids:\n                # Randomly decide if user rated this movie\n                if random.random() &gt; 0.3:  # 70% chance of rating\n                    rating = round(random.uniform(1, 5) * 2) / 2  # Rating from 1 to 5, in 0.5 steps\n\n                    session.run(\n                        \"\"\"\n                        MATCH (u:User {id: $user_id})\n                        MATCH (m:Movie {id: $movie_id})\n                        MERGE (u)-[r:RATED]-&gt;(m)\n                        SET r.rating = $rating\n                        \"\"\",\n                        user_id=user_id,\n                        movie_id=movie_id,\n                        rating=rating\n                    )\n                    print(f\"User {user_id} rated movie {movie_id} with {rating}\")\n\n    driver.close()\n\n# Function to get movie recommendations for a user\ndef get_movie_recommendations(user_id):\n    \"\"\"\n    Get movie recommendations for a user based on:\n    1. Movies they haven't seen\n    2. Movies liked by users with similar tastes\n    3. Movies in genres they like\n    \"\"\"\n    driver = get_driver()\n\n    recommendations = []\n\n    with driver.session() as session:\n        # Get movies the user hasn't rated,\n        # but are highly rated by users with similar tastes\n        result = session.run(\n            \"\"\"\n            MATCH (target:User {id: $user_id})-[r1:RATED]-&gt;(m:Movie)\n            MATCH (other:User)-[r2:RATED]-&gt;(m)\n            WHERE other.id &lt;&gt; $user_id AND abs(r1.rating - r2.rating) &lt; 1\n            MATCH (other)-[r3:RATED]-&gt;(rec:Movie)\n            WHERE r3.rating &gt;= 4\n            AND NOT EXISTS { MATCH (target)-[:RATED]-&gt;(rec) }\n            WITH rec, count(*) AS strength, avg(r3.rating) AS avg_rating\n            ORDER BY strength DESC, avg_rating DESC\n            LIMIT 5\n            RETURN rec\n            \"\"\",\n            user_id=user_id\n        )\n\n        for record in result:\n            movie_data = dict(record[\"rec\"].items())\n            movie = Movie(**movie_data)\n            recommendations.append(movie)\n\n    driver.close()\n    return recommendations\n\n# Get movies directed by a specific director\ndef get_movies_by_director(director_name):\n    \"\"\"Get all movies directed by a specific director\"\"\"\n    driver = get_driver()\n\n    movies_list = []\n\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (d:Director {name: $director_name})-[:DIRECTED]-&gt;(m:Movie)\n            RETURN m\n            \"\"\",\n            director_name=director_name\n        )\n\n        for record in result:\n            movie_data = dict(record[\"m\"].items())\n            movie = Movie(**movie_data)\n            movies_list.append(movie)\n\n    driver.close()\n    return movies_list\n\n# Get actors who worked with a specific actor\ndef get_co_actors(actor_name):\n    \"\"\"Get all actors who acted in the same movie as the specified actor\"\"\"\n    driver = get_driver()\n\n    co_actors = []\n\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (a:Actor {name: $actor_name})-[:ACTED_IN]-&gt;(m:Movie)&lt;-[:ACTED_IN]-(co:Actor)\n            WHERE co.name &lt;&gt; $actor_name\n            RETURN DISTINCT co\n            \"\"\",\n            actor_name=actor_name\n        )\n\n        for record in result:\n            actor_data = dict(record[\"co\"].items())\n            actor = Actor(**actor_data)\n            co_actors.append(actor)\n\n    driver.close()\n    return co_actors\n\n# Main function to demo the movie recommendation system\ndef main():\n    # Initialize and populate the database\n    initialize_database()\n    populate_database()\n\n    # Get movie recommendations for User One\n    print(\"\\nMovie recommendations for User One:\")\n    recommendations = get_movie_recommendations(\"u1\")\n    for movie in recommendations:\n        print(f\"  - {movie.title} ({movie.year}) - Rating: {movie.rating}\")\n\n    # Get movies directed by Christopher Nolan\n    print(\"\\nMovies directed by Christopher Nolan:\")\n    nolan_movies = get_movies_by_director(\"Christopher Nolan\")\n    for movie in nolan_movies:\n        print(f\"  - {movie.title} ({movie.year}) - Rating: {movie.rating}\")\n\n    # Get actors who worked with Keanu Reeves\n    print(\"\\nActors who worked with Keanu Reeves:\")\n    keanu_co_actors = get_co_actors(\"Keanu Reeves\")\n    for actor in keanu_co_actors:\n        print(f\"  - {actor.name} (Age: {actor.age})\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"neo4j_adapter/#error-handling-with-neo4j-adapter","title":"Error Handling with Neo4j Adapter","text":"<p>Let's demonstrate proper error handling for common Neo4j operations:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\nfrom pydapter.exceptions import ConnectionError, QueryError, ResourceError, ValidationError\n\n# Define a simple model\nclass Person(BaseModel):\n    id: str\n    name: str\n    age: int\n\ndef neo4j_error_handling():\n    print(\"Testing error handling for Neo4j operations...\")\n\n    # 1. Connection error - wrong authentication\n    try:\n        Neo4jAdapter.from_obj(\n            Person,\n            {\n                \"url\": \"bolt://localhost:7687\",\n                \"auth\": (\"neo4j\", \"wrong_password\"),\n                \"label\": \"Person\"\n            }\n        )\n    except ConnectionError as e:\n        print(f\"Authentication error handled: {e}\")\n\n    # 2. Connection error - wrong host\n    try:\n        Neo4jAdapter.from_obj(\n            Person,\n            {\n                \"url\": \"bolt://nonexistent-host:7687\",\n                \"auth\": (\"neo4j\", \"password\"),\n                \"label\": \"Person\"\n            }\n        )\n    except ConnectionError as e:\n        print(f\"Host connection error handled: {e}\")\n\n    # 3. Query error - Cypher syntax error\n    try:\n        # Create a valid connection but inject a syntax error\n        # Note: The adapter validates basic Cypher, but we can still get Neo4j errors\n        Neo4jAdapter.from_obj(\n            Person,\n            {\n                \"url\": \"bolt://localhost:7687\",\n                \"auth\": (\"neo4j\", \"password\"),\n                \"label\": \"Person\",\n                \"where\": \"n.age ==\" # Invalid Cypher syntax (missing value)\n            }\n        )\n    except QueryError as e:\n        print(f\"Cypher syntax error handled: {e}\")\n\n    # 4. Resource error - nonexistent label\n    try:\n        # This assumes the database is empty or this label doesn't exist\n        Neo4jAdapter.from_obj(\n            Person,\n            {\n                \"url\": \"bolt://localhost:7687\",\n                \"auth\": (\"neo4j\", \"password\"),\n                \"label\": \"NonexistentLabel\"\n            }\n        )\n    except ResourceError as e:\n        print(f\"Resource error handled: {e}\")\n\n# Run the error handling examples\nneo4j_error_handling()\n</code></pre>"},{"location":"neo4j_adapter/#using-neo4j-with-adaptable-mixin","title":"Using Neo4j with Adaptable Mixin","text":"<p>For a more ergonomic API, you can use the <code>Adaptable</code> mixin with the Neo4j adapter:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional\nfrom pydapter.core import Adaptable\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\n\n# Neo4j connection settings\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_AUTH = (\"neo4j\", \"password\")\n\n# Define a model with the Adaptable mixin\nclass Product(BaseModel, Adaptable):\n    id: str\n    name: str\n    price: float\n    category: str\n    in_stock: bool = True\n    tags: List[str] = []\n\n# Register the Neo4j adapter\nProduct.register_adapter(Neo4jAdapter)\n\ndef adaptable_mixin_demo():\n    # Create products\n    products = [\n        Product(id=\"prod1\", name=\"Laptop\", price=1299.99, category=\"Electronics\", tags=[\"computer\", \"portable\"]),\n        Product(id=\"prod2\", name=\"Smartphone\", price=899.99, category=\"Electronics\", tags=[\"mobile\", \"portable\"]),\n        Product(id=\"prod3\", name=\"Headphones\", price=199.99, category=\"Audio\", tags=[\"audio\", \"portable\"])\n    ]\n\n    # Store products using the mixin\n    print(\"Storing products using Adaptable mixin...\")\n    for product in products:\n        result = product.adapt_to(\n            obj_key=\"neo4j\",\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Product\",\n            merge_on=\"id\"\n        )\n        print(f\"Stored {product.name}: {result}\")\n\n    # Retrieve products by category\n    print(\"\\nRetrieving electronics products...\")\n    electronics = Product.adapt_from(\n        {\n            \"url\": NEO4J_URI,\n            \"auth\": NEO4J_AUTH,\n            \"label\": \"Product\",\n            \"where\": \"n.category = 'Electronics'\"\n        },\n        obj_key=\"neo4j\",\n        many=True\n    )\n\n    print(f\"Found {len(electronics)} electronics products:\")\n    for product in electronics:\n        print(f\"  - {product.name}: ${product.price}\")\n        print(f\"    Tags: {', '.join(product.tags)}\")\n\n# Run the adaptable mixin demo\nadaptable_mixin_demo()\n</code></pre>"},{"location":"neo4j_adapter/#complete-example-social-network-analysis","title":"Complete Example: Social Network Analysis","text":"<p>Let's build a more complete example that showcases Neo4j's strengths for social network analysis:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\nfrom pydapter.core import Adaptable\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\nfrom neo4j import GraphDatabase\nimport random\n\n# Neo4j connection settings\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_AUTH = (\"neo4j\", \"password\")\n\n# Define our models\nclass User(BaseModel, Adaptable):\n    id: str\n    username: str\n    full_name: Optional[str] = None\n    email: Optional[str] = None\n    location: Optional[str] = None\n    joined_date: Optional[str] = None\n\nclass Post(BaseModel, Adaptable):\n    id: str\n    content: str\n    created_at: str\n    likes: int = 0\n    user_id: str  # Author of the post\n\n# Register adapters\nUser.register_adapter(Neo4jAdapter)\nPost.register_adapter(Neo4jAdapter)\n\n# Helper function to create Neo4j driver\ndef get_driver():\n    return GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n\n# Initialize the database with schema and constraints\ndef initialize_database():\n    driver = get_driver()\n\n    with driver.session() as session:\n        # Create constraints for uniqueness\n        session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (u:User) REQUIRE u.id IS UNIQUE\")\n        session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (p:Post) REQUIRE p.id IS UNIQUE\")\n\n    driver.close()\n    print(\"Database initialized with constraints\")\n\n# Create relationships between users (follows) and between users and posts\ndef create_relationships(users, posts):\n    driver = get_driver()\n\n    with driver.session() as session:\n        # Connect users with their posts\n        print(\"\\nConnecting users with their posts...\")\n        for post in posts:\n            session.run(\n                \"\"\"\n                MATCH (u:User {id: $user_id})\n                MATCH (p:Post {id: $post_id})\n                MERGE (u)-[:POSTED]-&gt;(p)\n                \"\"\",\n                user_id=post.user_id,\n                post_id=post.id\n            )\n            print(f\"Connected user {post.user_id} with post {post.id}\")\n\n        # Create random follow relationships between users\n        print(\"\\nCreating follow relationships...\")\n        user_ids = [user.id for user in users]\n\n        for user_id in user_ids:\n            # Each user follows a random subset of other users\n            for other_id in user_ids:\n                if user_id != other_id and random.random() &lt; 0.3:  # 30% chance to follow\n                    session.run(\n                        \"\"\"\n                        MATCH (u1:User {id: $user_id})\n                        MATCH (u2:User {id: $other_id})\n                        MERGE (u1)-[:FOLLOWS]-&gt;(u2)\n                        \"\"\",\n                        user_id=user_id,\n                        other_id=other_id\n                    )\n                    print(f\"User {user_id} follows User {other_id}\")\n\n        # Create some likes on posts\n        print(\"\\nCreating likes on posts...\")\n        for user_id in user_ids:\n            for post in posts:\n                # Users don't like their own posts, and random chance to like others\n                if post.user_id != user_id and random.random() &lt; 0.4:  # 40% chance to like\n                    session.run(\n                        \"\"\"\n                        MATCH (u:User {id: $user_id})\n                        MATCH (p:Post {id: $post_id})\n                        MERGE (u)-[:LIKES]-&gt;(p)\n                        \"\"\",\n                        user_id=user_id,\n                        post_id=post.id\n                    )\n\n                    # Also update the likes count on the post\n                    session.run(\n                        \"\"\"\n                        MATCH (p:Post {id: $post_id})\n                        SET p.likes = p.likes + 1\n                        \"\"\",\n                        post_id=post.id\n                    )\n\n                    print(f\"User {user_id} likes Post {post.id}\")\n\n    driver.close()\n\n# Populate the database with users and posts\ndef populate_database():\n    # Create some users\n    users = [\n        User(\n            id=\"u1\",\n            username=\"alice_wonder\",\n            full_name=\"Alice Wonderland\",\n            email=\"alice@example.com\",\n            location=\"New York\",\n            joined_date=datetime(2022, 1, 15).isoformat()\n        ),\n        User(\n            id=\"u2\",\n            username=\"bob_builder\",\n            full_name=\"Bob Builder\",\n            email=\"bob@example.com\",\n            location=\"San Francisco\",\n            joined_date=datetime(2022, 2, 20).isoformat()\n        ),\n        User(\n            id=\"u3\",\n            username=\"charlie_brown\",\n            full_name=\"Charlie Brown\",\n            email=\"charlie@example.com\",\n            location=\"Chicago\",\n            joined_date=datetime(2022, 3, 10).isoformat()\n        ),\n        User(\n            id=\"u4\",\n            username=\"david_jones\",\n            full_name=\"David Jones\",\n            email=\"david@example.com\",\n            location=\"Miami\",\n            joined_date=datetime(2022, 4, 5).isoformat()\n        ),\n        User(\n            id=\"u5\",\n            username=\"emma_stone\",\n            full_name=\"Emma Stone\",\n            email=\"emma@example.com\",\n            location=\"Los Angeles\",\n            joined_date=datetime(2022, 5, 1).isoformat()\n        ),\n    ]\n\n    # Create some posts\n    posts = [\n        Post(\n            id=\"p1\",\n            content=\"Just learned about Neo4j and graph databases!\",\n            created_at=datetime(2023, 1, 5).isoformat(),\n            user_id=\"u1\"\n        ),\n        Post(\n            id=\"p2\",\n            content=\"Excited to start my new project with Python\",\n            created_at=datetime(2023, 1, 10).isoformat(),\n            user_id=\"u1\"\n        ),\n        Post(\n            id=\"p3\",\n            content=\"San Francisco has the best views!\",\n            created_at=datetime(2023, 1, 8).isoformat(),\n            user_id=\"u2\"\n        ),\n        Post(\n            id=\"p4\",\n            content=\"Working on a new recommendation algorithm\",\n            created_at=datetime(2023, 1, 12).isoformat(),\n            user_id=\"u3\"\n        ),\n        Post(\n            id=\"p5\",\n            content=\"Just finished reading a great book about AI\",\n            created_at=datetime(2023, 1, 15).isoformat(),\n            user_id=\"u3\"\n        ),\n        Post(\n            id=\"p6\",\n            content=\"Miami sunsets are unbeatable!\",\n            created_at=datetime(2023, 1, 14).isoformat(),\n            user_id=\"u4\"\n        ),\n        Post(\n            id=\"p7\",\n            content=\"Excited about new movie roles coming up\",\n            created_at=datetime(2023, 1, 18).isoformat(),\n            user_id=\"u5\"\n        ),\n    ]\n\n    # Store users in Neo4j\n    print(\"Storing users...\")\n    for user in users:\n        user.adapt_to(\n            obj_key=\"neo4j\",\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"User\",\n            merge_on=\"id\"\n        )\n\n    # Store posts in Neo4j\n    print(\"\\nStoring posts...\")\n    for post in posts:\n        post.adapt_to(\n            obj_key=\"neo4j\",\n            url=NEO4J_URI,\n            auth=NEO4J_AUTH,\n            label=\"Post\",\n            merge_on=\"id\"\n        )\n\n    # Create relationships\n    create_relationships(users, posts)\n\n    print(\"Database populated with sample data\")\n\n# Function to get a user's feed (posts from users they follow)\ndef get_user_feed(user_id):\n    \"\"\"Get posts from users that this user follows\"\"\"\n    driver = get_driver()\n\n    feed_posts = []\n\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (u:User {id: $user_id})-[:FOLLOWS]-&gt;(friend:User)-[:POSTED]-&gt;(p:Post)\n            RETURN p, friend.username AS author\n            ORDER BY p.created_at DESC\n            LIMIT 10\n            \"\"\",\n            user_id=user_id\n        )\n\n        for record in result:\n            post_data = dict(record[\"p\"].items())\n            post = Post(**post_data)\n            author = record[\"author\"]\n            feed_posts.append((post, author))\n\n    driver.close()\n    return feed_posts\n\n# Function to get recommended users to follow\ndef get_follow_recommendations(user_id):\n    \"\"\"Recommend users to follow based on mutual connections\"\"\"\n    driver = get_driver()\n\n    recommended_users = []\n\n    with driver.session() as session:\n        # Find users who are followed by people the user follows,\n        # but the user doesn't follow yet\n        result = session.run(\n            \"\"\"\n            MATCH (user:User {id: $user_id})-[:FOLLOWS]-&gt;(mutual:User)-[:FOLLOWS]-&gt;(recommended:User)\n            WHERE NOT (user)-[:FOLLOWS]-&gt;(recommended)\n            AND user.id &lt;&gt; recommended.id\n            WITH recommended, count(mutual) AS mutualCount\n            ORDER BY mutualCount DESC\n            LIMIT 5\n            RETURN recommended\n            \"\"\",\n            user_id=user_id\n        )\n\n        for record in result:\n            user_data = dict(record[\"recommended\"].items())\n            user = User(**user_data)\n            recommended_users.append(user)\n\n    driver.close()\n    return recommended_users\n\n# Function to get popular posts\ndef get_popular_posts():\n    \"\"\"Get posts with the most likes\"\"\"\n    driver = get_driver()\n\n    popular_posts = []\n\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (p:Post)\n            WITH p, p.likes AS likes\n            ORDER BY likes DESC\n            LIMIT 5\n            MATCH (author:User)-[:POSTED]-&gt;(p)\n            RETURN p, author.username AS author\n            \"\"\"\n        )\n\n        for record in result:\n            post_data = dict(record[\"p\"].items())\n            post = Post(**post_data)\n            author = record[\"author\"]\n            popular_posts.append((post, author))\n\n    driver.close()\n    return popular_posts\n\n# Main function to demo the social network\ndef main():\n    # Initialize and populate the database\n    initialize_database()\n    populate_database()\n\n    # Get user feed for Alice\n    print(\"\\nAlice's feed (posts from people she follows):\")\n    feed = get_user_feed(\"u1\")\n    for post, author in feed:\n        print(f\"@{author}: {post.content}\")\n        print(f\"  Likes: {post.likes} | Posted: {post.created_at}\")\n\n    # Get recommended users for Bob to follow\n    print(\"\\nRecommended users for Bob to follow:\")\n    recommendations = get_follow_recommendations(\"u2\")\n    for user in recommendations:\n        print(f\"  - {user.full_name} (@{user.username}) from {user.location}\")\n\n    # Get popular posts\n    print(\"\\nPopular posts across the network:\")\n    popular = get_popular_posts()\n    for i, (post, author) in enumerate(popular):\n        print(f\"{i+1}. @{author}: {post.content}\")\n        print(f\"   Likes: {post.likes}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"neo4j_adapter/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you've learned how to use pydapter's Neo4j adapter to seamlessly work with graph databases. We've covered:</p> <ol> <li>Basic setup and connection to Neo4j</li> <li>Modeling entities as Pydantic models</li> <li>Storing and retrieving data using the Neo4j adapter</li> <li>Creating and traversing relationships</li> <li>Building more complex graph applications</li> <li>Error handling and best practices</li> </ol> <p>Neo4j's graph structure is particularly powerful for data with complex relationships, like social networks, recommendation systems, and knowledge graphs. The pydapter adapter makes it easy to integrate Neo4j with your Pydantic-based Python applications, providing a clean interface for graph database operations.</p> <p>Some key advantages of using pydapter's Neo4j adapter include:</p> <ol> <li>Type safety and validation through Pydantic models</li> <li>Consistent error handling</li> <li>Simplified node creation and retrieval</li> <li>Integration with other pydapter adapters for multi-database applications</li> </ol> <p>Keep in mind that while the adapter handles nodes well, for relationship operations you'll often need to use the Neo4j driver directly for more complex graph traversals and Cypher queries.</p> <p>To learn more about Neo4j and graph modeling, check out the Neo4j documentation and Cypher query language.</p>"},{"location":"postgres_adapter/","title":"PostgreSQL Adapter Tutorial for Pydapter","text":"<p>This tutorial will show you how to use the PostgreSQL adapters in pydapter to seamlessly convert between Pydantic models and PostgreSQL databases. We'll cover both synchronous and asynchronous adapters.</p>"},{"location":"postgres_adapter/#prerequisites","title":"Prerequisites","text":""},{"location":"postgres_adapter/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code># Create a virtual environment if you haven't already\npython -m venv pydapter-demo\nsource pydapter-demo/bin/activate  # On Windows: pydapter-demo\\Scripts\\activate\n\n# Install dependencies\npip install pydantic sqlalchemy psycopg  # For synchronous adapter\npip install asyncpg  # For asynchronous adapter\n\n# Install pydapter (if you haven't done so already)\n# Either from PyPI when available:\n# pip install pydapter\n# Or from the repository:\ngit clone https://github.com/ohdearquant/pydapter.git\ncd pydapter\npip install -e .\n</code></pre>"},{"location":"postgres_adapter/#2-set-up-postgresql","title":"2. Set Up PostgreSQL","text":"<p>Make sure you have PostgreSQL installed and running. You can use a local installation or a Docker container:</p> <pre><code># Using Docker to run PostgreSQL\ndocker run --name pydapter-postgres -e POSTGRES_PASSWORD=password \\\n  -e POSTGRES_USER=pydapter -e POSTGRES_DB=pydapter_demo \\\n  -p 5432:5432 -d postgres:14\n\n# Alternatively, install PostgreSQL locally and create a database\n# createuser -s pydapter\n# createdb -O pydapter pydapter_demo\n</code></pre>"},{"location":"postgres_adapter/#3-create-a-test-table","title":"3. Create a Test Table","text":"<p>Connect to your PostgreSQL instance and create a test table:</p> <pre><code>CREATE TABLE IF NOT EXISTS users (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    email VARCHAR(100) UNIQUE NOT NULL,\n    active BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Optional: Add some test data\nINSERT INTO users (name, email) VALUES\n    ('Alice', 'alice@example.com'),\n    ('Bob', 'bob@example.com'),\n    ('Charlie', 'charlie@example.com');\n</code></pre>"},{"location":"postgres_adapter/#synchronous-postgresql-adapter","title":"Synchronous PostgreSQL Adapter","text":"<p>Let's start with the synchronous PostgreSQL adapter:</p> <pre><code>from pydantic import BaseModel\nfrom datetime import datetime\nfrom typing import Optional, List\nfrom pydapter.extras.postgres_ import PostgresAdapter\n\n# Define a Pydantic model that maps to our database table\nclass User(BaseModel):\n    id: Optional[int] = None\n    name: str\n    email: str\n    active: bool = True\n    created_at: Optional[datetime] = None\n\n# Connection details\ndb_config = {\n    \"engine_url\": \"postgresql+psycopg://pydapter:password@localhost/pydapter_demo\"\n    # You can also use:\n    # \"engine_url\": \"postgresql://pydapter:password@localhost/pydapter_demo\"\n    # Pydapter will convert it to the correct format\n}\n\n# Read data from the database\ndef read_users():\n    # Query all users\n    users = PostgresAdapter.from_obj(\n        User,\n        {\n            **db_config,\n            \"table\": \"users\",\n            \"selectors\": {}  # Empty selectors means \"select all\"\n        },\n        many=True  # Return a list of users\n    )\n\n    print(f\"Found {len(users)} users:\")\n    for user in users:\n        print(f\"  - {user.name} ({user.email}): Active={user.active}\")\n\n    return users\n\n# Query a specific user\ndef get_user_by_email(email):\n    try:\n        user = PostgresAdapter.from_obj(\n            User,\n            {\n                **db_config,\n                \"table\": \"users\",\n                \"selectors\": {\"email\": email}\n            },\n            many=False  # Return a single user\n        )\n        print(f\"Found user: {user.name} ({user.email})\")\n        return user\n    except Exception as e:\n        print(f\"Error finding user: {e}\")\n        return None\n\n# Create a new user\ndef create_user(name, email):\n    user = User(name=name, email=email)\n\n    result = PostgresAdapter.to_obj(\n        user,\n        **db_config,\n        table=\"users\",\n        many=False\n    )\n\n    print(f\"Created user: {result}\")\n    return user\n\n# Main function to demo the adapter\ndef main():\n    print(\"Reading all users:\")\n    users = read_users()\n\n    print(\"\\nFinding user by email:\")\n    alice = get_user_by_email(\"alice@example.com\")\n\n    print(\"\\nCreating a new user:\")\n    new_user = create_user(\"Dave\", \"dave@example.com\")\n\n    print(\"\\nVerifying new user was added:\")\n    read_users()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"postgres_adapter/#asynchronous-postgresql-adapter","title":"Asynchronous PostgreSQL Adapter","text":"<p>Now let's use the asynchronous version with asyncpg:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom datetime import datetime\nfrom typing import Optional, List\nfrom pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\n# Define a Pydantic model that maps to our database table\nclass User(BaseModel):\n    id: Optional[int] = None\n    name: str\n    email: str\n    active: bool = True\n    created_at: Optional[datetime] = None\n\n# Connection details\ndb_config = {\n    \"engine_url\": \"postgresql+asyncpg://pydapter:password@localhost/pydapter_demo\"\n    # You can also use:\n    # \"engine_url\": \"postgresql://pydapter:password@localhost/pydapter_demo\"\n    # Pydapter will convert it to the correct format with asyncpg\n}\n\n# Read data from the database asynchronously\nasync def read_users():\n    # Query all users\n    users = await AsyncPostgresAdapter.from_obj(\n        User,\n        {\n            **db_config,\n            \"table\": \"users\",\n            \"selectors\": {}  # Empty selectors means \"select all\"\n        },\n        many=True  # Return a list of users\n    )\n\n    print(f\"Found {len(users)} users:\")\n    for user in users:\n        print(f\"  - {user.name} ({user.email}): Active={user.active}\")\n\n    return users\n\n# Query a specific user asynchronously\nasync def get_user_by_email(email):\n    try:\n        user = await AsyncPostgresAdapter.from_obj(\n            User,\n            {\n                **db_config,\n                \"table\": \"users\",\n                \"selectors\": {\"email\": email}\n            },\n            many=False  # Return a single user\n        )\n        print(f\"Found user: {user.name} ({user.email})\")\n        return user\n    except Exception as e:\n        print(f\"Error finding user: {e}\")\n        return None\n\n# Create a new user asynchronously\nasync def create_user(name, email):\n    user = User(name=name, email=email)\n\n    result = await AsyncPostgresAdapter.to_obj(\n        user,\n        **db_config,\n        table=\"users\",\n        many=False\n    )\n\n    print(f\"Created user: {result}\")\n    return user\n\n# Main function to demo the async adapter\nasync def main():\n    print(\"Reading all users:\")\n    users = await read_users()\n\n    print(\"\\nFinding user by email:\")\n    alice = await get_user_by_email(\"alice@example.com\")\n\n    print(\"\\nCreating a new user:\")\n    new_user = await create_user(\"Eve\", \"eve@example.com\")\n\n    print(\"\\nVerifying new user was added:\")\n    await read_users()\n\n# Run the async main function\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"postgres_adapter/#advanced-usage-using-adaptable-mixin","title":"Advanced Usage: Using Adaptable Mixin","text":"<p>For a more ergonomic API, you can use the <code>AsyncAdaptable</code> mixin:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom datetime import datetime\nfrom typing import Optional, List\nfrom pydapter.async_core import AsyncAdaptable\nfrom pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\n# Define a model with the AsyncAdaptable mixin\nclass User(BaseModel, AsyncAdaptable):\n    id: Optional[int] = None\n    name: str\n    email: str\n    active: bool = True\n    created_at: Optional[datetime] = None\n\n# Register the async PostgreSQL adapter\nUser.register_async_adapter(AsyncPostgresAdapter)\n\n# Main async function\nasync def main():\n    # Connection configuration\n    db_config = {\n        \"engine_url\": \"postgresql+asyncpg://pydapter:password@localhost/pydapter_demo\",\n        \"table\": \"users\",\n        \"selectors\": {}\n    }\n\n    # Read users using the mixin methods\n    users = await User.adapt_from_async(db_config, obj_key=\"async_pg\", many=True)\n\n    print(f\"Found {len(users)} users:\")\n    for user in users:\n        print(f\"  - {user.name} ({user.email})\")\n\n    # Create a new user\n    new_user = User(name=\"Frank\", email=\"frank@example.com\")\n\n    # Save to database\n    result = await new_user.adapt_to_async(\n        obj_key=\"async_pg\",\n        engine_url=\"postgresql+asyncpg://pydapter:password@localhost/pydapter_demo\",\n        table=\"users\"\n    )\n\n    print(f\"\\nCreated new user: {result}\")\n\n    # Verify the user was added\n    updated_users = await User.adapt_from_async(db_config, obj_key=\"async_pg\", many=True)\n    print(f\"\\nUpdated user count: {len(updated_users)}\")\n\n# Run the async main function\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"postgres_adapter/#error-handling","title":"Error Handling","text":"<p>Let's demonstrate proper error handling for common PostgreSQL errors:</p> <pre><code>from pydapter.exceptions import ConnectionError, QueryError, ResourceError\nfrom pydapter.extras.postgres_ import PostgresAdapter\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    id: Optional[int] = None\n    name: str\n    email: str\n\ndef handle_postgres_errors():\n    # 1. Connection error - wrong password\n    try:\n        PostgresAdapter.from_obj(\n            User,\n            {\n                \"engine_url\": \"postgresql://pydapter:wrong_password@localhost/pydapter_demo\",\n                \"table\": \"users\"\n            }\n        )\n    except ConnectionError as e:\n        print(f\"Authentication error handled: {e}\")\n\n    # 2. Connection error - wrong host\n    try:\n        PostgresAdapter.from_obj(\n            User,\n            {\n                \"engine_url\": \"postgresql://pydapter:password@nonexistent_host/pydapter_demo\",\n                \"table\": \"users\"\n            }\n        )\n    except ConnectionError as e:\n        print(f\"Host connection error handled: {e}\")\n\n    # 3. Resource error - table doesn't exist\n    try:\n        PostgresAdapter.from_obj(\n            User,\n            {\n                \"engine_url\": \"postgresql://pydapter:password@localhost/pydapter_demo\",\n                \"table\": \"nonexistent_table\"\n            }\n        )\n    except ResourceError as e:\n        print(f\"Table resource error handled: {e}\")\n\n    # 4. Query error - SQL syntax error\n    try:\n        # This would normally be handled internally, but for demonstration\n        # you might encounter this when using raw SQL\n        pass\n    except QueryError as e:\n        print(f\"Query error handled: {e}\")\n\n# Run the error handling examples\nhandle_postgres_errors()\n</code></pre>"},{"location":"postgres_adapter/#practical-example-a-task-management-system","title":"Practical Example: A Task Management System","text":"<p>Let's create a more complete example of a task management system:</p> <pre><code>import asyncio\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom pydapter.async_core import AsyncAdaptable\nfrom pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\n# Database setup - Run this SQL in your PostgreSQL database first\n\"\"\"\nCREATE TABLE IF NOT EXISTS projects (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE IF NOT EXISTS tasks (\n    id SERIAL PRIMARY KEY,\n    project_id INTEGER REFERENCES projects(id),\n    title VARCHAR(200) NOT NULL,\n    description TEXT,\n    status VARCHAR(20) DEFAULT 'pending',\n    due_date TIMESTAMP,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\"\"\"\n\n# Define models with AsyncAdaptable\nclass Project(BaseModel, AsyncAdaptable):\n    id: Optional[int] = None\n    name: str\n    description: Optional[str] = None\n    created_at: Optional[datetime] = None\n\nclass Task(BaseModel, AsyncAdaptable):\n    id: Optional[int] = None\n    project_id: int\n    title: str\n    description: Optional[str] = None\n    status: str = \"pending\"\n    due_date: Optional[datetime] = None\n    created_at: Optional[datetime] = None\n\n# Register adapters\nProject.register_async_adapter(AsyncPostgresAdapter)\nTask.register_async_adapter(AsyncPostgresAdapter)\n\n# Database configuration\nDB_CONFIG = {\n    \"engine_url\": \"postgresql+asyncpg://pydapter:password@localhost/pydapter_demo\"\n}\n\n# Task management system class\nclass TaskManager:\n    def __init__(self, db_config):\n        self.db_config = db_config\n\n    async def create_project(self, name, description=None):\n        project = Project(name=name, description=description)\n        result = await project.adapt_to_async(\n            obj_key=\"async_pg\",\n            **self.db_config,\n            table=\"projects\"\n        )\n\n        # Get the new project with its ID\n        projects = await Project.adapt_from_async(\n            {\n                **self.db_config,\n                \"table\": \"projects\",\n                \"selectors\": {\"name\": name}\n            },\n            obj_key=\"async_pg\",\n            many=True\n        )\n\n        if projects:\n            return projects[0]\n        return None\n\n    async def get_projects(self):\n        return await Project.adapt_from_async(\n            {\n                **self.db_config,\n                \"table\": \"projects\"\n            },\n            obj_key=\"async_pg\",\n            many=True\n        )\n\n    async def create_task(self, project_id, title, description=None, due_date=None):\n        task = Task(\n            project_id=project_id,\n            title=title,\n            description=description,\n            due_date=due_date\n        )\n\n        result = await task.adapt_to_async(\n            obj_key=\"async_pg\",\n            **self.db_config,\n            table=\"tasks\"\n        )\n\n        return task\n\n    async def get_tasks_for_project(self, project_id):\n        return await Task.adapt_from_async(\n            {\n                **self.db_config,\n                \"table\": \"tasks\",\n                \"selectors\": {\"project_id\": project_id}\n            },\n            obj_key=\"async_pg\",\n            many=True\n        )\n\n    async def update_task_status(self, task_id, new_status):\n        # First, get the task\n        task = await Task.adapt_from_async(\n            {\n                **self.db_config,\n                \"table\": \"tasks\",\n                \"selectors\": {\"id\": task_id}\n            },\n            obj_key=\"async_pg\",\n            many=False\n        )\n\n        # Update the status\n        task.status = new_status\n\n        # Save back to database\n        result = await task.adapt_to_async(\n            obj_key=\"async_pg\",\n            **self.db_config,\n            table=\"tasks\"\n        )\n\n        return task\n\n# Main function to demo the task manager\nasync def main():\n    manager = TaskManager(DB_CONFIG)\n\n    # Create a new project\n    print(\"Creating a new project...\")\n    project = await manager.create_project(\n        \"Website Redesign\",\n        \"Redesign the company website with modern UI/UX\"\n    )\n    print(f\"Project created: {project.id} - {project.name}\")\n\n    # Add tasks to the project\n    print(\"\\nAdding tasks to the project...\")\n    tasks = [\n        await manager.create_task(\n            project.id,\n            \"Design mockups\",\n            \"Create initial design mockups for homepage\",\n            datetime.now().replace(day=datetime.now().day + 7)\n        ),\n        await manager.create_task(\n            project.id,\n            \"Frontend implementation\",\n            \"Implement the design in React\",\n            datetime.now().replace(day=datetime.now().day + 14)\n        ),\n        await manager.create_task(\n            project.id,\n            \"Backend API\",\n            \"Implement the required API endpoints\",\n            datetime.now().replace(day=datetime.now().day + 10)\n        )\n    ]\n\n    # Get all projects\n    print(\"\\nListing all projects:\")\n    projects = await manager.get_projects()\n    for proj in projects:\n        print(f\"  - {proj.id}: {proj.name}\")\n\n        # Get tasks for this project\n        proj_tasks = await manager.get_tasks_for_project(proj.id)\n        for task in proj_tasks:\n            print(f\"      - {task.title} [{task.status}] \" +\n                  (f\"(Due: {task.due_date.strftime('%Y-%m-%d')})\" if task.due_date else \"\"))\n\n    # Update a task status\n    print(\"\\nUpdating task status...\")\n    updated_task = await manager.update_task_status(tasks[0].id, \"in_progress\")\n    print(f\"Updated task: {updated_task.title} - Status: {updated_task.status}\")\n\n    # Final task list\n    print(\"\\nFinal task list:\")\n    final_tasks = await manager.get_tasks_for_project(project.id)\n    for task in final_tasks:\n        print(f\"  - {task.title} [{task.status}]\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"postgres_adapter/#handling-advanced-postgresql-features","title":"Handling Advanced PostgreSQL Features","text":"<p>PostgreSQL has many advanced features like JSON/JSONB fields, arrays, and full-text search. Here's how to work with some of these using pydapter:</p> <pre><code>import asyncio\nimport json\nfrom typing import List, Dict, Any, Optional\nfrom pydantic import BaseModel, Field\nfrom pydapter.async_core import AsyncAdaptable\nfrom pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\n# Database setup - Run this SQL in your PostgreSQL database first\n\"\"\"\nCREATE TABLE IF NOT EXISTS products (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    description TEXT,\n    price NUMERIC(10, 2) NOT NULL,\n    categories TEXT[] DEFAULT '{}',\n    metadata JSONB DEFAULT '{}'\n);\n\"\"\"\n\n# Define our model with PostgreSQL-specific types\nclass Product(BaseModel, AsyncAdaptable):\n    id: Optional[int] = None\n    name: str\n    description: Optional[str] = None\n    price: float\n    categories: List[str] = []\n    metadata: Dict[str, Any] = {}\n\n# Register adapter\nProduct.register_async_adapter(AsyncPostgresAdapter)\n\n# Database config\nDB_CONFIG = {\n    \"engine_url\": \"postgresql+asyncpg://pydapter:password@localhost/pydapter_demo\"\n}\n\nasync def demo_advanced_postgres_features():\n    # Create products with array and JSON data\n    products = [\n        Product(\n            name=\"Smartphone\",\n            description=\"Latest model with advanced features\",\n            price=999.99,\n            categories=[\"electronics\", \"mobile\", \"gadgets\"],\n            metadata={\n                \"brand\": \"TechX\",\n                \"model\": \"TX-2000\",\n                \"specs\": {\n                    \"cpu\": \"Octa-core\",\n                    \"ram\": \"8GB\",\n                    \"storage\": \"256GB\"\n                }\n            }\n        ),\n        Product(\n            name=\"Coffee Maker\",\n            description=\"Premium coffee machine for home or office\",\n            price=199.99,\n            categories=[\"appliances\", \"kitchen\", \"coffee\"],\n            metadata={\n                \"brand\": \"BrewMaster\",\n                \"features\": [\"programmable\", \"thermal carafe\", \"auto-clean\"],\n                \"dimensions\": {\n                    \"width\": 30,\n                    \"height\": 40,\n                    \"depth\": 20\n                }\n            }\n        )\n    ]\n\n    # Save products to database\n    print(\"Saving products with arrays and JSON data...\")\n    for product in products:\n        await product.adapt_to_async(\n            obj_key=\"async_pg\",\n            **DB_CONFIG,\n            table=\"products\"\n        )\n\n    # Query all products\n    print(\"\\nRetrieving products from database:\")\n    db_products = await Product.adapt_from_async(\n        {\n            **DB_CONFIG,\n            \"table\": \"products\"\n        },\n        obj_key=\"async_pg\",\n        many=True\n    )\n\n    # Display products with their array and JSON data\n    for product in db_products:\n        print(f\"\\n{product.name} - ${product.price}\")\n        print(f\"  Description: {product.description}\")\n        print(f\"  Categories: {', '.join(product.categories)}\")\n        print(f\"  Metadata: {json.dumps(product.metadata, indent=2)}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(demo_advanced_postgres_features())\n</code></pre>"},{"location":"postgres_adapter/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you learned how to use pydapter's PostgreSQL adapters to seamlessly work with both synchronous and asynchronous database operations. These adapters provide a clean interface for converting between Pydantic models and PostgreSQL database records, with specialized error handling for PostgreSQL-specific issues.</p> <p>The asynchronous adapter is particularly useful for high-performance applications where you want to avoid blocking I/O operations. By using the <code>AsyncAdaptable</code> mixin, you can create a more ergonomic API that makes your code cleaner and more maintainable.</p> <p>The key advantages of using pydapter's PostgreSQL adapters include:</p> <ol> <li>Automatic validation through Pydantic models</li> <li>Consistent error handling</li> <li>Support for both synchronous and asynchronous operations</li> <li>Easy conversion between models and database records</li> <li>Support for PostgreSQL-specific data types like arrays and JSONB</li> </ol> <p>Try experimenting with these adapters in your own projects to see how they can simplify your database interactions!</p>"},{"location":"protocols/","title":"Protocols Module","text":"<p>The Protocols module provides a set of standardized interfaces that can be used to add common capabilities to your models. These protocols follow a clean inheritance hierarchy and are designed to be composable, allowing you to mix and match capabilities as needed.</p>"},{"location":"protocols/#installation","title":"Installation","text":"<p>The Protocols module is available as an optional dependency. To use it, install pydapter with the <code>protocols</code> extra:</p> <pre><code>pip install pydapter[protocols]\n</code></pre> <p>This will install the required dependencies, including <code>typing-extensions</code>.</p>"},{"location":"protocols/#available-protocols","title":"Available Protocols","text":"<p>The Protocols module provides the following interfaces:</p>"},{"location":"protocols/#identifiable","title":"Identifiable","text":"<p>The <code>Identifiable</code> protocol provides a unique identifier for objects. It's the foundation of the protocol hierarchy.</p> <p>Key features:</p> <ul> <li>Automatic UUID generation</li> <li>String serialization of UUIDs</li> <li>UUID validation</li> <li>Hash implementation for use in sets and dictionaries</li> </ul> <pre><code>from pydapter.protocols import Identifiable\n\nclass User(Identifiable):\n    name: str\n    email: str\n\n# UUID is automatically generated\nuser = User(name=\"John Doe\", email=\"john@example.com\")\nprint(f\"User ID: {user.id}\")  # User ID: 3f7c8e9a-1d2b-4c3d-8e7f-5a6b7c8d9e0f\n</code></pre>"},{"location":"protocols/#temporal","title":"Temporal","text":"<p>The <code>Temporal</code> protocol adds creation and update timestamps to objects.</p> <p>Key features:</p> <ul> <li>Automatic creation timestamp</li> <li>Automatic update timestamp</li> <li>Method to manually update the timestamp</li> <li>ISO-8601 serialization of timestamps</li> </ul> <pre><code>from pydapter.protocols import Identifiable, Temporal\n\nclass User(Identifiable, Temporal):\n    name: str\n    email: str\n\n# Timestamps are automatically set\nuser = User(name=\"John Doe\", email=\"john@example.com\")\nprint(f\"Created at: {user.created_at}\")  # Created at: 2025-05-16T15:30:00+00:00\nprint(f\"Updated at: {user.updated_at}\")  # Updated at: 2025-05-16T15:30:00+00:00\n\n# Update the timestamp manually\nuser.name = \"Jane Doe\"\nuser.update_timestamp()\nprint(f\"Updated at: {user.updated_at}\")  # Updated at: 2025-05-16T15:31:00+00:00\n</code></pre>"},{"location":"protocols/#embedable","title":"Embedable","text":"<p>The <code>Embedable</code> protocol adds support for vector embeddings, which are commonly used in machine learning and natural language processing applications.</p> <p>Key features:</p> <ul> <li>Storage for embedding vectors</li> <li>Content field for the text to be embedded</li> <li>Dimension calculation</li> <li>Support for various embedding formats (list, JSON string)</li> </ul> <pre><code>from pydapter.protocols import Identifiable, Temporal, Embedable\n\nclass Document(Identifiable, Temporal, Embedable):\n    title: str\n\n# Create a document with an embedding\ndocument = Document(\n    title=\"Sample Document\",\n    content=\"This is a sample document for embedding.\",\n    embedding=[0.1, 0.2, 0.3, 0.4]\n)\n\nprint(f\"Embedding dimensions: {document.n_dim}\")  # Embedding dimensions: 4\n\n# Embeddings can also be provided as a JSON string\ndocument2 = Document(\n    title=\"Another Document\",\n    content=\"This is another sample document.\",\n    embedding=\"[0.5, 0.6, 0.7, 0.8]\"\n)\n</code></pre>"},{"location":"protocols/#invokable","title":"Invokable","text":"<p>The <code>Invokable</code> protocol adds function invocation capabilities with execution tracking.</p> <p>Key features:</p> <ul> <li>Execution status tracking</li> <li>Duration measurement</li> <li>Error handling</li> <li>Response storage</li> </ul> <pre><code>import asyncio\nfrom pydapter.protocols import Identifiable, Temporal, Invokable\n\nclass APICall(Identifiable, Temporal, Invokable):\n    endpoint: str\n\n    async def fetch_data(self):\n        # Simulate API call\n        await asyncio.sleep(1)\n        return {\"data\": \"Sample response\"}\n\n# Create an API call\napi_call = APICall(endpoint=\"/api/data\")\napi_call._invoke_function = api_call.fetch_data\n\n# Execute the call\nawait api_call.invoke()\n\nprint(f\"Status: {api_call.execution.status}\")  # Status: completed\nprint(f\"Duration: {api_call.execution.duration:.2f}s\")  # Duration: 1.00s\nprint(f\"Response: {api_call.execution.response}\")  # Response: {'data': 'Sample response'}\n</code></pre>"},{"location":"protocols/#event","title":"Event","text":"<p>The <code>Event</code> protocol combines the capabilities of <code>Identifiable</code>, <code>Temporal</code>, <code>Embedable</code>, and <code>Invokable</code> to provide a comprehensive event tracking interface.</p> <pre><code>from pydapter.protocols import Event\n\nclass LogEvent(Event):\n    event_type: str\n\n    async def process(self):\n        # Process the event\n        return {\"processed\": True}\n\n# Create an event\nlog_event = LogEvent(\n    event_type=\"system_log\",\n    content=\"User logged in\",\n)\nlog_event._invoke_function = log_event.process\n\n# Execute the event\nawait log_event.invoke()\n\nprint(f\"Event ID: {log_event.id}\")\nprint(f\"Created at: {log_event.created_at}\")\nprint(f\"Status: {log_event.execution.status}\")\n</code></pre>"},{"location":"protocols/#protocol-inheritance-hierarchy","title":"Protocol Inheritance Hierarchy","text":"<p>The protocols follow a hierarchical structure:</p> <pre><code>Identifiable\n    \u2502\n    \u251c\u2500\u2500 Temporal\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u2500 Embedable\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u2500 Invokable\n    \u2502               \u2502\n    \u2502               \u2514\u2500\u2500 Event\n    \u2502\n    \u2514\u2500\u2500 Other custom protocols...\n</code></pre> <p>This design allows you to compose protocols as needed, inheriting only the capabilities required for your specific use case.</p>"},{"location":"protocols/#best-practices","title":"Best Practices","text":""},{"location":"protocols/#composing-protocols","title":"Composing Protocols","text":"<p>When using multiple protocols, inherit them in the correct order to ensure proper initialization:</p> <pre><code># Correct order\nclass MyModel(Identifiable, Temporal, Embedable):\n    pass\n\n# Incorrect order - may cause initialization issues\nclass MyModel(Embedable, Temporal, Identifiable):\n    pass\n</code></pre>"},{"location":"protocols/#custom-content-creation","title":"Custom Content Creation","text":"<p>The <code>Embedable</code> protocol allows you to customize how content is created by overriding the <code>create_content</code> method:</p> <pre><code>class Document(Identifiable, Temporal, Embedable):\n    title: str\n    body: str\n\n    def create_content(self):\n        return f\"{self.title}\\n\\n{self.body}\"\n</code></pre>"},{"location":"protocols/#custom-invocation-functions","title":"Custom Invocation Functions","text":"<p>When using the <code>Invokable</code> protocol, you need to set the <code>_invoke_function</code> attribute to the function you want to invoke:</p> <pre><code>async def fetch_data(endpoint):\n    # Fetch data from endpoint\n    return {\"data\": \"Sample response\"}\n\napi_call = APICall(endpoint=\"/api/data\")\napi_call._invoke_function = fetch_data\napi_call._invoke_args = [api_call.endpoint]  # Arguments to pass to the function\n</code></pre>"},{"location":"protocols/#type-checking","title":"Type Checking","text":"<p>The protocols module is designed to work well with static type checkers like mypy. The protocols are defined using <code>typing_extensions.Protocol</code> and are marked as <code>runtime_checkable</code>, allowing for both static and runtime type checking.</p> <pre><code>from typing import List\nfrom pydapter.protocols import Identifiable\n\ndef process_identifiables(items: List[Identifiable]):\n    for item in items:\n        print(f\"Processing item {item.id}\")\n\n# This will pass type checking\nprocess_identifiables([User(name=\"John\"), Document(title=\"Sample\")])\n</code></pre>"},{"location":"protocols/#error-handling","title":"Error Handling","text":"<p>If you try to import protocols without the required dependencies, you'll get a clear error message:</p> <pre><code>ImportError: The 'protocols' feature requires the 'typing_extensions' package. Install it with: pip install pydapter[protocols]\n</code></pre> <p>This helps guide users to install the correct dependencies.</p>"},{"location":"protocols/#advanced-usage","title":"Advanced Usage","text":""},{"location":"protocols/#custom-protocol-extensions","title":"Custom Protocol Extensions","text":"<p>You can create your own protocols by extending the existing ones:</p> <pre><code>from pydapter.protocols import Identifiable, Temporal\nfrom pydantic import Field\n\nclass Versionable(Temporal):\n    \"\"\"Protocol for objects that support versioning.\"\"\"\n\n    version: int = Field(default=1)\n\n    def increment_version(self):\n        \"\"\"Increment the version and update the timestamp.\"\"\"\n        self.version += 1\n        self.update_timestamp()\n\nclass Document(Identifiable, Temporal, Versionable):\n    title: str\n    content: str\n</code></pre>"},{"location":"protocols/#integration-with-adapters","title":"Integration with Adapters","text":"<p>The protocols can be used with pydapter adapters to provide standardized interfaces for data access:</p> <pre><code>from pydapter.core import Adapter\nfrom pydapter.protocols import Identifiable, Temporal\n\nclass User(Identifiable, Temporal):\n    name: str\n    email: str\n\n# Create an adapter for a list of users\nusers = [\n    User(name=\"John Doe\", email=\"john@example.com\"),\n    User(name=\"Jane Doe\", email=\"jane@example.com\")\n]\nadapter = Adapter(users)\n\n# Query by ID\nuser = adapter.get(id=\"3f7c8e9a-1d2b-4c3d-8e7f-5a6b7c8d9e0f\")\n</code></pre>"},{"location":"qdrant_adapter/","title":"Vector Database Tutorial with Pydapter's Qdrant Adapters","text":"<p>This tutorial demonstrates how to use pydapter's Qdrant adapters to seamlessly work with vector embeddings for semantic search and similarity-based retrieval. We'll cover both synchronous and asynchronous implementations.</p>"},{"location":"qdrant_adapter/#introduction-to-vector-databases","title":"Introduction to Vector Databases","text":"<p>Vector databases are specialized storage systems designed for high-dimensional vector data (embeddings) that enable efficient similarity search. They're crucial for:</p> <ul> <li>Semantic search</li> <li>Recommendation systems</li> <li>Image similarity</li> <li>Document retrieval</li> <li>Natural language understanding</li> </ul> <p>Qdrant is a powerful vector database with extensive filtering capabilities, making it perfect for applications that need both semantic similarity and metadata filtering.</p>"},{"location":"qdrant_adapter/#prerequisites","title":"Prerequisites","text":""},{"location":"qdrant_adapter/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code># Create a virtual environment if you haven't already\npython -m venv pydapter-demo\nsource pydapter-demo/bin/activate  # On Windows: pydapter-demo\\Scripts\\activate\n\n# Install dependencies\npip install pydantic qdrant-client sentence-transformers numpy\n\n# Install pydapter (if you haven't done so already)\n# Either from PyPI when available:\n# pip install pydapter\n# Or from the repository:\ngit clone https://github.com/ohdearquant/pydapter.git\ncd pydapter\npip install -e .\n</code></pre>"},{"location":"qdrant_adapter/#2-set-up-qdrant","title":"2. Set Up Qdrant","text":"<p>You can run Qdrant locally using Docker:</p> <pre><code>docker run -p 6333:6333 -p 6334:6334 -v $(pwd)/qdrant_data:/qdrant/storage qdrant/qdrant\n</code></pre> <p>For testing, you can also use the in-memory mode without Docker.</p>"},{"location":"qdrant_adapter/#basic-example-synchronous-qdrant-adapter","title":"Basic Example: Synchronous Qdrant Adapter","text":"<p>Let's start by creating a document search system using the synchronous adapter:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.extras.qdrant_ import QdrantAdapter\n\n# Load a sentence transformer model to generate embeddings\nmodel = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dimensional embeddings\n\n# Define our document model with vector embeddings\nclass Document(BaseModel):\n    id: str\n    title: str\n    content: str\n    tags: List[str] = []\n    embedding: List[float] = []  # Vector embedding\n\n    def generate_embedding(self):\n        \"\"\"Generate embedding from the document content\"\"\"\n        self.embedding = model.encode(self.content).tolist()\n        return self\n\n# Create sample documents\nsample_docs = [\n    Document(\n        id=\"doc1\",\n        title=\"Introduction to Machine Learning\",\n        content=\"Machine learning is a field of artificial intelligence that uses \"\n                \"statistical techniques to give computer systems the ability to learn from data.\",\n        tags=[\"ML\", \"AI\", \"Data Science\"]\n    ),\n    Document(\n        id=\"doc2\",\n        title=\"Deep Learning Fundamentals\",\n        content=\"Deep learning is a subset of machine learning that uses neural networks \"\n                \"with many layers to analyze various factors of data.\",\n        tags=[\"Deep Learning\", \"Neural Networks\", \"AI\"]\n    ),\n    Document(\n        id=\"doc3\",\n        title=\"Natural Language Processing\",\n        content=\"NLP combines computational linguistics and AI to enable computers to \"\n                \"understand, interpret, and generate human language.\",\n        tags=[\"NLP\", \"AI\", \"Linguistics\"]\n    ),\n    Document(\n        id=\"doc4\",\n        title=\"Computer Vision\",\n        content=\"Computer vision is a field of AI that trains computers to interpret and \"\n                \"understand visual data from the world around us.\",\n        tags=[\"Computer Vision\", \"AI\", \"Image Processing\"]\n    ),\n]\n\n# Generate embeddings for each document\nfor doc in sample_docs:\n    doc.generate_embedding()\n\n# Store documents in Qdrant\ndef store_documents(documents):\n    print(f\"Storing {len(documents)} documents in Qdrant...\")\n\n    # Store in Qdrant using the QdrantAdapter\n    result = QdrantAdapter.to_obj(\n        documents,\n        collection=\"documents\",  # Collection name\n        url=None,  # Use in-memory storage for this example\n        many=True\n    )\n\n    print(f\"Storage result: {result}\")\n\n# Search for similar documents\ndef search_documents(query_text, top_k=2):\n    print(f\"Searching for documents similar to: '{query_text}'\")\n\n    # Generate embedding for the query\n    query_embedding = model.encode(query_text).tolist()\n\n    # Search in Qdrant using the QdrantAdapter\n    results = QdrantAdapter.from_obj(\n        Document,\n        {\n            \"collection\": \"documents\",\n            \"query_vector\": query_embedding,\n            \"top_k\": top_k,\n            \"url\": None  # Use in-memory storage\n        },\n        many=True\n    )\n\n    print(f\"Found {len(results)} similar documents:\")\n    for i, doc in enumerate(results):\n        print(f\"{i+1}. {doc.title}\")\n        print(f\"   Content: {doc.content}\")\n        print(f\"   Tags: {', '.join(doc.tags)}\")\n        print()\n\n    return results\n\n# Main function to demo the adapter\ndef main():\n    # Store documents\n    store_documents(sample_docs)\n\n    # Perform searches\n    search_documents(\"What is machine learning?\")\n    search_documents(\"How do computers understand language?\")\n    search_documents(\"How do computers process images?\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"qdrant_adapter/#asynchronous-qdrant-adapter","title":"Asynchronous Qdrant Adapter","text":"<p>Now let's implement the same functionality using the asynchronous adapter:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n# Load the sentence transformer model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Define our document model\nclass Document(BaseModel):\n    id: str\n    title: str\n    content: str\n    tags: List[str] = []\n    embedding: List[float] = []\n\n    def generate_embedding(self):\n        self.embedding = model.encode(self.content).tolist()\n        return self\n\n# Create sample documents (same as the synchronous example)\nsample_docs = [\n    Document(\n        id=\"doc1\",\n        title=\"Introduction to Machine Learning\",\n        content=\"Machine learning is a field of artificial intelligence that uses \"\n                \"statistical techniques to give computer systems the ability to learn from data.\",\n        tags=[\"ML\", \"AI\", \"Data Science\"]\n    ),\n    Document(\n        id=\"doc2\",\n        title=\"Deep Learning Fundamentals\",\n        content=\"Deep learning is a subset of machine learning that uses neural networks \"\n                \"with many layers to analyze various factors of data.\",\n        tags=[\"Deep Learning\", \"Neural Networks\", \"AI\"]\n    ),\n    Document(\n        id=\"doc3\",\n        title=\"Natural Language Processing\",\n        content=\"NLP combines computational linguistics and AI to enable computers to \"\n                \"understand, interpret, and generate human language.\",\n        tags=[\"NLP\", \"AI\", \"Linguistics\"]\n    ),\n    Document(\n        id=\"doc4\",\n        title=\"Computer Vision\",\n        content=\"Computer vision is a field of AI that trains computers to interpret and \"\n                \"understand visual data from the world around us.\",\n        tags=[\"Computer Vision\", \"AI\", \"Image Processing\"]\n    ),\n]\n\n# Generate embeddings for each document\nfor doc in sample_docs:\n    doc.generate_embedding()\n\n# Store documents in Qdrant asynchronously\nasync def store_documents(documents):\n    print(f\"Storing {len(documents)} documents in Qdrant...\")\n\n    # Store in Qdrant using the AsyncQdrantAdapter\n    result = await AsyncQdrantAdapter.to_obj(\n        documents,\n        collection=\"documents\",\n        url=None,  # Use in-memory storage\n        many=True\n    )\n\n    print(f\"Storage result: {result}\")\n\n# Search for similar documents asynchronously\nasync def search_documents(query_text, top_k=2):\n    print(f\"Searching for documents similar to: '{query_text}'\")\n\n    # Generate embedding for the query\n    query_embedding = model.encode(query_text).tolist()\n\n    # Search in Qdrant using the AsyncQdrantAdapter\n    results = await AsyncQdrantAdapter.from_obj(\n        Document,\n        {\n            \"collection\": \"documents\",\n            \"query_vector\": query_embedding,\n            \"top_k\": top_k,\n            \"url\": None\n        },\n        many=True\n    )\n\n    print(f\"Found {len(results)} similar documents:\")\n    for i, doc in enumerate(results):\n        print(f\"{i+1}. {doc.title}\")\n        print(f\"   Content: {doc.content}\")\n        print(f\"   Tags: {', '.join(doc.tags)}\")\n        print()\n\n    return results\n\n# Main async function\nasync def main():\n    # Store documents\n    await store_documents(sample_docs)\n\n    # Perform searches\n    await search_documents(\"What is machine learning?\")\n    await search_documents(\"How do computers understand language?\")\n    await search_documents(\"How do computers process images?\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"qdrant_adapter/#advanced-example-semantic-product-search-with-filtering","title":"Advanced Example: Semantic Product Search with Filtering","text":"<p>Let's build a more practical example - a product search system that combines semantic similarity with metadata filtering:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional, Any\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.async_core import AsyncAdaptable\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n# Load the sentence transformer model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Define our product model with the AsyncAdaptable mixin\nclass Product(BaseModel, AsyncAdaptable):\n    id: str\n    name: str\n    description: str\n    price: float\n    category: str\n    brand: str\n    tags: List[str] = []\n    embedding: List[float] = []\n\n    def generate_embedding(self):\n        # Combine name and description for better semantic search\n        text = f\"{self.name}. {self.description}\"\n        self.embedding = model.encode(text).tolist()\n        return self\n\n# Register the async Qdrant adapter\nProduct.register_async_adapter(AsyncQdrantAdapter)\n\n# Sample products\nsample_products = [\n    Product(\n        id=\"p1\",\n        name=\"Premium Wireless Headphones\",\n        description=\"Noise-cancelling wireless headphones with 30-hour battery life and \"\n                   \"premium sound quality.\",\n        price=299.99,\n        category=\"Electronics\",\n        brand=\"SoundMaster\",\n        tags=[\"wireless\", \"noise-cancelling\", \"premium\", \"headphones\"]\n    ),\n    Product(\n        id=\"p2\",\n        name=\"Ultra-Slim Laptop\",\n        description=\"Lightweight laptop with 15-inch display, 16GB RAM, and 512GB SSD. \"\n                   \"Perfect for productivity on the go.\",\n        price=1299.99,\n        category=\"Electronics\",\n        brand=\"TechPro\",\n        tags=[\"laptop\", \"lightweight\", \"powerful\", \"portable\"]\n    ),\n    Product(\n        id=\"p3\",\n        name=\"Smart Fitness Watch\",\n        description=\"Track your fitness goals with this advanced smartwatch featuring \"\n                   \"heart rate monitoring, GPS, and sleep tracking.\",\n        price=199.99,\n        category=\"Wearables\",\n        brand=\"FitTech\",\n        tags=[\"fitness\", \"smartwatch\", \"health\", \"tracking\"]\n    ),\n    Product(\n        id=\"p4\",\n        name=\"Wireless Earbuds\",\n        description=\"Compact wireless earbuds with crystal clear sound, water resistance, \"\n                   \"and 24-hour battery life with the charging case.\",\n        price=129.99,\n        category=\"Electronics\",\n        brand=\"SoundMaster\",\n        tags=[\"wireless\", \"earbuds\", \"compact\", \"waterproof\"]\n    ),\n    Product(\n        id=\"p5\",\n        name=\"Professional DSLR Camera\",\n        description=\"High-end DSLR camera with 24MP sensor, 4K video recording, and \"\n                   \"professional-grade image quality.\",\n        price=1499.99,\n        category=\"Photography\",\n        brand=\"OptixPro\",\n        tags=[\"camera\", \"professional\", \"DSLR\", \"high-quality\"]\n    ),\n]\n\n# Generate embeddings for all products\nfor product in sample_products:\n    product.generate_embedding()\n\n# Product search system\nclass ProductSearchSystem:\n    def __init__(self, collection_name=\"products\", url=None):\n        self.collection_name = collection_name\n        self.url = url\n\n    async def initialize(self, products):\n        \"\"\"Initialize the search system with products\"\"\"\n        print(f\"Initializing product search system with {len(products)} products...\")\n\n        # Store products in Qdrant\n        results = []\n        for product in products:\n            result = await product.adapt_to_async(\n                obj_key=\"async_qdrant\",\n                collection=self.collection_name,\n                url=self.url\n            )\n            results.append(result)\n\n        print(\"Product search system initialized successfully\")\n        return results\n\n    async def search(self, query_text, filters=None, top_k=3):\n        \"\"\"Search for products by semantic similarity with optional filtering\"\"\"\n        print(f\"Searching for products similar to: '{query_text}'\")\n        if filters:\n            filter_desc = \", \".join(f\"{k}={v}\" for k, v in filters.items())\n            print(f\"With filters: {filter_desc}\")\n\n        # Generate embedding for the query\n        query_embedding = model.encode(query_text).tolist()\n\n        # We'll do the filtering in Python since pydapter doesn't directly expose Qdrant's filtering\n        # In a real implementation, you could extend the adapter to support Qdrant's filtering\n\n        # First, search by vector similarity\n        results = await Product.adapt_from_async(\n            {\n                \"collection\": self.collection_name,\n                \"query_vector\": query_embedding,\n                \"top_k\": top_k * 3 if filters else top_k,  # Get more results for filtering\n                \"url\": self.url\n            },\n            obj_key=\"async_qdrant\",\n            many=True\n        )\n\n        # Apply filters if specified\n        if filters:\n            filtered_results = []\n            for product in results:\n                match = True\n                for key, value in filters.items():\n                    if hasattr(product, key):\n                        if isinstance(value, list):\n                            # For list values (e.g., checking if a tag is in tags)\n                            if isinstance(getattr(product, key), list):\n                                if not any(v in getattr(product, key) for v in value):\n                                    match = False\n                                    break\n                        else:\n                            # For exact value matching\n                            if getattr(product, key) != value:\n                                match = False\n                                break\n                if match:\n                    filtered_results.append(product)\n\n            # Limit to top_k after filtering\n            results = filtered_results[:top_k]\n\n        print(f\"Found {len(results)} matching products:\")\n        for i, product in enumerate(results):\n            print(f\"{i+1}. {product.name} - ${product.price}\")\n            print(f\"   Brand: {product.brand}, Category: {product.category}\")\n            print(f\"   Description: {product.description}\")\n            print(f\"   Tags: {', '.join(product.tags)}\")\n            print()\n\n        return results\n\n# Main function to demo the advanced search system\nasync def main():\n    # Create and initialize the search system\n    search_system = ProductSearchSystem()\n    await search_system.initialize(sample_products)\n\n    # Perform various searches\n    print(\"\\n--- Basic Semantic Search ---\")\n    await search_system.search(\"wireless audio devices\")\n\n    print(\"\\n--- Search with Brand Filter ---\")\n    await search_system.search(\"wireless audio\", filters={\"brand\": \"SoundMaster\"})\n\n    print(\"\\n--- Search with Price Filter ---\")\n    await search_system.search(\"portable computing device\", filters={\"category\": \"Electronics\"})\n\n    print(\"\\n--- Search with Tag Filter ---\")\n    await search_system.search(\"advanced technology\", filters={\"tags\": [\"professional\", \"powerful\"]})\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"qdrant_adapter/#working-with-a-persistent-qdrant-instance","title":"Working with a Persistent Qdrant Instance","text":"<p>For production use, you'll want to connect to a persistent Qdrant instance rather than using in-memory storage. Here's how to do it:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom typing import List\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n# Define model and transformer as before\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nclass Document(BaseModel):\n    id: str\n    title: str\n    content: str\n    embedding: List[float] = []\n\n    def generate_embedding(self):\n        self.embedding = model.encode(self.content).tolist()\n        return self\n\nasync def demo_persistent_qdrant():\n    # Connect to Qdrant running in Docker\n    qdrant_url = \"http://localhost:6333\"\n\n    # Create a test document\n    doc = Document(\n        id=\"test1\",\n        title=\"Test Document\",\n        content=\"This is a test document to verify connection to a persistent Qdrant instance.\"\n    ).generate_embedding()\n\n    try:\n        # Store the document\n        print(\"Storing document in persistent Qdrant...\")\n        result = await AsyncQdrantAdapter.to_obj(\n            doc,\n            collection=\"test_collection\",\n            url=qdrant_url\n        )\n        print(f\"Storage result: {result}\")\n\n        # Search for the document\n        print(\"\\nRetrieving document from persistent Qdrant...\")\n        query_embedding = model.encode(\"test document verify\").tolist()\n        results = await AsyncQdrantAdapter.from_obj(\n            Document,\n            {\n                \"collection\": \"test_collection\",\n                \"query_vector\": query_embedding,\n                \"url\": qdrant_url\n            },\n            many=True\n        )\n\n        print(f\"Retrieved {len(results)} documents:\")\n        for doc in results:\n            print(f\"  - {doc.title}: {doc.content}\")\n\n    except Exception as e:\n        print(f\"Error connecting to Qdrant: {e}\")\n        print(\"Make sure Qdrant is running on localhost:6333\")\n\nif __name__ == \"__main__\":\n    asyncio.run(demo_persistent_qdrant())\n</code></pre>"},{"location":"qdrant_adapter/#error-handling-for-vector-operations","title":"Error Handling for Vector Operations","text":"<p>Let's demonstrate proper error handling for common Qdrant operations:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom typing import List\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.exceptions import ConnectionError, QueryError, ResourceError, ValidationError\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n# Define a simple model\nclass ImageVector(BaseModel):\n    id: str\n    embedding: List[float]\n\nasync def handle_vector_errors():\n    print(\"Testing error handling for vector operations...\")\n\n    # 1. Validation error - empty vector\n    try:\n        invalid_vector = ImageVector(id=\"test1\", embedding=[])\n        await AsyncQdrantAdapter.to_obj(\n            invalid_vector,\n            collection=\"test_collection\",\n            url=None\n        )\n    except ValidationError as e:\n        print(f\"Vector validation error handled: {e}\")\n\n    # 2. Validation error - inconsistent vector dimensions\n    try:\n        # First create a collection with 5D vectors\n        valid_vector = ImageVector(id=\"test2\", embedding=[0.1, 0.2, 0.3, 0.4, 0.5])\n        await AsyncQdrantAdapter.to_obj(\n            valid_vector,\n            collection=\"dimension_test\",\n            url=None\n        )\n\n        # Then try to add a 3D vector to the same collection\n        invalid_vector = ImageVector(id=\"test3\", embedding=[0.1, 0.2, 0.3])\n        await AsyncQdrantAdapter.to_obj(\n            invalid_vector,\n            collection=\"dimension_test\",\n            url=None\n        )\n    except ValidationError as e:\n        print(f\"Vector dimension mismatch handled: {e}\")\n\n    # 3. Connection error - wrong URL\n    try:\n        vector = ImageVector(id=\"test4\", embedding=[0.1, 0.2, 0.3, 0.4, 0.5])\n        await AsyncQdrantAdapter.to_obj(\n            vector,\n            collection=\"test_collection\",\n            url=\"http://nonexistent-qdrant-host:6333\"\n        )\n    except ConnectionError as e:\n        print(f\"Connection error handled: {e}\")\n\n    # 4. Resource error - collection doesn't exist\n    try:\n        await AsyncQdrantAdapter.from_obj(\n            ImageVector,\n            {\n                \"collection\": \"nonexistent_collection\",\n                \"query_vector\": [0.1, 0.2, 0.3, 0.4, 0.5],\n                \"url\": None\n            }\n        )\n    except ResourceError as e:\n        print(f\"Resource error handled: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(handle_vector_errors())\n</code></pre>"},{"location":"qdrant_adapter/#advanced-topics-working-with-high-dimensional-vectors","title":"Advanced Topics: Working with High-Dimensional Vectors","text":"<p>For production applications, you'll often work with higher-dimensional vectors. Here's how to use pydapter with larger models:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n# Load a larger model with higher dimensions\n# ada-002 generates 1536-dimensional vectors, better for semantic similarity\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')  # 768 dimensions\n\nclass Document(BaseModel):\n    id: str\n    title: str\n    content: str\n    embedding: List[float] = []\n\n    def generate_embedding(self):\n        self.embedding = model.encode(self.content).tolist()\n        return self\n\nasync def demo_high_dim_vectors():\n    print(\"Demonstrating high-dimensional vector operations...\")\n\n    # Create a document with high-dim embedding\n    doc = Document(\n        id=\"highdim1\",\n        title=\"High-Dimensional Vector Example\",\n        content=\"This document has a higher-dimensional embedding vector for better \"\n               \"semantic search accuracy.\"\n    ).generate_embedding()\n\n    print(f\"Generated embedding with {len(doc.embedding)} dimensions\")\n\n    # Store in Qdrant\n    print(\"Storing document...\")\n    result = await AsyncQdrantAdapter.to_obj(\n        doc,\n        collection=\"highdim_documents\",\n        url=None\n    )\n\n    # Search with a similar query\n    query_text = \"semantic search with high dimensions\"\n    print(f\"Searching with query: '{query_text}'\")\n\n    query_embedding = model.encode(query_text).tolist()\n    results = await AsyncQdrantAdapter.from_obj(\n        Document,\n        {\n            \"collection\": \"highdim_documents\",\n            \"query_vector\": query_embedding,\n            \"url\": None\n        },\n        many=True\n    )\n\n    print(f\"Found {len(results)} results:\")\n    for doc in results:\n        print(f\"  - {doc.title}: {doc.content}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(demo_high_dim_vectors())\n</code></pre>"},{"location":"qdrant_adapter/#real-world-application-document-search-engine","title":"Real-World Application: Document Search Engine","text":"<p>Let's build a more complete document search engine with pydapter and Qdrant:</p> <pre><code>import asyncio\nimport os\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional, Any\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom pydapter.async_core import AsyncAdaptable\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n# Load the sentence transformer model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nclass Document(BaseModel, AsyncAdaptable):\n    id: str\n    title: str\n    content: str\n    author: Optional[str] = None\n    date: Optional[str] = None\n    source: Optional[str] = None\n    tags: List[str] = []\n    embedding: List[float] = []\n\n    def generate_embedding(self):\n        # Combine title and content for better semantic search\n        text = f\"{self.title}. {self.content}\"\n        self.embedding = model.encode(text).tolist()\n        return self\n\n# Register the async Qdrant adapter\nDocument.register_async_adapter(AsyncQdrantAdapter)\n\nclass DocumentSearchEngine:\n    def __init__(self, collection_name=\"documents\", url=None):\n        self.collection_name = collection_name\n        self.url = url\n\n    async def add_document(self, document):\n        \"\"\"Add a single document to the search engine\"\"\"\n        # Generate embedding if not already present\n        if not document.embedding:\n            document.generate_embedding()\n\n        # Store in Qdrant\n        result = await document.adapt_to_async(\n            obj_key=\"async_qdrant\",\n            collection=self.collection_name,\n            url=self.url\n        )\n\n        return result\n\n    async def add_documents(self, documents):\n        \"\"\"Add multiple documents to the search engine\"\"\"\n        results = []\n        for doc in documents:\n            result = await self.add_document(doc)\n            results.append(result)\n\n        return results\n\n    async def search(self, query_text, filters=None, top_k=5):\n        \"\"\"Search for documents similar to the query text with optional filters\"\"\"\n        # Generate embedding for the query\n        query_embedding = model.encode(query_text).tolist()\n\n        # Get raw results (we'll filter in Python)\n        raw_results = await Document.adapt_from_async(\n            {\n                \"collection\": self.collection_name,\n                \"query_vector\": query_embedding,\n                \"top_k\": top_k * 3 if filters else top_k,  # Get more results for filtering\n                \"url\": self.url\n            },\n            obj_key=\"async_qdrant\",\n            many=True\n        )\n\n        # Apply filters if any\n        if filters:\n            filtered_results = []\n            for doc in raw_results:\n                match = True\n                for key, value in filters.items():\n                    if hasattr(doc, key):\n                        if isinstance(value, list):\n                            # For tags or other list fields\n                            if isinstance(getattr(doc, key), list):\n                                if not any(v in getattr(doc, key) for v in value):\n                                    match = False\n                                    break\n                        elif key == \"date\" and isinstance(value, dict):\n                            # Special handling for date range filters\n                            doc_date = getattr(doc, key)\n                            if not doc_date:\n                                match = False\n                                break\n\n                            if \"from\" in value and doc_date &lt; value[\"from\"]:\n                                match = False\n                                break\n\n                            if \"to\" in value and doc_date &gt; value[\"to\"]:\n                                match = False\n                                break\n                        else:\n                            # For exact matching\n                            if getattr(doc, key) != value:\n                                match = False\n                                break\n\n                if match:\n                    filtered_results.append(doc)\n\n            # Limit to top_k after filtering\n            results = filtered_results[:top_k]\n        else:\n            results = raw_results[:top_k]\n\n        return results\n\n    async def get_document(self, doc_id):\n        \"\"\"Retrieve a specific document by ID\"\"\"\n        # In a real implementation, you would use Qdrant's point_id search\n        # For now, we'll use the vector search and filter in Python\n        try:\n            # Get a document with a similar ID (not ideal, but works for the example)\n            results = await Document.adapt_from_async(\n                {\n                    \"collection\": self.collection_name,\n                    \"query_vector\": model.encode(doc_id).tolist(),\n                    \"top_k\": 10,\n                    \"url\": self.url\n                },\n                obj_key=\"async_qdrant\",\n                many=True\n            )\n\n            # Find the exact ID match\n            for doc in results:\n                if doc.id == doc_id:\n                    return doc\n\n            return None\n        except Exception as e:\n            print(f\"Error retrieving document: {e}\")\n            return None\n\n    async def delete_document(self, doc_id):\n        \"\"\"Delete a document by ID\"\"\"\n        # This functionality would require extending the adapter\n        # to support Qdrant's delete_points method\n        raise NotImplementedError(\"Delete functionality not yet implemented\")\n\nasync def main():\n    # Create sample documents\n    documents = [\n        Document(\n            id=\"doc1\",\n            title=\"Introduction to Vector Databases\",\n            content=\"Vector databases store high-dimensional vectors and enable semantic \"\n                   \"search based on similarity rather than exact matching.\",\n            author=\"Jane Smith\",\n            date=\"2023-01-15\",\n            source=\"TechBlog\",\n            tags=[\"vector-database\", \"semantic-search\", \"embeddings\"]\n        ),\n        Document(\n            id=\"doc2\",\n            title=\"Machine Learning Fundamentals\",\n            content=\"Machine learning algorithms learn patterns from data without being \"\n                   \"explicitly programmed. They improve with experience.\",\n            author=\"John Doe\",\n            date=\"2023-02-20\",\n            source=\"AI Journal\",\n            tags=[\"machine-learning\", \"AI\", \"algorithms\"]\n        ),\n        Document(\n            id=\"doc3\",\n            title=\"Natural Language Processing Techniques\",\n            content=\"NLP enables computers to understand human language by processing, \"\n                   \"analyzing, and generating text data.\",\n            author=\"Jane Smith\",\n            date=\"2023-03-10\",\n            source=\"AI Journal\",\n            tags=[\"NLP\", \"text-processing\", \"AI\"]\n        ),\n        Document(\n            id=\"doc4\",\n            title=\"Semantic Search Implementation\",\n            content=\"Implementing semantic search requires converting text to vector \"\n                   \"embeddings and finding similar vectors efficiently.\",\n            author=\"Alex Johnson\",\n            date=\"2023-04-05\",\n            source=\"TechBlog\",\n            tags=[\"semantic-search\", \"embeddings\", \"implementation\"]\n        ),\n        Document(\n            id=\"doc5\",\n            title=\"Vector Database Comparison\",\n            content=\"Comparing popular vector databases like Qdrant, Pinecone, and Milvus \"\n                   \"for semantic search applications.\",\n            author=\"Chris Williams\",\n            date=\"2023-05-12\",\n            source=\"Database Review\",\n            tags=[\"vector-database\", \"comparison\", \"Qdrant\", \"Pinecone\", \"Milvus\"]\n        ),\n    ]\n\n    # Initialize the search engine\n    search_engine = DocumentSearchEngine()\n\n    # Add sample documents\n    print(\"Adding sample documents to the search engine...\")\n    await search_engine.add_documents(documents)\n\n    # Perform searches\n    print(\"\\n--- Basic Semantic Search ---\")\n    results = await search_engine.search(\"How do vector databases work?\")\n    print(f\"Found {len(results)} documents:\")\n    for i, doc in enumerate(results):\n        print(f\"{i+1}. {doc.title}\")\n        print(f\"   Author: {doc.author}, Date: {doc.date}\")\n        print(f\"   Content: {doc.content}\")\n        print()\n\n    print(\"\\n--- Search with Author Filter ---\")\n    author_results = await search_engine.search(\n        \"AI and machine learning techniques\",\n        filters={\"author\": \"Jane Smith\"}\n    )\n    print(f\"Found {len(author_results)} documents by Jane Smith:\")\n    for i, doc in enumerate(author_results):\n        print(f\"{i+1}. {doc.title}\")\n        print(f\"   Author: {doc.author}, Date: {doc.date}\")\n        print(f\"   Content: {doc.content}\")\n        print()\n\n    print(\"\\n--- Search with Tag Filter ---\")\n    tag_results = await search_engine.search(\n        \"database technology\",\n        filters={\"tags\": [\"vector-database\"]}\n    )\n    print(f\"Found {len(tag_results)} documents with 'vector-database' tag:\")\n    for i, doc in enumerate(tag_results):\n        print(f\"{i+1}. {doc.title}\")\n        print(f\"   Tags: {', '.join(doc.tags)}\")\n        print(f\"   Content: {doc.content}\")\n        print()\n\n    print(\"\\n--- Get Document by ID ---\")\n    doc = await search_engine.get_document(\"doc3\")\n    if doc:\n        print(f\"Retrieved document: {doc.title}\")\n        print(f\"Content: {doc.content}\")\n    else:\n        print(\"Document not found\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"qdrant_adapter/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you've learned how to use pydapter's Qdrant adapters to build semantic search applications with vector embeddings. We've covered:</p> <ol> <li>Setting up Qdrant and generating vector embeddings</li> <li>Using both synchronous and asynchronous adapters</li> <li>Storing Pydantic models with embeddings in Qdrant</li> <li>Performing similarity searches</li> <li>Implementing filtering and advanced search features</li> <li>Building complete search applications</li> </ol> <p>Vector databases like Qdrant are powerful tools for implementing semantic search, recommendation systems, and other AI applications that require similarity matching rather than exact keyword matching.</p> <p>The pydapter adapters make it easy to integrate vector database functionality into your Python applications, with a clean and consistent interface for working with Pydantic models.</p> <p>By combining pydapter's adapters with pre-trained embedding models like sentence-transformers, you can quickly build sophisticated semantic search systems with minimal code.</p>"},{"location":"sql_model_adapter/","title":"pydapter 0.1.4 Tutorial","text":"<p>Bridge Pydantic \u21c6 SQLAlchemy (with optional pgvector)</p>"},{"location":"sql_model_adapter/#1-installation","title":"1 Installation","text":"<pre><code># core features\npip install pydapter&gt;=0.1.4 sqlalchemy&gt;=2.0 alembic\n\n# add pgvector support and drivers\npip install pydapter[pgvector] psycopg[binary] pgvector\n</code></pre>"},{"location":"sql_model_adapter/#2-quick-start-scalar-models","title":"2 Quick-start (scalar models)","text":""},{"location":"sql_model_adapter/#21-define-your-validation-model","title":"2.1 Define your validation model","text":"<pre><code>from pydantic import BaseModel\n\nclass UserSchema(BaseModel):\n    id: int | None = None          # promoted to PK\n    name: str\n    email: str | None = None\n    active: bool = True\n</code></pre>"},{"location":"sql_model_adapter/#22-generate-the-orm-class","title":"2.2 Generate the ORM class","text":"<pre><code>from pydapter.model_adapters import SQLModelAdapter\n\nUserSQL = SQLModelAdapter.pydantic_model_to_sql(UserSchema)\n</code></pre> <p><code>UserSQL</code> is a fully-mapped SQLAlchemy declarative model\u2014Alembic will pick it up automatically.</p>"},{"location":"sql_model_adapter/#23-round-trip-back-to-pydantic-optional","title":"2.3 Round-trip back to Pydantic (optional)","text":"<pre><code>RoundTrip = SQLModelAdapter.sql_model_to_pydantic(UserSQL)\nuser_json = RoundTrip.model_validate(UserSQL(name=\"Ann\")).model_dump()\n</code></pre>"},{"location":"sql_model_adapter/#3-embeddings-with-pgvector","title":"3 Embeddings with <code>pgvector</code>","text":""},{"location":"sql_model_adapter/#31-validation-layer","title":"3.1 Validation layer","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass DocSchema(BaseModel):\n    id: int | None = None\n    text: str\n    embedding: list[float] = Field(..., vector_dim=768)\n</code></pre>"},{"location":"sql_model_adapter/#32-generate-vector-aware-model","title":"3.2 Generate vector-aware model","text":"<pre><code>from pydapter.model_adapters import SQLVectorModelAdapter\n\nDocSQL = SQLVectorModelAdapter.pydantic_model_to_sql(DocSchema)\n</code></pre> <p>Result:</p> <pre><code>Column('embedding', Vector(768), nullable=False)\n</code></pre>"},{"location":"sql_model_adapter/#33-reverse-conversion","title":"3.3 Reverse conversion","text":"<pre><code>DocSchemaRT = SQLVectorModelAdapter.sql_model_to_pydantic(DocSQL)\nassert DocSchemaRT.model_fields[\"embedding\"].json_schema_extra[\"vector_dim\"] == 768\n</code></pre>"},{"location":"sql_model_adapter/#4-alembic-integration","title":"4 Alembic integration","text":"<ol> <li>Add pgvector extension (first migration only)</li> </ol> <pre><code># env.py or an initial upgrade() block\nop.execute(\"CREATE EXTENSION IF NOT EXISTS pgvector\")\n</code></pre> <ol> <li>Autogenerate migrations</li> </ol> <pre><code>alembic revision --autogenerate -m \"init tables\"\n</code></pre> <p>All columns\u2014including <code>Vector(dim)</code>\u2014appear in the diff.</p>"},{"location":"sql_model_adapter/#5-advanced-options","title":"5 Advanced options","text":"Need How Custom table name <code>SQLModelAdapter.pydantic_model_to_sql(UserSchema, table_name=\"users\")</code> Alternate PK field <code>\u2026, pk_field=\"uuid\"</code> Cache generated classes Wrap the call in your own memoization layer; generation runs once per import. Unsupported types Extend <code>_PY_TO_SQL</code> / <code>_SQL_TO_PY</code> dictionaries or subclass the adapter."},{"location":"sql_model_adapter/#6-testing-ci","title":"6 Testing &amp; CI","text":"<p>Unit tests rely only on SQLAlchemy inspection\u2014no database spin-up.</p> <pre><code>pytest -q\n</code></pre> <p>To include vector tests:</p> <pre><code>pytest -q -m \"not pgvector\"          # skip\npytest -q                            # run all (pgvector installed)\n</code></pre>"},{"location":"sql_model_adapter/#7-troubleshooting","title":"7 Troubleshooting","text":"Symptom Fix <code>TypeError: Unsupported type \u2026</code> Add a mapping in the adapter or exclude the field. Alembic shows no changes Ensure generated classes share <code>metadata</code> or are imported in <code>env.py</code>. Vector dim missing Provide <code>vector_dim</code> in <code>json_schema_extra</code>, or accept flexible dimension."},{"location":"sql_model_adapter/#8-wrap-up","title":"8 Wrap-up","text":"<p>pydapter 0.1.4 lets you:</p> <ul> <li>Keep one source of truth\u2014your Pydantic models.</li> <li>Ship migrations without hand-writing ORM classes.</li> <li>Store embeddings directly in Postgres with pgvector.</li> </ul> <p>Update, generate, migrate\u2014done. Happy coding! \ud83d\ude80</p>"},{"location":"testing/","title":"Testing in pydapter","text":"<p>pydapter uses a comprehensive testing strategy to ensure reliability and correctness of all adapters. This document explains the testing approach and how to run tests.</p>"},{"location":"testing/#testing-strategy","title":"Testing Strategy","text":"<p>pydapter employs two main types of tests:</p> <ol> <li>Unit Tests - Test adapter functionality in isolation using mocks</li> <li>Integration Tests - Test adapters with real database systems using    TestContainers</li> </ol>"},{"location":"testing/#unit-tests","title":"Unit Tests","text":"<p>Unit tests are designed to test adapter functionality without requiring external dependencies. These tests use mocks to simulate database connections and responses, making them fast and reliable.</p> <p>Example unit test for a database adapter:</p> <pre><code>def test_postgres_adapter_to_obj(mocker):\n    # Mock SQLAlchemy engine and connection\n    mock_engine = mocker.patch(\"sqlalchemy.create_engine\")\n    mock_conn = mock_engine.return_value.begin.return_value.__enter__.return_value\n\n    # Create test model\n    test_model = TestModel(id=1, name=\"test\", value=42.0)\n\n    # Test adapter\n    PostgresAdapter.to_obj(test_model, engine_url=\"postgresql://test\", table=\"test_table\")\n\n    # Verify SQL execution was called with correct parameters\n    mock_conn.execute.assert_called_once()\n</code></pre>"},{"location":"testing/#integration-tests","title":"Integration Tests","text":"<p>Integration tests verify that adapters work correctly with actual database systems. These tests use TestContainers to spin up isolated database instances in Docker containers during test execution.</p>"},{"location":"testing/#supported-databases","title":"Supported Databases","text":"<p>pydapter includes integration tests for:</p> <ul> <li>PostgreSQL - SQL database adapter tests</li> <li>MongoDB - Document database adapter tests</li> <li>Neo4j - Graph database adapter tests</li> <li>Qdrant - Vector database adapter tests</li> </ul>"},{"location":"testing/#testcontainers-setup","title":"TestContainers Setup","text":"<p>Integration tests use pytest fixtures to create and manage database containers:</p> <pre><code>@pytest.fixture(scope=\"session\")\ndef pg_url():\n    \"\"\"PostgreSQL container fixture for tests.\"\"\"\n    from testcontainers.postgres import PostgresContainer\n\n    with PostgresContainer(\"postgres:16-alpine\") as pg:\n        url = pg.get_connection_url()\n        yield url\n</code></pre> <p>These fixtures handle container lifecycle, ensuring proper cleanup after tests complete.</p>"},{"location":"testing/#example-integration-test","title":"Example Integration Test","text":"<pre><code>def test_postgres_single_record(pg_url, sync_model_factory, postgres_table):\n    \"\"\"Test PostgreSQL adapter with a single record.\"\"\"\n    # Create test instance\n    test_model = sync_model_factory(id=42, name=\"test_postgres\", value=12.34)\n\n    # Register adapter\n    test_model.__class__.register_adapter(PostgresAdapter)\n\n    # Store in database\n    test_model.adapt_to(obj_key=\"postgres\", engine_url=pg_url, table=\"test_table\")\n\n    # Retrieve from database\n    retrieved = test_model.__class__.adapt_from(\n        {\"engine_url\": pg_url, \"table\": \"test_table\", \"selectors\": {\"id\": 42}},\n        obj_key=\"postgres\",\n        many=False,\n    )\n\n    # Verify data integrity\n    assert retrieved.id == test_model.id\n    assert retrieved.name == test_model.name\n    assert retrieved.value == test_model.value\n</code></pre>"},{"location":"testing/#running-tests","title":"Running Tests","text":""},{"location":"testing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Docker (for integration tests)</li> </ul>"},{"location":"testing/#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/ohdearquant/pydapter.git\ncd pydapter\n\n# Install development dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"testing/#running-all-tests","title":"Running All Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"testing/#running-specific-tests","title":"Running Specific Tests","text":"<pre><code># Run only unit tests\npytest tests/test_*.py -k \"not test_integration\"\n\n# Run only integration tests\npytest tests/test_integration_*.py\n\n# Run tests for a specific adapter\npytest tests/test_*postgres*.py\n</code></pre>"},{"location":"testing/#docker-availability-check","title":"Docker Availability Check","text":"<p>Integration tests automatically check for Docker availability and are skipped if Docker is not running:</p> <pre><code>def is_docker_available():\n    \"\"\"Check if Docker is available.\"\"\"\n    import subprocess\n    try:\n        subprocess.run([\"docker\", \"info\"], check=True, capture_output=True)\n        return True\n    except (subprocess.SubprocessError, FileNotFoundError):\n        return False\n\n# Skip tests if Docker is not available\npytestmark = pytest.mark.skipif(\n    not is_docker_available(), reason=\"Docker is not available\"\n)\n</code></pre>"},{"location":"testing/#writing-new-tests","title":"Writing New Tests","text":"<p>When contributing new adapters or features to pydapter, please include both unit tests and integration tests:</p> <ol> <li>Unit tests should test the adapter's functionality in isolation using    mocks</li> <li>Integration tests should verify the adapter works with a real database    instance</li> </ol>"},{"location":"testing/#integration-test-template","title":"Integration Test Template","text":"<pre><code>def test_new_adapter_integration(container_url, model_factory, cleanup_fixture):\n    \"\"\"Test new adapter with a real database.\"\"\"\n    # Create test instance\n    test_model = model_factory(id=1, name=\"test\", value=42.0)\n\n    # Register adapter\n    test_model.__class__.register_adapter(NewAdapter)\n\n    # Store in database\n    test_model.adapt_to(obj_key=\"new_adapter\", url=container_url, ...)\n\n    # Retrieve from database\n    retrieved = test_model.__class__.adapt_from(\n        {\"url\": container_url, ...},\n        obj_key=\"new_adapter\",\n        many=False,\n    )\n\n    # Verify data integrity\n    assert retrieved.id == test_model.id\n    assert retrieved.name == test_model.name\n    assert retrieved.value == test_model.value\n</code></pre>"},{"location":"testing/#test-coverage","title":"Test Coverage","text":"<p>pydapter aims to maintain high test coverage. You can generate a coverage report with:</p> <pre><code>pytest --cov=pydapter\n</code></pre> <p>For a detailed HTML report:</p> <pre><code>pytest --cov=pydapter --cov-report=html\n</code></pre> <p>This will create a <code>htmlcov</code> directory with the coverage report.</p>"},{"location":"api/adapters/","title":"Adapters API","text":"<p>This page documents the built-in adapters provided by pydapter.</p>"},{"location":"api/adapters/#csv-adapter","title":"CSV Adapter","text":""},{"location":"api/adapters/#pydapter.adapters.csv_","title":"<code>pydapter.adapters.csv_</code>","text":"<p>CSV Adapter for Pydantic Models.</p> <p>This module provides the CsvAdapter class for converting between Pydantic models and CSV data formats. It supports reading from CSV files or strings and writing Pydantic models to CSV format.</p>"},{"location":"api/adapters/#pydapter.adapters.csv_-classes","title":"Classes","text":""},{"location":"api/adapters/#pydapter.adapters.csv_.CsvAdapter","title":"<code>CsvAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Adapter for converting between Pydantic models and CSV data.</p> <p>This adapter handles CSV files and strings, providing methods to: - Parse CSV data into Pydantic model instances - Convert Pydantic models to CSV format - Handle various CSV dialects and formatting options</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"csv\")</p> <code>DEFAULT_CSV_KWARGS</code> <p>Default CSV parsing parameters</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.adapters.csv_ import CsvAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Parse CSV data\ncsv_data = \"name,age\\nJohn,30\\nJane,25\"\npeople = CsvAdapter.from_obj(Person, csv_data, many=True)\n\n# Convert to CSV\ncsv_output = CsvAdapter.to_obj(people)\n</code></pre> Source code in <code>src/pydapter/adapters/csv_.py</code> <pre><code>class CsvAdapter(Adapter[T]):\n    \"\"\"\n    Adapter for converting between Pydantic models and CSV data.\n\n    This adapter handles CSV files and strings, providing methods to:\n    - Parse CSV data into Pydantic model instances\n    - Convert Pydantic models to CSV format\n    - Handle various CSV dialects and formatting options\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"csv\")\n        DEFAULT_CSV_KWARGS: Default CSV parsing parameters\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.adapters.csv_ import CsvAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Parse CSV data\n        csv_data = \"name,age\\\\nJohn,30\\\\nJane,25\"\n        people = CsvAdapter.from_obj(Person, csv_data, many=True)\n\n        # Convert to CSV\n        csv_output = CsvAdapter.to_obj(people)\n        ```\n    \"\"\"\n\n    obj_key = \"csv\"\n\n    # Default CSV dialect settings\n    DEFAULT_CSV_KWARGS = {\n        \"escapechar\": \"\\\\\",\n        \"quotechar\": '\"',\n        \"delimiter\": \",\",\n        \"quoting\": csv.QUOTE_MINIMAL,\n    }\n\n    # ---------------- incoming\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: str | Path,\n        /,\n        *,\n        many: bool = True,\n        **kw,\n    ):\n        try:\n            # Handle file path or string content\n            if isinstance(obj, Path):\n                try:\n                    text = Path(obj).read_text()\n                except Exception as e:\n                    raise ParseError(f\"Failed to read CSV file: {e}\", source=str(obj))\n            else:\n                text = obj\n\n            # Sanitize text to remove NULL bytes\n            text = text.replace(\"\\0\", \"\")\n\n            if not text.strip():\n                raise ParseError(\n                    \"Empty CSV content\",\n                    source=str(obj)[:100] if isinstance(obj, str) else str(obj),\n                )\n\n            # Merge default CSV kwargs with user-provided kwargs\n            csv_kwargs = cls.DEFAULT_CSV_KWARGS.copy()\n            csv_kwargs.update(kw)  # User-provided kwargs override defaults\n\n            # Parse CSV\n            try:\n                # Extract specific parameters from csv_kwargs\n                delimiter = \",\"\n                quotechar = '\"'\n                escapechar = \"\\\\\"\n                quoting = csv.QUOTE_MINIMAL\n\n                if \"delimiter\" in csv_kwargs:\n                    delimiter = str(csv_kwargs.pop(\"delimiter\"))\n                if \"quotechar\" in csv_kwargs:\n                    quotechar = str(csv_kwargs.pop(\"quotechar\"))\n                if \"escapechar\" in csv_kwargs:\n                    escapechar = str(csv_kwargs.pop(\"escapechar\"))\n                if \"quoting\" in csv_kwargs:\n                    quoting_value = csv_kwargs.pop(\"quoting\")\n                    if isinstance(quoting_value, int):\n                        quoting = quoting_value\n                    else:\n                        quoting = csv.QUOTE_MINIMAL\n\n                reader = csv.DictReader(\n                    io.StringIO(text),\n                    delimiter=delimiter,\n                    quotechar=quotechar,\n                    escapechar=escapechar,\n                    quoting=quoting,\n                )\n                rows = list(reader)\n\n                if not rows:\n                    return [] if many else None\n\n                # Check for missing fieldnames\n                if not reader.fieldnames:\n                    raise ParseError(\"CSV has no headers\", source=text[:100])\n\n                # Check for missing required fields in the model\n                model_fields = subj_cls.model_fields\n                required_fields = [\n                    field for field, info in model_fields.items() if info.is_required()\n                ]\n\n                missing_fields = [\n                    field for field in required_fields if field not in reader.fieldnames\n                ]\n\n                if missing_fields:\n                    raise ParseError(\n                        f\"CSV missing required fields: {', '.join(missing_fields)}\",\n                        source=text[:100],\n                        fields=missing_fields,\n                    )\n\n                # Convert rows to model instances\n                result = []\n                for i, row in enumerate(rows):\n                    try:\n                        result.append(subj_cls.model_validate(row))\n                    except ValidationError as e:\n                        raise AdapterValidationError(\n                            f\"Validation error in row {i + 1}: {e}\",\n                            data=row,\n                            row=i + 1,\n                            errors=e.errors(),\n                        )\n\n                # If there's only one row and many=False, return a single object\n                if len(result) == 1 and not many:\n                    return result[0]\n                # Otherwise, return a list of objects\n                return result\n\n            except csv.Error as e:\n                raise ParseError(f\"CSV parsing error: {e}\", source=text[:100])\n\n        except (ParseError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ParseError(\n                f\"Unexpected error parsing CSV: {e}\",\n                source=str(obj)[:100] if isinstance(obj, str) else str(obj),\n            )\n\n    # ---------------- outgoing\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | list[T],\n        /,\n        *,\n        many: bool = True,\n        **kw,\n    ) -&gt; str:\n        try:\n            items = subj if isinstance(subj, list) else [subj]\n\n            if not items:\n                return \"\"\n\n            buf = io.StringIO()\n\n            # Sanitize any string values to remove NULL bytes\n            sanitized_items = []\n            for item in items:\n                item_dict = item.model_dump()\n                for key, value in item_dict.items():\n                    if isinstance(value, str):\n                        item_dict[key] = value.replace(\"\\0\", \"\")\n                sanitized_items.append(item_dict)\n\n            # Merge default CSV kwargs with user-provided kwargs\n            csv_kwargs = cls.DEFAULT_CSV_KWARGS.copy()\n            csv_kwargs.update(kw)  # User-provided kwargs override defaults\n\n            # Get fieldnames from the first item\n            fieldnames = list(items[0].model_dump().keys())\n\n            # Extract specific parameters from csv_kwargs\n            delimiter = \",\"\n            quotechar = '\"'\n            escapechar = \"\\\\\"\n            quoting = csv.QUOTE_MINIMAL\n\n            if \"delimiter\" in csv_kwargs:\n                delimiter = str(csv_kwargs.pop(\"delimiter\"))\n            if \"quotechar\" in csv_kwargs:\n                quotechar = str(csv_kwargs.pop(\"quotechar\"))\n            if \"escapechar\" in csv_kwargs:\n                escapechar = str(csv_kwargs.pop(\"escapechar\"))\n            if \"quoting\" in csv_kwargs:\n                quoting_value = csv_kwargs.pop(\"quoting\")\n                if isinstance(quoting_value, int):\n                    quoting = quoting_value\n                else:\n                    quoting = csv.QUOTE_MINIMAL\n\n            writer = csv.DictWriter(\n                buf,\n                fieldnames=fieldnames,\n                delimiter=delimiter,\n                quotechar=quotechar,\n                escapechar=escapechar,\n                quoting=quoting,\n            )\n            writer.writeheader()\n            writer.writerows([i.model_dump() for i in items])\n            return buf.getvalue()\n\n        except Exception as e:\n            # Wrap exceptions\n            raise ParseError(f\"Error generating CSV: {e}\")\n</code></pre>"},{"location":"api/adapters/#json-adapter","title":"JSON Adapter","text":""},{"location":"api/adapters/#pydapter.adapters.json_","title":"<code>pydapter.adapters.json_</code>","text":"<p>JSON Adapter for Pydantic Models.</p> <p>This module provides the JsonAdapter class for converting between Pydantic models and JSON data formats. It supports reading from JSON files, strings, or bytes and writing Pydantic models to JSON format.</p>"},{"location":"api/adapters/#pydapter.adapters.json_-classes","title":"Classes","text":""},{"location":"api/adapters/#pydapter.adapters.json_.JsonAdapter","title":"<code>JsonAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Adapter for converting between Pydantic models and JSON data.</p> <p>This adapter handles JSON files, strings, and byte data, providing methods to: - Parse JSON data into Pydantic model instances - Convert Pydantic models to JSON format - Handle both single objects and arrays of objects</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"json\")</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.adapters.json_ import JsonAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Parse JSON data\njson_data = '{\"name\": \"John\", \"age\": 30}'\nperson = JsonAdapter.from_obj(Person, json_data)\n\n# Parse JSON array\njson_array = '[{\"name\": \"John\", \"age\": 30}, {\"name\": \"Jane\", \"age\": 25}]'\npeople = JsonAdapter.from_obj(Person, json_array, many=True)\n\n# Convert to JSON\njson_output = JsonAdapter.to_obj(person)\n</code></pre> Source code in <code>src/pydapter/adapters/json_.py</code> <pre><code>class JsonAdapter(Adapter[T]):\n    \"\"\"\n    Adapter for converting between Pydantic models and JSON data.\n\n    This adapter handles JSON files, strings, and byte data, providing methods to:\n    - Parse JSON data into Pydantic model instances\n    - Convert Pydantic models to JSON format\n    - Handle both single objects and arrays of objects\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"json\")\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.adapters.json_ import JsonAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Parse JSON data\n        json_data = '{\"name\": \"John\", \"age\": 30}'\n        person = JsonAdapter.from_obj(Person, json_data)\n\n        # Parse JSON array\n        json_array = '[{\"name\": \"John\", \"age\": 30}, {\"name\": \"Jane\", \"age\": 25}]'\n        people = JsonAdapter.from_obj(Person, json_array, many=True)\n\n        # Convert to JSON\n        json_output = JsonAdapter.to_obj(person)\n        ```\n    \"\"\"\n\n    obj_key = \"json\"\n\n    # ---------------- incoming\n    @classmethod\n    def from_obj(\n        cls, subj_cls: type[T], obj: str | bytes | Path, /, *, many=False, **kw\n    ):\n        try:\n            # Handle file path\n            if isinstance(obj, Path):\n                try:\n                    text = Path(obj).read_text()\n                except Exception as e:\n                    raise ParseError(f\"Failed to read JSON file: {e}\", source=str(obj))\n            else:\n                text = obj.decode(\"utf-8\") if isinstance(obj, bytes) else obj\n            # Check for empty input\n            if not text or (isinstance(text, str) and not text.strip()):\n                raise ParseError(\n                    \"Empty JSON content\",\n                    source=str(obj)[:100] if isinstance(obj, str) else str(obj),\n                )\n\n            # Parse JSON\n            try:\n                data = json.loads(text)\n            except json.JSONDecodeError as e:\n                raise ParseError(\n                    f\"Invalid JSON: {e}\",\n                    source=str(text)[:100] if isinstance(text, str) else str(text),\n                    position=e.pos,\n                    line=e.lineno,\n                    column=e.colno,\n                )\n\n            # Validate against model\n            try:\n                if many:\n                    if not isinstance(data, list):\n                        raise AdapterValidationError(\n                            \"Expected JSON array for many=True\", data=data\n                        )\n                    return [subj_cls.model_validate(i) for i in data]\n                return subj_cls.model_validate(data)\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=data,\n                    errors=e.errors(),\n                )\n\n        except (ParseError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ParseError(\n                f\"Unexpected error parsing JSON: {e}\",\n                source=str(obj)[:100] if isinstance(obj, str) else str(obj),\n            )\n\n    # ---------------- outgoing\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many=False, **kw) -&gt; str:\n        try:\n            items = subj if isinstance(subj, list) else [subj]\n\n            if not items:\n                return \"[]\" if many else \"{}\"\n\n            # Extract JSON serialization options from kwargs\n            json_kwargs = {\n                \"indent\": kw.pop(\"indent\", 2),\n                \"sort_keys\": kw.pop(\"sort_keys\", True),\n                \"ensure_ascii\": kw.pop(\"ensure_ascii\", False),\n            }\n\n            payload = [i.model_dump() for i in items] if many else items[0].model_dump()\n            return json.dumps(payload, **json_kwargs)\n\n        except Exception as e:\n            # Wrap exceptions\n            raise ParseError(f\"Error generating JSON: {e}\")\n</code></pre>"},{"location":"api/adapters/#toml-adapter","title":"TOML Adapter","text":""},{"location":"api/adapters/#pydapter.adapters.toml_","title":"<code>pydapter.adapters.toml_</code>","text":"<p>TOML Adapter for Pydantic Models.</p> <p>This module provides the TomlAdapter class for converting between Pydantic models and TOML data formats. It supports reading from TOML files or strings and writing Pydantic models to TOML format.</p>"},{"location":"api/adapters/#pydapter.adapters.toml_-classes","title":"Classes","text":""},{"location":"api/adapters/#pydapter.adapters.toml_.TomlAdapter","title":"<code>TomlAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Adapter for converting between Pydantic models and TOML data.</p> <p>This adapter handles TOML files and strings, providing methods to: - Parse TOML data into Pydantic model instances - Convert Pydantic models to TOML format - Handle both single objects and arrays of objects</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"toml\")</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.adapters.toml_ import TomlAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Parse TOML data\ntoml_data = '''\nname = \"John\"\nage = 30\n'''\nperson = TomlAdapter.from_obj(Person, toml_data)\n\n# Parse TOML array\ntoml_array = '''\n[[people]]\nname = \"John\"\nage = 30\n\n[[people]]\nname = \"Jane\"\nage = 25\n'''\npeople = TomlAdapter.from_obj(Person, toml_array, many=True)\n\n# Convert to TOML\ntoml_output = TomlAdapter.to_obj(person)\n</code></pre> Source code in <code>src/pydapter/adapters/toml_.py</code> <pre><code>class TomlAdapter(Adapter[T]):\n    \"\"\"\n    Adapter for converting between Pydantic models and TOML data.\n\n    This adapter handles TOML files and strings, providing methods to:\n    - Parse TOML data into Pydantic model instances\n    - Convert Pydantic models to TOML format\n    - Handle both single objects and arrays of objects\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"toml\")\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.adapters.toml_ import TomlAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Parse TOML data\n        toml_data = '''\n        name = \"John\"\n        age = 30\n        '''\n        person = TomlAdapter.from_obj(Person, toml_data)\n\n        # Parse TOML array\n        toml_array = '''\n        [[people]]\n        name = \"John\"\n        age = 30\n\n        [[people]]\n        name = \"Jane\"\n        age = 25\n        '''\n        people = TomlAdapter.from_obj(Person, toml_array, many=True)\n\n        # Convert to TOML\n        toml_output = TomlAdapter.to_obj(person)\n        ```\n    \"\"\"\n\n    obj_key = \"toml\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: str | Path, /, *, many=False, **kw):\n        try:\n            # Handle file path\n            if isinstance(obj, Path):\n                try:\n                    text = Path(obj).read_text()\n                except Exception as e:\n                    raise ParseError(f\"Failed to read TOML file: {e}\", source=str(obj))\n            else:\n                text = obj\n\n            # Check for empty input\n            if not text or (isinstance(text, str) and not text.strip()):\n                raise ParseError(\n                    \"Empty TOML content\",\n                    source=str(obj)[:100] if isinstance(obj, str) else str(obj),\n                )\n\n            # Parse TOML\n            try:\n                parsed = toml.loads(text, **kw)\n            except toml.TomlDecodeError as e:\n                raise ParseError(\n                    f\"Invalid TOML: {e}\",\n                    source=str(text)[:100] if isinstance(text, str) else str(text),\n                )\n\n            # Validate against model\n            try:\n                if many:\n                    return [subj_cls.model_validate(x) for x in _ensure_list(parsed)]\n                return subj_cls.model_validate(parsed)\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=parsed,\n                    errors=e.errors(),\n                )\n\n        except (ParseError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ParseError(\n                f\"Unexpected error parsing TOML: {e}\",\n                source=str(obj)[:100] if isinstance(obj, str) else str(obj),\n            )\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many=False, **kw) -&gt; str:\n        try:\n            items = subj if isinstance(subj, list) else [subj]\n\n            if not items:\n                return \"\"\n\n            payload = (\n                {\"items\": [i.model_dump() for i in items]}\n                if many\n                else items[0].model_dump()\n            )\n            return toml.dumps(payload, **kw)\n\n        except Exception as e:\n            # Wrap exceptions\n            raise ParseError(f\"Error generating TOML: {e}\")\n</code></pre>"},{"location":"api/core/","title":"Core API Reference","text":"<p>The <code>pydapter.core</code> module provides the foundational adapter system for converting between Pydantic models and various data formats. It implements a registry-based pattern that enables stateless, bidirectional data transformations.</p>"},{"location":"api/core/#installation","title":"Installation","text":"<pre><code>pip install pydapter\n</code></pre>"},{"location":"api/core/#overview","title":"Overview","text":"<p>The core module establishes the fundamental concepts of pydapter:</p> <ul> <li>Adapter Protocol: Defines the interface for data conversion</li> <li>Registry System: Manages and discovers adapters</li> <li>Adaptable Mixin: Provides convenient model integration</li> <li>Error Handling: Comprehensive exception hierarchy for debugging</li> </ul> <pre><code>Core Architecture:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Adapter     \u2502  \u2502 AdapterRegistry \u2502  \u2502    Adaptable    \u2502\n\u2502   (Protocol)    \u2502  \u2502   (Manager)     \u2502  \u2502    (Mixin)      \u2502\n\u2502                 \u2502  \u2502                 \u2502  \u2502                 \u2502\n\u2502 from_obj()      \u2502  \u2502 register()      \u2502  \u2502 adapt_from()    \u2502\n\u2502 to_obj()        \u2502  \u2502 get()           \u2502  \u2502 adapt_to()      \u2502\n\u2502 obj_key         \u2502  \u2502 adapt_from()    \u2502  \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 adapt_to()      \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The system supports both synchronous and asynchronous operations through parallel implementations in <code>pydapter.core</code> and <code>pydapter.async_core</code>.</p>"},{"location":"api/core/#core-protocols","title":"Core Protocols","text":""},{"location":"api/core/#adapter","title":"Adapter","text":"<p>Module: <code>pydapter.core</code></p> <p>Defines the interface for stateless data conversion between Pydantic models and external formats.</p> <p>Protocol Interface:</p> <pre><code>@runtime_checkable\nclass Adapter(Protocol[T]):\n    \"\"\"Stateless conversion helper.\"\"\"\n\n    obj_key: ClassVar[str]  # Unique identifier for the adapter\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: Any, /, *, many: bool = False, **kw): ...\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw): ...\n</code></pre> <p>Key Concepts:</p> <ul> <li>Stateless: Adapters should not maintain internal state</li> <li>Bidirectional: Support both <code>from_obj</code> (import) and <code>to_obj</code> (export)</li> <li>Type-safe: Use generic typing for type safety</li> <li>Batch Support: Handle single items or collections via <code>many</code> parameter</li> </ul> <p>Implementation Example:</p> <pre><code>from pydapter.core import Adapter\nfrom pydantic import BaseModel\nimport json\n\nclass JSONAdapter(Adapter):\n    obj_key = \"json\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: str, /, *, many: bool = False, **kw):\n        \"\"\"Convert JSON string to Pydantic model(s).\"\"\"\n        data = json.loads(obj)\n        if many:\n            return [subj_cls.model_validate(item) for item in data]\n        return subj_cls.model_validate(data)\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw):\n        \"\"\"Convert Pydantic model(s) to JSON string.\"\"\"\n        if many or isinstance(subj, list):\n            data = [item.model_dump() for item in subj]\n        else:\n            data = subj.model_dump()\n        return json.dumps(data, **kw)\n\n# Usage\nclass User(BaseModel):\n    name: str\n    email: str\n\njson_data = '{\"name\": \"John\", \"email\": \"john@example.com\"}'\nuser = JSONAdapter.from_obj(User, json_data)\nback_to_json = JSONAdapter.to_obj(user)\n</code></pre>"},{"location":"api/core/#asyncadapter","title":"AsyncAdapter","text":"<p>Module: <code>pydapter.async_core</code></p> <p>Asynchronous counterpart to the <code>Adapter</code> protocol for operations requiring async/await.</p> <p>Protocol Interface:</p> <pre><code>@runtime_checkable\nclass AsyncAdapter(Protocol[T]):\n    \"\"\"Stateless, **async** conversion helper.\"\"\"\n\n    obj_key: ClassVar[str]\n\n    @classmethod\n    async def from_obj(\n        cls, subj_cls: type[T], obj: Any, /, *, many: bool = False, **kw\n    ) -&gt; T | list[T]: ...\n\n    @classmethod\n    async def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw) -&gt; Any: ...\n</code></pre> <p>Implementation Example:</p> <pre><code>from pydapter.async_core import AsyncAdapter\nimport aiohttp\nimport json\n\nclass HTTPAPIAdapter(AsyncAdapter):\n    obj_key = \"http_api\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: str, /, *, many: bool = False, **kw):\n        \"\"\"Fetch data from HTTP API and convert to model(s).\"\"\"\n        async with aiohttp.ClientSession() as session:\n            async with session.get(obj) as response:\n                data = await response.json()\n                if many:\n                    return [subj_cls.model_validate(item) for item in data]\n                return subj_cls.model_validate(data)\n\n    @classmethod\n    async def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw):\n        \"\"\"Convert model(s) to API payload.\"\"\"\n        if many or isinstance(subj, list):\n            return [item.model_dump() for item in subj]\n        return subj.model_dump()\n\n# Usage\nusers = await HTTPAPIAdapter.from_obj(User, \"https://api.example.com/users\", many=True)\n</code></pre>"},{"location":"api/core/#registry-system","title":"Registry System","text":""},{"location":"api/core/#adapterregistry","title":"AdapterRegistry","text":"<p>Module: <code>pydapter.core</code></p> <p>Manages adapter registration and provides convenient access methods.</p> <p>Class Interface:</p> <pre><code>class AdapterRegistry:\n    def __init__(self) -&gt; None: ...\n\n    def register(self, adapter_cls: type[Adapter]) -&gt; None: ...\n    def get(self, obj_key: str) -&gt; type[Adapter]: ...\n    def adapt_from(self, subj_cls: type[T], obj, *, obj_key: str, **kw): ...\n    def adapt_to(self, subj, *, obj_key: str, **kw): ...\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.core import AdapterRegistry\n\n# Create registry\nregistry = AdapterRegistry()\n\n# Register adapters\nregistry.register(JSONAdapter)\nregistry.register(CSVAdapter)\n\n# Use via registry\nuser = registry.adapt_from(User, json_data, obj_key=\"json\")\ncsv_data = registry.adapt_to(user, obj_key=\"csv\")\n\n# Direct adapter access\nadapter_cls = registry.get(\"json\")\nuser = adapter_cls.from_obj(User, json_data)\n</code></pre> <p>Error Handling:</p> <p>The registry provides comprehensive error handling:</p> <pre><code>from pydapter.exceptions import AdapterNotFoundError, AdapterError\n\ntry:\n    user = registry.adapt_from(User, data, obj_key=\"unknown\")\nexcept AdapterNotFoundError as e:\n    print(f\"No adapter found: {e}\")\nexcept AdapterError as e:\n    print(f\"Adaptation failed: {e}\")\n</code></pre>"},{"location":"api/core/#asyncadapterregistry","title":"AsyncAdapterRegistry","text":"<p>Module: <code>pydapter.async_core</code></p> <p>Asynchronous version of <code>AdapterRegistry</code> for async adapters.</p> <p>Usage:</p> <pre><code>from pydapter.async_core import AsyncAdapterRegistry\n\n# Create async registry\nasync_registry = AsyncAdapterRegistry()\nasync_registry.register(HTTPAPIAdapter)\n\n# Use with async/await\nusers = await async_registry.adapt_from(User, api_url, obj_key=\"http_api\", many=True)\n</code></pre>"},{"location":"api/core/#adaptable-mixin","title":"Adaptable Mixin","text":""},{"location":"api/core/#adaptable","title":"Adaptable","text":"<p>Module: <code>pydapter.core</code></p> <p>Mixin class that integrates adapter functionality directly into Pydantic models.</p> <p>Class Interface:</p> <pre><code>class Adaptable:\n    \"\"\"Mixin that endows any Pydantic model with adapt-from / adapt-to.\"\"\"\n\n    _adapter_registry: ClassVar[AdapterRegistry | None] = None\n\n    @classmethod\n    def _registry(cls) -&gt; AdapterRegistry: ...\n\n    @classmethod\n    def register_adapter(cls, adapter_cls: type[Adapter]) -&gt; None: ...\n\n    @classmethod\n    def adapt_from(cls, obj, *, obj_key: str, **kw): ...\n\n    def adapt_to(self, *, obj_key: str, **kw): ...\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.core import Adaptable\nfrom pydantic import BaseModel\n\nclass User(BaseModel, Adaptable):\n    name: str\n    email: str\n    age: int\n\n# Register adapters for this model\nUser.register_adapter(JSONAdapter)\nUser.register_adapter(CSVAdapter)\n\n# Use adapter methods directly on the model\nuser = User.adapt_from(json_data, obj_key=\"json\")\ncsv_output = user.adapt_to(obj_key=\"csv\")\n\n# Class method for creating from external data\nusers = User.adapt_from(csv_file_content, obj_key=\"csv\", many=True)\n</code></pre> <p>Advanced Usage:</p> <pre><code># Custom model with multiple adapters\nclass Product(BaseModel, Adaptable):\n    id: str\n    name: str\n    price: float\n    category: str\n\n# Register multiple adapters\nProduct.register_adapter(JSONAdapter)\nProduct.register_adapter(XMLAdapter)\nProduct.register_adapter(DatabaseAdapter)\n\n# Chain conversions\nproduct = Product.adapt_from(xml_data, obj_key=\"xml\")\njson_data = product.adapt_to(obj_key=\"json\")\ndatabase_record = product.adapt_to(obj_key=\"database\")\n</code></pre>"},{"location":"api/core/#exception-hierarchy","title":"Exception Hierarchy","text":""},{"location":"api/core/#core-exceptions","title":"Core Exceptions","text":"<p>Module: <code>pydapter.exceptions</code></p> <p>Comprehensive exception system for error handling and debugging.</p>"},{"location":"api/core/#adaptererror","title":"AdapterError","text":"<p>Base exception for all pydapter errors.</p> <pre><code>class AdapterError(Exception):\n    \"\"\"Base exception for all pydapter errors.\"\"\"\n\n    def __init__(self, message: str, **context: Any):\n        super().__init__(message)\n        self.message = message\n        self.context = context  # Additional error context\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.exceptions import AdapterError\n\ntry:\n    result = adapter.from_obj(Model, invalid_data)\nexcept AdapterError as e:\n    print(f\"Error: {e.message}\")\n    print(f\"Context: {e.context}\")\n</code></pre>"},{"location":"api/core/#validationerror","title":"ValidationError","text":"<p>Exception for data validation failures.</p> <pre><code>class ValidationError(AdapterError):\n    \"\"\"Exception raised when data validation fails.\"\"\"\n\n    def __init__(self, message: str, data: Optional[Any] = None, **context: Any):\n        super().__init__(message, **context)\n        self.data = data  # The data that failed validation\n</code></pre>"},{"location":"api/core/#typeconversionerror","title":"TypeConversionError","text":"<p>Exception for type conversion failures.</p> <pre><code>class TypeConversionError(ValidationError):\n    \"\"\"Exception raised when type conversion fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        source_type: Optional[type] = None,\n        target_type: Optional[type] = None,\n        field_name: Optional[str] = None,\n        model_name: Optional[str] = None,\n        **context: Any,\n    ): ...\n</code></pre>"},{"location":"api/core/#adapternotfounderror","title":"AdapterNotFoundError","text":"<p>Exception when no adapter is registered for a given key.</p> <pre><code>from pydapter.exceptions import AdapterNotFoundError\n\ntry:\n    adapter = registry.get(\"nonexistent\")\nexcept AdapterNotFoundError as e:\n    print(f\"Adapter not found: {e}\")\n</code></pre>"},{"location":"api/core/#configurationerror","title":"ConfigurationError","text":"<p>Exception for adapter configuration issues.</p> <pre><code>from pydapter.exceptions import ConfigurationError\n\nclass BadAdapter:\n    # Missing obj_key will raise ConfigurationError\n    pass\n\ntry:\n    registry.register(BadAdapter)\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"api/core/#advanced-usage-patterns","title":"Advanced Usage Patterns","text":""},{"location":"api/core/#custom-adapter-development","title":"Custom Adapter Development","text":"<p>Create specialized adapters for specific use cases:</p> <pre><code>from pydapter.core import Adapter\nfrom typing import Any, TypeVar\nimport yaml\n\nT = TypeVar(\"T\")\n\nclass YAMLAdapter(Adapter[T]):\n    obj_key = \"yaml\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: str, /, *, many: bool = False, **kw):\n        \"\"\"Convert YAML string to Pydantic model(s).\"\"\"\n        data = yaml.safe_load(obj)\n        if many:\n            return [subj_cls.model_validate(item) for item in data]\n        return subj_cls.model_validate(data)\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw):\n        \"\"\"Convert Pydantic model(s) to YAML string.\"\"\"\n        if many or isinstance(subj, list):\n            data = [item.model_dump() for item in subj]\n        else:\n            data = subj.model_dump()\n        return yaml.dump(data, **kw)\n</code></pre>"},{"location":"api/core/#adapter-composition","title":"Adapter Composition","text":"<p>Combine multiple adapters for complex workflows:</p> <pre><code>class DataPipeline:\n    def __init__(self, model_cls, registry: AdapterRegistry):\n        self.model_cls = model_cls\n        self.registry = registry\n\n    def transform(self, data, from_format: str, to_format: str, **kw):\n        \"\"\"Transform data from one format to another via Pydantic model.\"\"\"\n        # Parse input format to model\n        model_instance = self.registry.adapt_from(\n            self.model_cls, data, obj_key=from_format, **kw\n        )\n\n        # Convert model to output format\n        return self.registry.adapt_to(\n            model_instance, obj_key=to_format, **kw\n        )\n\n# Usage\npipeline = DataPipeline(User, registry)\njson_data = pipeline.transform(csv_data, \"csv\", \"json\")\n</code></pre>"},{"location":"api/core/#error-recovery","title":"Error Recovery","text":"<p>Implement robust error handling with fallbacks:</p> <pre><code>def safe_adapt_from(model_cls, data, primary_key: str, fallback_key: str, registry):\n    \"\"\"Attempt adaptation with fallback on failure.\"\"\"\n    try:\n        return registry.adapt_from(model_cls, data, obj_key=primary_key)\n    except AdapterError as e:\n        print(f\"Primary adapter {primary_key} failed: {e}\")\n        try:\n            return registry.adapt_from(model_cls, data, obj_key=fallback_key)\n        except AdapterError as fallback_error:\n            print(f\"Fallback adapter {fallback_key} also failed: {fallback_error}\")\n            raise AdapterError(\n                f\"Both {primary_key} and {fallback_key} adapters failed\",\n                primary_error=str(e),\n                fallback_error=str(fallback_error)\n            )\n\n# Usage\nuser = safe_adapt_from(User, data, \"json\", \"yaml\", registry)\n</code></pre>"},{"location":"api/core/#best-practices","title":"Best Practices","text":""},{"location":"api/core/#adapter-design","title":"Adapter Design","text":"<ol> <li>Stateless Design: Keep adapters stateless for thread safety</li> <li>Clear obj_key: Use descriptive, unique keys for adapter identification</li> <li>Error Handling: Provide meaningful error messages with context</li> <li>Type Safety: Use proper type hints and validation</li> <li>Documentation: Document expected input/output formats</li> </ol>"},{"location":"api/core/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Lazy Loading: Register adapters only when needed</li> <li>Batch Processing: Use <code>many=True</code> for collections</li> <li>Caching: Cache registry lookups for frequently used adapters</li> <li>Memory Management: Be mindful of memory usage with large datasets</li> </ol>"},{"location":"api/core/#registry-management","title":"Registry Management","text":"<ol> <li>Global Registry: Use a single global registry for consistency</li> <li>Namespace Keys: Use namespaced keys to avoid conflicts (e.g., \"db.postgres\")</li> <li>Validation: Validate adapter implementations before registration</li> <li>Testing: Test all registered adapters thoroughly</li> </ol>"},{"location":"api/core/#error-handling","title":"Error Handling","text":"<ol> <li>Specific Exceptions: Use specific exception types for different error conditions</li> <li>Context Information: Include relevant context in exception details</li> <li>Logging: Log adapter errors for debugging</li> <li>Recovery Strategies: Implement fallback mechanisms where appropriate</li> </ol>"},{"location":"api/core/#integration-examples","title":"Integration Examples","text":""},{"location":"api/core/#database-integration","title":"Database Integration","text":"<pre><code>from pydapter.core import Adapter\nimport sqlite3\n\nclass SQLiteAdapter(Adapter):\n    obj_key = \"sqlite\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: str, /, *, many: bool = False, **kw):\n        \"\"\"Load from SQLite database.\"\"\"\n        conn = sqlite3.connect(kw.get('database', ':memory:'))\n        conn.row_factory = sqlite3.Row\n        cursor = conn.cursor()\n\n        if many:\n            cursor.execute(f\"SELECT * FROM {kw.get('table', subj_cls.__name__.lower())}\")\n            rows = cursor.fetchall()\n            return [subj_cls.model_validate(dict(row)) for row in rows]\n        else:\n            cursor.execute(\n                f\"SELECT * FROM {kw.get('table', subj_cls.__name__.lower())} WHERE id = ?\",\n                (kw.get('id'),)\n            )\n            row = cursor.fetchone()\n            return subj_cls.model_validate(dict(row)) if row else None\n</code></pre>"},{"location":"api/core/#web-api-integration","title":"Web API Integration","text":"<pre><code>from pydapter.async_core import AsyncAdapter\nimport httpx\n\nclass RESTAPIAdapter(AsyncAdapter):\n    obj_key = \"rest_api\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: str, /, *, many: bool = False, **kw):\n        \"\"\"Fetch from REST API.\"\"\"\n        async with httpx.AsyncClient() as client:\n            response = await client.get(obj, params=kw.get('params', {}))\n            response.raise_for_status()\n            data = response.json()\n\n            if many:\n                return [subj_cls.model_validate(item) for item in data]\n            return subj_cls.model_validate(data)\n\n    @classmethod\n    async def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw):\n        \"\"\"Post to REST API.\"\"\"\n        url = kw.get('url')\n        if not url:\n            raise ValueError(\"URL required for REST API adapter\")\n\n        async with httpx.AsyncClient() as client:\n            if many or isinstance(subj, list):\n                data = [item.model_dump() for item in subj]\n            else:\n                data = subj.model_dump()\n\n            response = await client.post(url, json=data)\n            response.raise_for_status()\n            return response.json()\n</code></pre>"},{"location":"api/core/#migration-guide","title":"Migration Guide","text":"<p>When upgrading from previous versions:</p> <ol> <li>Adapter Interface: Update custom adapters to use new protocol interface</li> <li>Error Handling: Migrate to new exception hierarchy</li> <li>Registry Usage: Use <code>AdapterRegistry</code> for better organization</li> <li>Async Support: Consider migrating to async adapters for I/O operations</li> <li>Type Safety: Add proper type hints to existing adapters</li> </ol> <p>For detailed migration instructions, see the Migration Guide.</p>"},{"location":"api/core/#auto-generated-api-reference","title":"Auto-generated API Reference","text":"<p>The following sections contain auto-generated API documentation:</p>"},{"location":"api/core/#core-module","title":"Core Module","text":""},{"location":"api/core/#pydapter.core","title":"<code>pydapter.core</code>","text":"<p>pydapter.core - Adapter protocol, registry, Adaptable mix-in.</p>"},{"location":"api/core/#pydapter.core-classes","title":"Classes","text":""},{"location":"api/core/#pydapter.core.Adaptable","title":"<code>Adaptable</code>","text":"<p>Mixin class that adds adapter functionality to Pydantic models.</p> <p>This mixin provides convenient methods for converting to/from various data formats by maintaining a registry of adapters and providing high-level convenience methods.</p> <p>When mixed into a Pydantic model, it adds: - Class methods for registering adapters - Class methods for creating instances from external formats - Instance methods for converting to external formats</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.core import Adaptable\nfrom pydapter.adapters.json_ import JsonAdapter\n\nclass Person(BaseModel, Adaptable):\n    name: str\n    age: int\n\n# Register an adapter\nPerson.register_adapter(JsonAdapter)\n\n# Create from JSON\njson_data = '{\"name\": \"John\", \"age\": 30}'\nperson = Person.adapt_from(json_data, obj_key=\"json\")\n\n# Convert to JSON\njson_output = person.adapt_to(obj_key=\"json\")\n</code></pre> Source code in <code>src/pydapter/core.py</code> <pre><code>class Adaptable:\n    \"\"\"\n    Mixin class that adds adapter functionality to Pydantic models.\n\n    This mixin provides convenient methods for converting to/from various data formats\n    by maintaining a registry of adapters and providing high-level convenience methods.\n\n    When mixed into a Pydantic model, it adds:\n    - Class methods for registering adapters\n    - Class methods for creating instances from external formats\n    - Instance methods for converting to external formats\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.core import Adaptable\n        from pydapter.adapters.json_ import JsonAdapter\n\n        class Person(BaseModel, Adaptable):\n            name: str\n            age: int\n\n        # Register an adapter\n        Person.register_adapter(JsonAdapter)\n\n        # Create from JSON\n        json_data = '{\"name\": \"John\", \"age\": 30}'\n        person = Person.adapt_from(json_data, obj_key=\"json\")\n\n        # Convert to JSON\n        json_output = person.adapt_to(obj_key=\"json\")\n        ```\n    \"\"\"\n\n    _adapter_registry: ClassVar[AdapterRegistry | None] = None\n\n    @classmethod\n    def _registry(cls) -&gt; AdapterRegistry:\n        \"\"\"Get or create the adapter registry for this class.\"\"\"\n        if cls._adapter_registry is None:\n            cls._adapter_registry = AdapterRegistry()\n        return cls._adapter_registry\n\n    @classmethod\n    def register_adapter(cls, adapter_cls: type[Adapter]) -&gt; None:\n        \"\"\"\n        Register an adapter class with this model.\n\n        Args:\n            adapter_cls: The adapter class to register\n        \"\"\"\n        cls._registry().register(adapter_cls)\n\n    @classmethod\n    def adapt_from(cls, obj: Any, *, obj_key: str, **kw: Any) -&gt; Any:\n        \"\"\"\n        Create model instance(s) from external data format.\n\n        Args:\n            obj: The source data in the specified format\n            obj_key: The key identifying which adapter to use\n            **kw: Additional adapter-specific arguments\n\n        Returns:\n            Model instance(s) created from the source data\n        \"\"\"\n        return cls._registry().adapt_from(cls, obj, obj_key=obj_key, **kw)\n\n    def adapt_to(self, *, obj_key: str, **kw: Any) -&gt; Any:\n        \"\"\"\n        Convert this model instance to external data format.\n\n        Args:\n            obj_key: The key identifying which adapter to use\n            **kw: Additional adapter-specific arguments\n\n        Returns:\n            Data in the specified external format\n        \"\"\"\n        return self._registry().adapt_to(self, obj_key=obj_key, **kw)\n</code></pre>"},{"location":"api/core/#pydapter.core.Adaptable-functions","title":"Functions","text":""},{"location":"api/core/#pydapter.core.Adaptable.adapt_from","title":"<code>adapt_from(obj, *, obj_key, **kw)</code>  <code>classmethod</code>","text":"<p>Create model instance(s) from external data format.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The source data in the specified format</p> required <code>obj_key</code> <code>str</code> <p>The key identifying which adapter to use</p> required <code>**kw</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Model instance(s) created from the source data</p> Source code in <code>src/pydapter/core.py</code> <pre><code>@classmethod\ndef adapt_from(cls, obj: Any, *, obj_key: str, **kw: Any) -&gt; Any:\n    \"\"\"\n    Create model instance(s) from external data format.\n\n    Args:\n        obj: The source data in the specified format\n        obj_key: The key identifying which adapter to use\n        **kw: Additional adapter-specific arguments\n\n    Returns:\n        Model instance(s) created from the source data\n    \"\"\"\n    return cls._registry().adapt_from(cls, obj, obj_key=obj_key, **kw)\n</code></pre>"},{"location":"api/core/#pydapter.core.Adaptable.adapt_to","title":"<code>adapt_to(*, obj_key, **kw)</code>","text":"<p>Convert this model instance to external data format.</p> <p>Parameters:</p> Name Type Description Default <code>obj_key</code> <code>str</code> <p>The key identifying which adapter to use</p> required <code>**kw</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Data in the specified external format</p> Source code in <code>src/pydapter/core.py</code> <pre><code>def adapt_to(self, *, obj_key: str, **kw: Any) -&gt; Any:\n    \"\"\"\n    Convert this model instance to external data format.\n\n    Args:\n        obj_key: The key identifying which adapter to use\n        **kw: Additional adapter-specific arguments\n\n    Returns:\n        Data in the specified external format\n    \"\"\"\n    return self._registry().adapt_to(self, obj_key=obj_key, **kw)\n</code></pre>"},{"location":"api/core/#pydapter.core.Adaptable.register_adapter","title":"<code>register_adapter(adapter_cls)</code>  <code>classmethod</code>","text":"<p>Register an adapter class with this model.</p> <p>Parameters:</p> Name Type Description Default <code>adapter_cls</code> <code>type[Adapter]</code> <p>The adapter class to register</p> required Source code in <code>src/pydapter/core.py</code> <pre><code>@classmethod\ndef register_adapter(cls, adapter_cls: type[Adapter]) -&gt; None:\n    \"\"\"\n    Register an adapter class with this model.\n\n    Args:\n        adapter_cls: The adapter class to register\n    \"\"\"\n    cls._registry().register(adapter_cls)\n</code></pre>"},{"location":"api/core/#pydapter.core.Adapter","title":"<code>Adapter</code>","text":"<p>               Bases: <code>Protocol[T]</code></p> <p>Protocol defining the interface for data format adapters.</p> <p>Adapters are stateless conversion helpers that transform data between Pydantic models and various formats (CSV, JSON, TOML, etc.).</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <code>str</code> <p>Unique identifier for the adapter type (e.g., \"csv\", \"json\")</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.adapters.json_ import JsonAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Convert from JSON to Pydantic model\njson_data = '{\"name\": \"John\", \"age\": 30}'\nperson = JsonAdapter.from_obj(Person, json_data)\n\n# Convert from Pydantic model to JSON\njson_output = JsonAdapter.to_obj(person)\n</code></pre> Source code in <code>src/pydapter/core.py</code> <pre><code>@runtime_checkable\nclass Adapter(Protocol[T]):\n    \"\"\"\n    Protocol defining the interface for data format adapters.\n\n    Adapters are stateless conversion helpers that transform data between\n    Pydantic models and various formats (CSV, JSON, TOML, etc.).\n\n    Attributes:\n        obj_key: Unique identifier for the adapter type (e.g., \"csv\", \"json\")\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.adapters.json_ import JsonAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Convert from JSON to Pydantic model\n        json_data = '{\"name\": \"John\", \"age\": 30}'\n        person = JsonAdapter.from_obj(Person, json_data)\n\n        # Convert from Pydantic model to JSON\n        json_output = JsonAdapter.to_obj(person)\n        ```\n    \"\"\"\n\n    obj_key: ClassVar[str]\n\n    @classmethod\n    def from_obj(\n        cls, subj_cls: type[T], obj: Any, /, *, many: bool = False, **kw: Any\n    ) -&gt; T | list[T]:\n        \"\"\"\n        Convert from external format to Pydantic model instances.\n\n        Args:\n            subj_cls: The Pydantic model class to instantiate\n            obj: The source data in the adapter's format\n            many: If True, expect/return a list of instances\n            **kw: Additional adapter-specific arguments\n\n        Returns:\n            Single model instance or list of instances based on 'many' parameter\n        \"\"\"\n        ...\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw: Any) -&gt; Any:\n        \"\"\"\n        Convert from Pydantic model instances to external format.\n\n        Args:\n            subj: Single model instance or list of instances\n            many: If True, handle as list of instances\n            **kw: Additional adapter-specific arguments\n\n        Returns:\n            Data in the adapter's external format\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/core/#pydapter.core.Adapter-functions","title":"Functions","text":""},{"location":"api/core/#pydapter.core.Adapter.from_obj","title":"<code>from_obj(subj_cls, obj, /, *, many=False, **kw)</code>  <code>classmethod</code>","text":"<p>Convert from external format to Pydantic model instances.</p> <p>Parameters:</p> Name Type Description Default <code>subj_cls</code> <code>type[T]</code> <p>The Pydantic model class to instantiate</p> required <code>obj</code> <code>Any</code> <p>The source data in the adapter's format</p> required <code>many</code> <code>bool</code> <p>If True, expect/return a list of instances</p> <code>False</code> <code>**kw</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>T | list[T]</code> <p>Single model instance or list of instances based on 'many' parameter</p> Source code in <code>src/pydapter/core.py</code> <pre><code>@classmethod\ndef from_obj(\n    cls, subj_cls: type[T], obj: Any, /, *, many: bool = False, **kw: Any\n) -&gt; T | list[T]:\n    \"\"\"\n    Convert from external format to Pydantic model instances.\n\n    Args:\n        subj_cls: The Pydantic model class to instantiate\n        obj: The source data in the adapter's format\n        many: If True, expect/return a list of instances\n        **kw: Additional adapter-specific arguments\n\n    Returns:\n        Single model instance or list of instances based on 'many' parameter\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/#pydapter.core.Adapter.to_obj","title":"<code>to_obj(subj, /, *, many=False, **kw)</code>  <code>classmethod</code>","text":"<p>Convert from Pydantic model instances to external format.</p> <p>Parameters:</p> Name Type Description Default <code>subj</code> <code>T | list[T]</code> <p>Single model instance or list of instances</p> required <code>many</code> <code>bool</code> <p>If True, handle as list of instances</p> <code>False</code> <code>**kw</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Data in the adapter's external format</p> Source code in <code>src/pydapter/core.py</code> <pre><code>@classmethod\ndef to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw: Any) -&gt; Any:\n    \"\"\"\n    Convert from Pydantic model instances to external format.\n\n    Args:\n        subj: Single model instance or list of instances\n        many: If True, handle as list of instances\n        **kw: Additional adapter-specific arguments\n\n    Returns:\n        Data in the adapter's external format\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/#pydapter.core.AdapterRegistry","title":"<code>AdapterRegistry</code>","text":"<p>Registry for managing and accessing data format adapters.</p> <p>The registry maintains a mapping of adapter keys to adapter classes, providing a centralized way to register and retrieve adapters for different data formats.</p> Example <pre><code>from pydapter.core import AdapterRegistry\nfrom pydapter.adapters.json_ import JsonAdapter\n\nregistry = AdapterRegistry()\nregistry.register(JsonAdapter)\n\n# Use the registry to adapt data\njson_data = '{\"name\": \"John\", \"age\": 30}'\nperson = registry.adapt_from(Person, json_data, obj_key=\"json\")\n</code></pre> Source code in <code>src/pydapter/core.py</code> <pre><code>class AdapterRegistry:\n    \"\"\"\n    Registry for managing and accessing data format adapters.\n\n    The registry maintains a mapping of adapter keys to adapter classes,\n    providing a centralized way to register and retrieve adapters for\n    different data formats.\n\n    Example:\n        ```python\n        from pydapter.core import AdapterRegistry\n        from pydapter.adapters.json_ import JsonAdapter\n\n        registry = AdapterRegistry()\n        registry.register(JsonAdapter)\n\n        # Use the registry to adapt data\n        json_data = '{\"name\": \"John\", \"age\": 30}'\n        person = registry.adapt_from(Person, json_data, obj_key=\"json\")\n        ```\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize an empty adapter registry.\"\"\"\n        self._reg: dict[str, type[Adapter]] = {}\n\n    def register(self, adapter_cls: type[Adapter]) -&gt; None:\n        \"\"\"\n        Register an adapter class with the registry.\n\n        Args:\n            adapter_cls: The adapter class to register. Must have an 'obj_key' attribute.\n\n        Raises:\n            ConfigurationError: If the adapter class doesn't define 'obj_key'\n        \"\"\"\n        key = getattr(adapter_cls, \"obj_key\", None)\n        if not key:\n            raise ConfigurationError(\n                \"Adapter must define 'obj_key'\", adapter_cls=adapter_cls.__name__\n            )\n        self._reg[key] = adapter_cls\n\n    def get(self, obj_key: str) -&gt; type[Adapter]:\n        \"\"\"\n        Retrieve an adapter class by its key.\n\n        Args:\n            obj_key: The key identifier for the adapter\n\n        Returns:\n            The adapter class associated with the key\n\n        Raises:\n            AdapterNotFoundError: If no adapter is registered for the given key\n        \"\"\"\n        try:\n            return self._reg[obj_key]\n        except KeyError as exc:\n            raise AdapterNotFoundError(\n                f\"No adapter registered for '{obj_key}'\", obj_key=obj_key\n            ) from exc\n\n    def adapt_from(\n        self, subj_cls: type[T], obj: Any, *, obj_key: str, **kw: Any\n    ) -&gt; T | list[T]:\n        \"\"\"\n        Convenience method to convert from external format to Pydantic model.\n\n        Args:\n            subj_cls: The Pydantic model class to instantiate\n            obj: The source data in the specified format\n            obj_key: The key identifying which adapter to use\n            **kw: Additional adapter-specific arguments\n\n        Returns:\n            Model instance(s) created from the source data\n\n        Raises:\n            AdapterNotFoundError: If no adapter is registered for obj_key\n            AdapterError: If the adaptation process fails\n        \"\"\"\n        try:\n            result = self.get(obj_key).from_obj(subj_cls, obj, **kw)\n            if result is None:\n                raise AdapterError(f\"Adapter {obj_key} returned None\", adapter=obj_key)\n            return result\n\n        except Exception as exc:\n            if isinstance(exc, AdapterError) or isinstance(exc, PYDAPTER_PYTHON_ERRORS):\n                raise\n\n            raise AdapterError(\n                f\"Error adapting from {obj_key}\", original_error=str(exc)\n            ) from exc\n\n    def adapt_to(self, subj: Any, *, obj_key: str, **kw: Any) -&gt; Any:\n        \"\"\"\n        Convenience method to convert from Pydantic model to external format.\n\n        Args:\n            subj: The model instance(s) to convert\n            obj_key: The key identifying which adapter to use\n            **kw: Additional adapter-specific arguments\n\n        Returns:\n            Data in the specified external format\n\n        Raises:\n            AdapterNotFoundError: If no adapter is registered for obj_key\n            AdapterError: If the adaptation process fails\n        \"\"\"\n        try:\n            result = self.get(obj_key).to_obj(subj, **kw)\n            if result is None:\n                raise AdapterError(f\"Adapter {obj_key} returned None\", adapter=obj_key)\n            return result\n\n        except Exception as exc:\n            if isinstance(exc, AdapterError) or isinstance(exc, PYDAPTER_PYTHON_ERRORS):\n                raise\n\n            raise AdapterError(\n                f\"Error adapting to {obj_key}\", original_error=str(exc)\n            ) from exc\n</code></pre>"},{"location":"api/core/#pydapter.core.AdapterRegistry-functions","title":"Functions","text":""},{"location":"api/core/#pydapter.core.AdapterRegistry.__init__","title":"<code>__init__()</code>","text":"<p>Initialize an empty adapter registry.</p> Source code in <code>src/pydapter/core.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize an empty adapter registry.\"\"\"\n    self._reg: dict[str, type[Adapter]] = {}\n</code></pre>"},{"location":"api/core/#pydapter.core.AdapterRegistry.adapt_from","title":"<code>adapt_from(subj_cls, obj, *, obj_key, **kw)</code>","text":"<p>Convenience method to convert from external format to Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>subj_cls</code> <code>type[T]</code> <p>The Pydantic model class to instantiate</p> required <code>obj</code> <code>Any</code> <p>The source data in the specified format</p> required <code>obj_key</code> <code>str</code> <p>The key identifying which adapter to use</p> required <code>**kw</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>T | list[T]</code> <p>Model instance(s) created from the source data</p> <p>Raises:</p> Type Description <code>AdapterNotFoundError</code> <p>If no adapter is registered for obj_key</p> <code>AdapterError</code> <p>If the adaptation process fails</p> Source code in <code>src/pydapter/core.py</code> <pre><code>def adapt_from(\n    self, subj_cls: type[T], obj: Any, *, obj_key: str, **kw: Any\n) -&gt; T | list[T]:\n    \"\"\"\n    Convenience method to convert from external format to Pydantic model.\n\n    Args:\n        subj_cls: The Pydantic model class to instantiate\n        obj: The source data in the specified format\n        obj_key: The key identifying which adapter to use\n        **kw: Additional adapter-specific arguments\n\n    Returns:\n        Model instance(s) created from the source data\n\n    Raises:\n        AdapterNotFoundError: If no adapter is registered for obj_key\n        AdapterError: If the adaptation process fails\n    \"\"\"\n    try:\n        result = self.get(obj_key).from_obj(subj_cls, obj, **kw)\n        if result is None:\n            raise AdapterError(f\"Adapter {obj_key} returned None\", adapter=obj_key)\n        return result\n\n    except Exception as exc:\n        if isinstance(exc, AdapterError) or isinstance(exc, PYDAPTER_PYTHON_ERRORS):\n            raise\n\n        raise AdapterError(\n            f\"Error adapting from {obj_key}\", original_error=str(exc)\n        ) from exc\n</code></pre>"},{"location":"api/core/#pydapter.core.AdapterRegistry.adapt_to","title":"<code>adapt_to(subj, *, obj_key, **kw)</code>","text":"<p>Convenience method to convert from Pydantic model to external format.</p> <p>Parameters:</p> Name Type Description Default <code>subj</code> <code>Any</code> <p>The model instance(s) to convert</p> required <code>obj_key</code> <code>str</code> <p>The key identifying which adapter to use</p> required <code>**kw</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Data in the specified external format</p> <p>Raises:</p> Type Description <code>AdapterNotFoundError</code> <p>If no adapter is registered for obj_key</p> <code>AdapterError</code> <p>If the adaptation process fails</p> Source code in <code>src/pydapter/core.py</code> <pre><code>def adapt_to(self, subj: Any, *, obj_key: str, **kw: Any) -&gt; Any:\n    \"\"\"\n    Convenience method to convert from Pydantic model to external format.\n\n    Args:\n        subj: The model instance(s) to convert\n        obj_key: The key identifying which adapter to use\n        **kw: Additional adapter-specific arguments\n\n    Returns:\n        Data in the specified external format\n\n    Raises:\n        AdapterNotFoundError: If no adapter is registered for obj_key\n        AdapterError: If the adaptation process fails\n    \"\"\"\n    try:\n        result = self.get(obj_key).to_obj(subj, **kw)\n        if result is None:\n            raise AdapterError(f\"Adapter {obj_key} returned None\", adapter=obj_key)\n        return result\n\n    except Exception as exc:\n        if isinstance(exc, AdapterError) or isinstance(exc, PYDAPTER_PYTHON_ERRORS):\n            raise\n\n        raise AdapterError(\n            f\"Error adapting to {obj_key}\", original_error=str(exc)\n        ) from exc\n</code></pre>"},{"location":"api/core/#pydapter.core.AdapterRegistry.get","title":"<code>get(obj_key)</code>","text":"<p>Retrieve an adapter class by its key.</p> <p>Parameters:</p> Name Type Description Default <code>obj_key</code> <code>str</code> <p>The key identifier for the adapter</p> required <p>Returns:</p> Type Description <code>type[Adapter]</code> <p>The adapter class associated with the key</p> <p>Raises:</p> Type Description <code>AdapterNotFoundError</code> <p>If no adapter is registered for the given key</p> Source code in <code>src/pydapter/core.py</code> <pre><code>def get(self, obj_key: str) -&gt; type[Adapter]:\n    \"\"\"\n    Retrieve an adapter class by its key.\n\n    Args:\n        obj_key: The key identifier for the adapter\n\n    Returns:\n        The adapter class associated with the key\n\n    Raises:\n        AdapterNotFoundError: If no adapter is registered for the given key\n    \"\"\"\n    try:\n        return self._reg[obj_key]\n    except KeyError as exc:\n        raise AdapterNotFoundError(\n            f\"No adapter registered for '{obj_key}'\", obj_key=obj_key\n        ) from exc\n</code></pre>"},{"location":"api/core/#pydapter.core.AdapterRegistry.register","title":"<code>register(adapter_cls)</code>","text":"<p>Register an adapter class with the registry.</p> <p>Parameters:</p> Name Type Description Default <code>adapter_cls</code> <code>type[Adapter]</code> <p>The adapter class to register. Must have an 'obj_key' attribute.</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If the adapter class doesn't define 'obj_key'</p> Source code in <code>src/pydapter/core.py</code> <pre><code>def register(self, adapter_cls: type[Adapter]) -&gt; None:\n    \"\"\"\n    Register an adapter class with the registry.\n\n    Args:\n        adapter_cls: The adapter class to register. Must have an 'obj_key' attribute.\n\n    Raises:\n        ConfigurationError: If the adapter class doesn't define 'obj_key'\n    \"\"\"\n    key = getattr(adapter_cls, \"obj_key\", None)\n    if not key:\n        raise ConfigurationError(\n            \"Adapter must define 'obj_key'\", adapter_cls=adapter_cls.__name__\n        )\n    self._reg[key] = adapter_cls\n</code></pre>"},{"location":"api/core/#async-core-module","title":"Async Core Module","text":""},{"location":"api/core/#pydapter.async_core","title":"<code>pydapter.async_core</code>","text":"<p>pydapter.async_core - async counterparts to the sync Adapter stack</p>"},{"location":"api/core/#pydapter.async_core-classes","title":"Classes","text":""},{"location":"api/core/#pydapter.async_core.AsyncAdaptable","title":"<code>AsyncAdaptable</code>","text":"<p>Mixin that endows any Pydantic model with async adapt-from / adapt-to.</p> Source code in <code>src/pydapter/async_core.py</code> <pre><code>class AsyncAdaptable:\n    \"\"\"\n    Mixin that endows any Pydantic model with async adapt-from / adapt-to.\n    \"\"\"\n\n    _async_registry: ClassVar[AsyncAdapterRegistry | None] = None\n\n    # registry access\n    @classmethod\n    def _areg(cls) -&gt; AsyncAdapterRegistry:\n        if cls._async_registry is None:\n            cls._async_registry = AsyncAdapterRegistry()\n        return cls._async_registry\n\n    @classmethod\n    def register_async_adapter(cls, adapter_cls: type[AsyncAdapter]) -&gt; None:\n        cls._areg().register(adapter_cls)\n\n    # helpers\n    @classmethod\n    async def adapt_from_async(cls, obj, *, obj_key: str, **kw):\n        return await cls._areg().adapt_from(cls, obj, obj_key=obj_key, **kw)\n\n    async def adapt_to_async(self, *, obj_key: str, **kw):\n        return await self._areg().adapt_to(self, obj_key=obj_key, **kw)\n</code></pre>"},{"location":"api/core/#pydapter.async_core.AsyncAdapter","title":"<code>AsyncAdapter</code>","text":"<p>               Bases: <code>Protocol[T]</code></p> <p>Stateless, async conversion helper.</p> Source code in <code>src/pydapter/async_core.py</code> <pre><code>@runtime_checkable\nclass AsyncAdapter(Protocol[T]):\n    \"\"\"Stateless, **async** conversion helper.\"\"\"\n\n    obj_key: ClassVar[str]\n\n    @classmethod\n    async def from_obj(\n        cls, subj_cls: type[T], obj: Any, /, *, many: bool = False, **kw\n    ) -&gt; T | list[T]: ...\n\n    @classmethod\n    async def to_obj(cls, subj: T | list[T], /, *, many: bool = False, **kw) -&gt; Any: ...\n</code></pre>"},{"location":"api/core/#exceptions","title":"Exceptions","text":""},{"location":"api/core/#pydapter.exceptions","title":"<code>pydapter.exceptions</code>","text":"<p>pydapter.exceptions - Custom exception hierarchy for pydapter.</p>"},{"location":"api/core/#pydapter.exceptions-classes","title":"Classes","text":""},{"location":"api/core/#pydapter.exceptions.AdapterError","title":"<code>AdapterError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all pydapter errors.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class AdapterError(Exception):\n    \"\"\"Base exception for all pydapter errors.\"\"\"\n\n    def __init__(self, message: str, **context: Any):\n        super().__init__(message)\n        self.message = message\n        self.context = context\n\n    def __str__(self) -&gt; str:\n        context_str = \", \".join(f\"{k}={v!r}\" for k, v in self.context.items())\n        if context_str:\n            return f\"{self.message} ({context_str})\"\n        return self.message\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.AdapterNotFoundError","title":"<code>AdapterNotFoundError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when an adapter is not found.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class AdapterNotFoundError(AdapterError):\n    \"\"\"Exception raised when an adapter is not found.\"\"\"\n\n    def __init__(self, message: str, obj_key: Optional[str] = None, **context: Any):\n        super().__init__(message, **context)\n        self.obj_key = obj_key\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.ConfigurationError","title":"<code>ConfigurationError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when adapter configuration is invalid.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class ConfigurationError(AdapterError):\n    \"\"\"Exception raised when adapter configuration is invalid.\"\"\"\n\n    def __init__(\n        self, message: str, config: Optional[dict[str, Any]] = None, **context: Any\n    ):\n        super().__init__(message, **context)\n        self.config = config\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.ConnectionError","title":"<code>ConnectionError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when a connection to a data source fails.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class ConnectionError(AdapterError):\n    \"\"\"Exception raised when a connection to a data source fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        adapter: Optional[str] = None,\n        url: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, **context)\n        self.adapter = adapter\n        self.url = url\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.ParseError","title":"<code>ParseError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when data parsing fails.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class ParseError(AdapterError):\n    \"\"\"Exception raised when data parsing fails.\"\"\"\n\n    def __init__(self, message: str, source: Optional[str] = None, **context: Any):\n        super().__init__(message, **context)\n        self.source = source\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.QueryError","title":"<code>QueryError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when a query to a data source fails.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class QueryError(AdapterError):\n    \"\"\"Exception raised when a query to a data source fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        query: Optional[Any] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, **context)\n        self.query = query\n        self.adapter = adapter\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.ResourceError","title":"<code>ResourceError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when a resource (file, database, etc.) cannot be accessed.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class ResourceError(AdapterError):\n    \"\"\"Exception raised when a resource (file, database, etc.) cannot be accessed.\"\"\"\n\n    def __init__(self, message: str, resource: Optional[str] = None, **context: Any):\n        super().__init__(message, **context)\n        self.resource = resource\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.TypeConversionError","title":"<code>TypeConversionError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Exception raised when type conversion fails.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class TypeConversionError(ValidationError):\n    \"\"\"Exception raised when type conversion fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        source_type: Optional[type] = None,\n        target_type: Optional[type] = None,\n        field_name: Optional[str] = None,\n        model_name: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, **context)\n        self.source_type = source_type\n        self.target_type = target_type\n        self.field_name = field_name\n        self.model_name = model_name\n</code></pre>"},{"location":"api/core/#pydapter.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Exception raised when data validation fails.</p> Source code in <code>src/pydapter/exceptions.py</code> <pre><code>class ValidationError(AdapterError):\n    \"\"\"Exception raised when data validation fails.\"\"\"\n\n    def __init__(self, message: str, data: Optional[Any] = None, **context: Any):\n        super().__init__(message, **context)\n        self.data = data\n</code></pre>"},{"location":"api/extras/","title":"Extras API","text":"<p>This page documents the extra adapters provided by pydapter.</p>"},{"location":"api/extras/#excel-adapter","title":"Excel Adapter","text":""},{"location":"api/extras/#pydapter.extras.excel_","title":"<code>pydapter.extras.excel_</code>","text":"<p>Excel adapter (requires pandas + xlsxwriter engine).</p>"},{"location":"api/extras/#pydapter.extras.excel_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.excel_.ExcelAdapter","title":"<code>ExcelAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Adapter for converting between Pydantic models and Excel files.</p> <p>This adapter handles Excel (.xlsx) files, providing methods to: - Read Excel files into Pydantic model instances - Write Pydantic models to Excel files - Support for different sheets and pandas read_excel options</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"xlsx\")</p> Example <pre><code>from pathlib import Path\nfrom pydantic import BaseModel\nfrom pydapter.extras.excel_ import ExcelAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Read from Excel file\nexcel_file = Path(\"people.xlsx\")\npeople = ExcelAdapter.from_obj(Person, excel_file, many=True)\n\n# Write to Excel file\noutput_bytes = ExcelAdapter.to_obj(people, many=True)\nwith open(\"output.xlsx\", \"wb\") as f:\n    f.write(output_bytes)\n</code></pre> Source code in <code>src/pydapter/extras/excel_.py</code> <pre><code>class ExcelAdapter(Adapter[T]):\n    \"\"\"\n    Adapter for converting between Pydantic models and Excel files.\n\n    This adapter handles Excel (.xlsx) files, providing methods to:\n    - Read Excel files into Pydantic model instances\n    - Write Pydantic models to Excel files\n    - Support for different sheets and pandas read_excel options\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"xlsx\")\n\n    Example:\n        ```python\n        from pathlib import Path\n        from pydantic import BaseModel\n        from pydapter.extras.excel_ import ExcelAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Read from Excel file\n        excel_file = Path(\"people.xlsx\")\n        people = ExcelAdapter.from_obj(Person, excel_file, many=True)\n\n        # Write to Excel file\n        output_bytes = ExcelAdapter.to_obj(people, many=True)\n        with open(\"output.xlsx\", \"wb\") as f:\n            f.write(output_bytes)\n        ```\n    \"\"\"\n\n    obj_key = \"xlsx\"\n\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: str | Path | bytes,\n        /,\n        *,\n        many: bool = True,\n        sheet_name: str | int = 0,\n        **kw: Any,\n    ) -&gt; T | list[T]:\n        \"\"\"\n        Convert Excel data to Pydantic model instances.\n\n        Args:\n            subj_cls: The Pydantic model class to instantiate\n            obj: Excel file path, file-like object, or bytes\n            many: If True, convert all rows; if False, convert only first row\n            sheet_name: Sheet name or index to read (default: 0)\n            **kw: Additional arguments passed to pandas.read_excel\n\n        Returns:\n            List of model instances if many=True, single instance if many=False\n\n        Raises:\n            ResourceError: If the Excel file cannot be read\n            AdapterError: If the data cannot be converted to models\n        \"\"\"\n        try:\n            if isinstance(obj, bytes):\n                df = pd.read_excel(io.BytesIO(obj), sheet_name=sheet_name, **kw)\n            else:\n                df = pd.read_excel(obj, sheet_name=sheet_name, **kw)\n            return DataFrameAdapter.from_obj(subj_cls, df, many=many)\n        except FileNotFoundError as e:\n            raise ResourceError(f\"File not found: {e}\", resource=str(obj)) from e\n        except ValueError as e:\n            raise AdapterError(\n                f\"Error adapting from xlsx (original_error='{e}')\", adapter=\"xlsx\"\n            ) from e\n        except Exception as e:\n            raise AdapterError(\n                f\"Unexpected error in Excel adapter: {e}\", adapter=\"xlsx\"\n            ) from e\n\n    # outgoing\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | list[T],\n        /,\n        *,\n        many=True,\n        sheet_name=\"Sheet1\",\n        **kw,\n    ) -&gt; bytes:\n        df = DataFrameAdapter.to_obj(subj, many=many)\n        buf = io.BytesIO()\n        with pd.ExcelWriter(buf, engine=\"xlsxwriter\") as wr:\n            df.to_excel(wr, sheet_name=sheet_name, index=False)\n        return buf.getvalue()\n</code></pre>"},{"location":"api/extras/#pydapter.extras.excel_.ExcelAdapter-functions","title":"Functions","text":""},{"location":"api/extras/#pydapter.extras.excel_.ExcelAdapter.from_obj","title":"<code>from_obj(subj_cls, obj, /, *, many=True, sheet_name=0, **kw)</code>  <code>classmethod</code>","text":"<p>Convert Excel data to Pydantic model instances.</p> <p>Parameters:</p> Name Type Description Default <code>subj_cls</code> <code>type[T]</code> <p>The Pydantic model class to instantiate</p> required <code>obj</code> <code>str | Path | bytes</code> <p>Excel file path, file-like object, or bytes</p> required <code>many</code> <code>bool</code> <p>If True, convert all rows; if False, convert only first row</p> <code>True</code> <code>sheet_name</code> <code>str | int</code> <p>Sheet name or index to read (default: 0)</p> <code>0</code> <code>**kw</code> <code>Any</code> <p>Additional arguments passed to pandas.read_excel</p> <code>{}</code> <p>Returns:</p> Type Description <code>T | list[T]</code> <p>List of model instances if many=True, single instance if many=False</p> <p>Raises:</p> Type Description <code>ResourceError</code> <p>If the Excel file cannot be read</p> <code>AdapterError</code> <p>If the data cannot be converted to models</p> Source code in <code>src/pydapter/extras/excel_.py</code> <pre><code>@classmethod\ndef from_obj(\n    cls,\n    subj_cls: type[T],\n    obj: str | Path | bytes,\n    /,\n    *,\n    many: bool = True,\n    sheet_name: str | int = 0,\n    **kw: Any,\n) -&gt; T | list[T]:\n    \"\"\"\n    Convert Excel data to Pydantic model instances.\n\n    Args:\n        subj_cls: The Pydantic model class to instantiate\n        obj: Excel file path, file-like object, or bytes\n        many: If True, convert all rows; if False, convert only first row\n        sheet_name: Sheet name or index to read (default: 0)\n        **kw: Additional arguments passed to pandas.read_excel\n\n    Returns:\n        List of model instances if many=True, single instance if many=False\n\n    Raises:\n        ResourceError: If the Excel file cannot be read\n        AdapterError: If the data cannot be converted to models\n    \"\"\"\n    try:\n        if isinstance(obj, bytes):\n            df = pd.read_excel(io.BytesIO(obj), sheet_name=sheet_name, **kw)\n        else:\n            df = pd.read_excel(obj, sheet_name=sheet_name, **kw)\n        return DataFrameAdapter.from_obj(subj_cls, df, many=many)\n    except FileNotFoundError as e:\n        raise ResourceError(f\"File not found: {e}\", resource=str(obj)) from e\n    except ValueError as e:\n        raise AdapterError(\n            f\"Error adapting from xlsx (original_error='{e}')\", adapter=\"xlsx\"\n        ) from e\n    except Exception as e:\n        raise AdapterError(\n            f\"Unexpected error in Excel adapter: {e}\", adapter=\"xlsx\"\n        ) from e\n</code></pre>"},{"location":"api/extras/#pandas-adapter","title":"Pandas Adapter","text":""},{"location":"api/extras/#pydapter.extras.pandas_","title":"<code>pydapter.extras.pandas_</code>","text":"<p>DataFrame &amp; Series adapters (require <code>pandas</code>).</p>"},{"location":"api/extras/#pydapter.extras.pandas_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.pandas_.DataFrameAdapter","title":"<code>DataFrameAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Adapter for converting between Pydantic models and pandas DataFrames.</p> <p>This adapter handles pandas DataFrame objects, providing methods to: - Convert DataFrame rows to Pydantic model instances - Convert Pydantic models to DataFrame rows - Handle both single records and multiple records</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"pd.DataFrame\")</p> Example <pre><code>import pandas as pd\nfrom pydantic import BaseModel\nfrom pydapter.extras.pandas_ import DataFrameAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Create DataFrame\ndf = pd.DataFrame([\n    {\"name\": \"John\", \"age\": 30},\n    {\"name\": \"Jane\", \"age\": 25}\n])\n\n# Convert to Pydantic models\npeople = DataFrameAdapter.from_obj(Person, df, many=True)\n\n# Convert back to DataFrame\ndf_output = DataFrameAdapter.to_obj(people, many=True)\n</code></pre> Source code in <code>src/pydapter/extras/pandas_.py</code> <pre><code>class DataFrameAdapter(Adapter[T]):\n    \"\"\"\n    Adapter for converting between Pydantic models and pandas DataFrames.\n\n    This adapter handles pandas DataFrame objects, providing methods to:\n    - Convert DataFrame rows to Pydantic model instances\n    - Convert Pydantic models to DataFrame rows\n    - Handle both single records and multiple records\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"pd.DataFrame\")\n\n    Example:\n        ```python\n        import pandas as pd\n        from pydantic import BaseModel\n        from pydapter.extras.pandas_ import DataFrameAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Create DataFrame\n        df = pd.DataFrame([\n            {\"name\": \"John\", \"age\": 30},\n            {\"name\": \"Jane\", \"age\": 25}\n        ])\n\n        # Convert to Pydantic models\n        people = DataFrameAdapter.from_obj(Person, df, many=True)\n\n        # Convert back to DataFrame\n        df_output = DataFrameAdapter.to_obj(people, many=True)\n        ```\n    \"\"\"\n\n    obj_key = \"pd.DataFrame\"\n\n    @classmethod\n    def from_obj(\n        cls, subj_cls: type[T], obj: pd.DataFrame, /, *, many: bool = True, **kw: Any\n    ) -&gt; T | list[T]:\n        \"\"\"\n        Convert DataFrame to Pydantic model instances.\n\n        Args:\n            subj_cls: The Pydantic model class to instantiate\n            obj: The pandas DataFrame to convert\n            many: If True, convert all rows; if False, convert only first row\n            **kw: Additional arguments passed to model_validate\n\n        Returns:\n            List of model instances if many=True, single instance if many=False\n        \"\"\"\n        if many:\n            return [subj_cls.model_validate(r) for r in obj.to_dict(orient=\"records\")]\n        return subj_cls.model_validate(obj.iloc[0].to_dict(), **kw)\n\n    @classmethod\n    def to_obj(\n        cls, subj: T | list[T], /, *, many: bool = True, **kw: Any\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert Pydantic model instances to pandas DataFrame.\n\n        Args:\n            subj: Single model instance or list of instances\n            many: If True, handle as multiple instances\n            **kw: Additional arguments passed to DataFrame constructor\n\n        Returns:\n            pandas DataFrame with model data\n        \"\"\"\n        items = subj if isinstance(subj, list) else [subj]\n        return pd.DataFrame([i.model_dump() for i in items], **kw)\n</code></pre>"},{"location":"api/extras/#pydapter.extras.pandas_.DataFrameAdapter-functions","title":"Functions","text":""},{"location":"api/extras/#pydapter.extras.pandas_.DataFrameAdapter.from_obj","title":"<code>from_obj(subj_cls, obj, /, *, many=True, **kw)</code>  <code>classmethod</code>","text":"<p>Convert DataFrame to Pydantic model instances.</p> <p>Parameters:</p> Name Type Description Default <code>subj_cls</code> <code>type[T]</code> <p>The Pydantic model class to instantiate</p> required <code>obj</code> <code>DataFrame</code> <p>The pandas DataFrame to convert</p> required <code>many</code> <code>bool</code> <p>If True, convert all rows; if False, convert only first row</p> <code>True</code> <code>**kw</code> <code>Any</code> <p>Additional arguments passed to model_validate</p> <code>{}</code> <p>Returns:</p> Type Description <code>T | list[T]</code> <p>List of model instances if many=True, single instance if many=False</p> Source code in <code>src/pydapter/extras/pandas_.py</code> <pre><code>@classmethod\ndef from_obj(\n    cls, subj_cls: type[T], obj: pd.DataFrame, /, *, many: bool = True, **kw: Any\n) -&gt; T | list[T]:\n    \"\"\"\n    Convert DataFrame to Pydantic model instances.\n\n    Args:\n        subj_cls: The Pydantic model class to instantiate\n        obj: The pandas DataFrame to convert\n        many: If True, convert all rows; if False, convert only first row\n        **kw: Additional arguments passed to model_validate\n\n    Returns:\n        List of model instances if many=True, single instance if many=False\n    \"\"\"\n    if many:\n        return [subj_cls.model_validate(r) for r in obj.to_dict(orient=\"records\")]\n    return subj_cls.model_validate(obj.iloc[0].to_dict(), **kw)\n</code></pre>"},{"location":"api/extras/#pydapter.extras.pandas_.DataFrameAdapter.to_obj","title":"<code>to_obj(subj, /, *, many=True, **kw)</code>  <code>classmethod</code>","text":"<p>Convert Pydantic model instances to pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>subj</code> <code>T | list[T]</code> <p>Single model instance or list of instances</p> required <code>many</code> <code>bool</code> <p>If True, handle as multiple instances</p> <code>True</code> <code>**kw</code> <code>Any</code> <p>Additional arguments passed to DataFrame constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas DataFrame with model data</p> Source code in <code>src/pydapter/extras/pandas_.py</code> <pre><code>@classmethod\ndef to_obj(\n    cls, subj: T | list[T], /, *, many: bool = True, **kw: Any\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert Pydantic model instances to pandas DataFrame.\n\n    Args:\n        subj: Single model instance or list of instances\n        many: If True, handle as multiple instances\n        **kw: Additional arguments passed to DataFrame constructor\n\n    Returns:\n        pandas DataFrame with model data\n    \"\"\"\n    items = subj if isinstance(subj, list) else [subj]\n    return pd.DataFrame([i.model_dump() for i in items], **kw)\n</code></pre>"},{"location":"api/extras/#pydapter.extras.pandas_.SeriesAdapter","title":"<code>SeriesAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Adapter for converting between Pydantic models and pandas Series.</p> <p>This adapter handles pandas Series objects, providing methods to: - Convert Series to a single Pydantic model instance - Convert Pydantic model to Series - Only supports single records (many=False)</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"pd.Series\")</p> Example <pre><code>import pandas as pd\nfrom pydantic import BaseModel\nfrom pydapter.extras.pandas_ import SeriesAdapter\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Create Series\nseries = pd.Series({\"name\": \"John\", \"age\": 30})\n\n# Convert to Pydantic model\nperson = SeriesAdapter.from_obj(Person, series)\n\n# Convert back to Series\nseries_output = SeriesAdapter.to_obj(person)\n</code></pre> Source code in <code>src/pydapter/extras/pandas_.py</code> <pre><code>class SeriesAdapter(Adapter[T]):\n    \"\"\"\n    Adapter for converting between Pydantic models and pandas Series.\n\n    This adapter handles pandas Series objects, providing methods to:\n    - Convert Series to a single Pydantic model instance\n    - Convert Pydantic model to Series\n    - Only supports single records (many=False)\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"pd.Series\")\n\n    Example:\n        ```python\n        import pandas as pd\n        from pydantic import BaseModel\n        from pydapter.extras.pandas_ import SeriesAdapter\n\n        class Person(BaseModel):\n            name: str\n            age: int\n\n        # Create Series\n        series = pd.Series({\"name\": \"John\", \"age\": 30})\n\n        # Convert to Pydantic model\n        person = SeriesAdapter.from_obj(Person, series)\n\n        # Convert back to Series\n        series_output = SeriesAdapter.to_obj(person)\n        ```\n    \"\"\"\n\n    obj_key = \"pd.Series\"\n\n    @classmethod\n    def from_obj(\n        cls, subj_cls: type[T], obj: pd.Series, /, *, many: bool = False, **kw: Any\n    ) -&gt; T:\n        \"\"\"\n        Convert pandas Series to Pydantic model instance.\n\n        Args:\n            subj_cls: The Pydantic model class to instantiate\n            obj: The pandas Series to convert\n            many: Must be False (Series only supports single records)\n            **kw: Additional arguments passed to model_validate\n\n        Returns:\n            Single model instance\n\n        Raises:\n            ValueError: If many=True is specified\n        \"\"\"\n        if many:\n            raise ValueError(\"SeriesAdapter supports single records only.\")\n        return subj_cls.model_validate(obj.to_dict(), **kw)\n\n    @classmethod\n    def to_obj(\n        cls, subj: T | list[T], /, *, many: bool = False, **kw: Any\n    ) -&gt; pd.Series:\n        if many or isinstance(subj, list):\n            raise ValueError(\"SeriesAdapter supports single records only.\")\n        return pd.Series(subj.model_dump(), **kw)\n</code></pre>"},{"location":"api/extras/#pydapter.extras.pandas_.SeriesAdapter-functions","title":"Functions","text":""},{"location":"api/extras/#pydapter.extras.pandas_.SeriesAdapter.from_obj","title":"<code>from_obj(subj_cls, obj, /, *, many=False, **kw)</code>  <code>classmethod</code>","text":"<p>Convert pandas Series to Pydantic model instance.</p> <p>Parameters:</p> Name Type Description Default <code>subj_cls</code> <code>type[T]</code> <p>The Pydantic model class to instantiate</p> required <code>obj</code> <code>Series</code> <p>The pandas Series to convert</p> required <code>many</code> <code>bool</code> <p>Must be False (Series only supports single records)</p> <code>False</code> <code>**kw</code> <code>Any</code> <p>Additional arguments passed to model_validate</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>Single model instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If many=True is specified</p> Source code in <code>src/pydapter/extras/pandas_.py</code> <pre><code>@classmethod\ndef from_obj(\n    cls, subj_cls: type[T], obj: pd.Series, /, *, many: bool = False, **kw: Any\n) -&gt; T:\n    \"\"\"\n    Convert pandas Series to Pydantic model instance.\n\n    Args:\n        subj_cls: The Pydantic model class to instantiate\n        obj: The pandas Series to convert\n        many: Must be False (Series only supports single records)\n        **kw: Additional arguments passed to model_validate\n\n    Returns:\n        Single model instance\n\n    Raises:\n        ValueError: If many=True is specified\n    \"\"\"\n    if many:\n        raise ValueError(\"SeriesAdapter supports single records only.\")\n    return subj_cls.model_validate(obj.to_dict(), **kw)\n</code></pre>"},{"location":"api/extras/#sql-adapter","title":"SQL Adapter","text":""},{"location":"api/extras/#pydapter.extras.sql_","title":"<code>pydapter.extras.sql_</code>","text":"<p>Generic SQL adapter using SQLAlchemy Core (requires <code>sqlalchemy&gt;=2.0</code>).</p>"},{"location":"api/extras/#pydapter.extras.sql_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.sql_.SQLAdapter","title":"<code>SQLAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Generic SQL adapter using SQLAlchemy Core for database operations.</p> <p>This adapter provides methods to: - Execute SQL queries and convert results to Pydantic models - Insert Pydantic models as rows into database tables - Support for various SQL databases through SQLAlchemy - Handle both raw SQL and table-based operations</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"sql\")</p> Example <pre><code>import sqlalchemy as sa\nfrom pydantic import BaseModel\nfrom pydapter.extras.sql_ import SQLAdapter\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\n# Setup database connection\nengine = sa.create_engine(\"sqlite:///example.db\")\nmetadata = sa.MetaData()\n\n# Query from database\nquery = \"SELECT id, name, email FROM users WHERE active = true\"\nusers = SQLAdapter.from_obj(\n    User,\n    query,\n    many=True,\n    engine=engine\n)\n\n# Insert to database\nnew_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\nSQLAdapter.to_obj(\n    new_users,\n    many=True,\n    table=\"users\",\n    metadata=metadata\n)\n</code></pre> Source code in <code>src/pydapter/extras/sql_.py</code> <pre><code>class SQLAdapter(Adapter[T]):\n    \"\"\"\n    Generic SQL adapter using SQLAlchemy Core for database operations.\n\n    This adapter provides methods to:\n    - Execute SQL queries and convert results to Pydantic models\n    - Insert Pydantic models as rows into database tables\n    - Support for various SQL databases through SQLAlchemy\n    - Handle both raw SQL and table-based operations\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"sql\")\n\n    Example:\n        ```python\n        import sqlalchemy as sa\n        from pydantic import BaseModel\n        from pydapter.extras.sql_ import SQLAdapter\n\n        class User(BaseModel):\n            id: int\n            name: str\n            email: str\n\n        # Setup database connection\n        engine = sa.create_engine(\"sqlite:///example.db\")\n        metadata = sa.MetaData()\n\n        # Query from database\n        query = \"SELECT id, name, email FROM users WHERE active = true\"\n        users = SQLAdapter.from_obj(\n            User,\n            query,\n            many=True,\n            engine=engine\n        )\n\n        # Insert to database\n        new_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\n        SQLAdapter.to_obj(\n            new_users,\n            many=True,\n            table=\"users\",\n            metadata=metadata\n        )\n        ```\n    \"\"\"\n\n    obj_key = \"sql\"\n\n    @staticmethod\n    def _table(metadata: sa.MetaData, table: str, engine=None) -&gt; sa.Table:\n        \"\"\"\n        Helper method to get a SQLAlchemy Table object with autoloading.\n\n        Args:\n            metadata: SQLAlchemy MetaData instance\n            table: Name of the table to load\n            engine: Optional SQLAlchemy engine for autoloading\n\n        Returns:\n            SQLAlchemy Table object\n\n        Raises:\n            ResourceError: If table is not found or cannot be accessed\n        \"\"\"\n        try:\n            # Use engine if provided, otherwise use metadata.bind\n            autoload_with = engine if engine is not None else metadata.bind  # type: ignore\n            return sa.Table(table, metadata, autoload_with=autoload_with)\n        except sq_exc.NoSuchTableError as e:\n            raise ResourceError(f\"Table '{table}' not found\", resource=table) from e\n        except Exception as e:\n            raise ResourceError(\n                f\"Error accessing table '{table}': {e}\", resource=table\n            ) from e\n\n    # ---- incoming\n    @classmethod\n    def from_obj(\n        cls,\n        subj_cls: type[T],\n        obj: dict,\n        /,\n        *,\n        many=True,\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if \"engine_url\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'engine_url'\", data=obj\n                )\n            if \"table\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'table'\", data=obj\n                )\n\n            # Create engine and connect to database\n            try:\n                eng = sa.create_engine(obj[\"engine_url\"], future=True)\n            except Exception as e:\n                raise ConnectionError(\n                    f\"Failed to create database engine: {e}\",\n                    adapter=\"sql\",\n                    url=obj[\"engine_url\"],\n                ) from e\n\n            # Create metadata and get table\n            try:\n                md = sa.MetaData()\n                md.reflect(bind=eng)\n                tbl = cls._table(md, obj[\"table\"], engine=eng)\n            except ResourceError:\n                # Re-raise ResourceError from _table\n                raise\n            except Exception as e:\n                raise ResourceError(\n                    f\"Error accessing table metadata: {e}\",\n                    resource=obj[\"table\"],\n                ) from e\n\n            # Build query\n            stmt = sa.select(tbl).filter_by(**obj.get(\"selectors\", {}))\n\n            # Execute query\n            try:\n                with eng.begin() as conn:\n                    rows = conn.execute(stmt).fetchall()\n            except Exception as e:\n                raise QueryError(\n                    f\"Error executing query: {e}\",\n                    query=str(stmt),\n                    adapter=\"sql\",\n                ) from e\n\n            # Handle empty result set\n            if not rows:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No rows found matching the query\",\n                    resource=obj[\"table\"],\n                    selectors=obj.get(\"selectors\", {}),\n                )\n\n            # Convert rows to model instances\n            try:\n                if many:\n                    return [subj_cls.model_validate(r._mapping) for r in rows]\n                return subj_cls.model_validate(rows[0]._mapping)\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=rows[0]._mapping if not many else [r._mapping for r in rows],\n                    errors=e.errors(),\n                ) from e\n\n        except AdapterError:\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(f\"Unexpected error in SQL adapter: {e}\", adapter=\"sql\")\n\n    # ---- outgoing\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | Sequence[T],\n        /,\n        *,\n        engine_url: str,\n        table: str,\n        many=True,\n        **kw,\n    ) -&gt; dict[str, Any]:\n        try:\n            # Validate required parameters\n            if not engine_url:\n                raise AdapterValidationError(\"Missing required parameter 'engine_url'\")\n            if not table:\n                raise AdapterValidationError(\"Missing required parameter 'table'\")\n\n            # Create engine and connect to database\n            try:\n                eng = sa.create_engine(engine_url, future=True)\n            except Exception as e:\n                raise ConnectionError(\n                    f\"Failed to create database engine: {e}\",\n                    adapter=\"sql\",\n                    url=engine_url,\n                ) from e\n\n            # Create metadata and get table\n            try:\n                md = sa.MetaData()\n                md.reflect(bind=eng)\n                tbl = cls._table(md, table, engine=eng)\n            except ResourceError:\n                # Re-raise ResourceError from _table\n                raise\n            except Exception as e:\n                raise ResourceError(\n                    f\"Error accessing table metadata: {e}\",\n                    resource=table,\n                ) from e\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return {\"success\": True, \"count\": 0}  # Nothing to insert\n\n            rows = [i.model_dump() for i in items]\n\n            # Execute insert or update (upsert)\n            try:\n                with eng.begin() as conn:\n                    # Get primary key columns\n                    pk_columns = [c.name for c in tbl.primary_key.columns]\n\n                    if not pk_columns:\n                        # If no primary key, just insert\n                        conn.execute(sa.insert(tbl), rows)\n                    else:\n                        # For PostgreSQL, use ON CONFLICT DO UPDATE\n                        for row in rows:\n                            # Build the values to update (excluding primary key columns)\n                            update_values = {\n                                k: v for k, v in row.items() if k not in pk_columns\n                            }\n                            if not update_values:\n                                # If only primary key columns, just try to insert\n                                stmt = sa.insert(tbl).values(**row)\n                            else:\n                                # Otherwise, do an upsert\n                                stmt = postgresql.insert(tbl).values(**row)\n                                stmt = stmt.on_conflict_do_update(\n                                    index_elements=pk_columns, set_=update_values\n                                )\n                            conn.execute(stmt)\n            except Exception as e:\n                raise QueryError(\n                    f\"Error executing insert/update: {e}\",\n                    query=f\"UPSERT INTO {table}\",\n                    adapter=\"sql\",\n                ) from e\n\n            # Return a success indicator instead of None\n            return {\"success\": True, \"count\": len(rows)}\n\n        except AdapterError:\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(f\"Unexpected error in SQL adapter: {e}\", adapter=\"sql\")\n</code></pre>"},{"location":"api/extras/#postgresql-adapter","title":"PostgreSQL Adapter","text":""},{"location":"api/extras/#pydapter.extras.postgres_","title":"<code>pydapter.extras.postgres_</code>","text":"<p>PostgresAdapter - thin preset over SQLAdapter (pgvector-ready if you add vec column).</p>"},{"location":"api/extras/#pydapter.extras.postgres_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.postgres_.PostgresAdapter","title":"<code>PostgresAdapter</code>","text":"<p>               Bases: <code>SQLAdapter[T]</code></p> <p>PostgreSQL-specific adapter extending SQLAdapter with PostgreSQL optimizations.</p> <p>This adapter provides: - PostgreSQL-specific connection handling and error messages - Default PostgreSQL connection string - Enhanced error handling for common PostgreSQL issues - Support for pgvector when vector columns are present</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"postgres\")</p> <code>DEFAULT</code> <p>Default PostgreSQL connection string</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.extras.postgres_ import PostgresAdapter\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\n# Query with custom connection\nquery_config = {\n    \"query\": \"SELECT id, name, email FROM users WHERE active = true\",\n    \"engine_url\": \"postgresql+psycopg://user:pass@localhost/mydb\"\n}\nusers = PostgresAdapter.from_obj(User, query_config, many=True)\n\n# Insert with default connection\ninsert_config = {\n    \"table\": \"users\",\n    \"engine_url\": \"postgresql+psycopg://user:pass@localhost/mydb\"\n}\nnew_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\nPostgresAdapter.to_obj(new_users, insert_config, many=True)\n</code></pre> Source code in <code>src/pydapter/extras/postgres_.py</code> <pre><code>class PostgresAdapter(SQLAdapter[T]):\n    \"\"\"\n    PostgreSQL-specific adapter extending SQLAdapter with PostgreSQL optimizations.\n\n    This adapter provides:\n    - PostgreSQL-specific connection handling and error messages\n    - Default PostgreSQL connection string\n    - Enhanced error handling for common PostgreSQL issues\n    - Support for pgvector when vector columns are present\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"postgres\")\n        DEFAULT: Default PostgreSQL connection string\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.extras.postgres_ import PostgresAdapter\n\n        class User(BaseModel):\n            id: int\n            name: str\n            email: str\n\n        # Query with custom connection\n        query_config = {\n            \"query\": \"SELECT id, name, email FROM users WHERE active = true\",\n            \"engine_url\": \"postgresql+psycopg://user:pass@localhost/mydb\"\n        }\n        users = PostgresAdapter.from_obj(User, query_config, many=True)\n\n        # Insert with default connection\n        insert_config = {\n            \"table\": \"users\",\n            \"engine_url\": \"postgresql+psycopg://user:pass@localhost/mydb\"\n        }\n        new_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\n        PostgresAdapter.to_obj(new_users, insert_config, many=True)\n        ```\n    \"\"\"\n\n    obj_key = \"postgres\"\n    DEFAULT = \"postgresql+psycopg://user:pass@localhost/db\"\n\n    @classmethod\n    def from_obj(cls, subj_cls, obj: dict, /, **kw):\n        try:\n            # Set default connection string if not provided\n            obj.setdefault(\"engine_url\", cls.DEFAULT)\n\n            # Add PostgreSQL-specific error handling\n            try:\n                return super().from_obj(subj_cls, obj, **kw)\n            except Exception as e:\n                # Check for common PostgreSQL-specific errors\n                error_str = str(e).lower()\n                if \"authentication\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL authentication failed: {e}\",\n                        adapter=\"postgres\",\n                        url=obj[\"engine_url\"],\n                    ) from e\n                elif \"connection\" in error_str and \"refused\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL connection refused: {e}\",\n                        adapter=\"postgres\",\n                        url=obj[\"engine_url\"],\n                    ) from e\n                elif \"does not exist\" in error_str and \"database\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL database does not exist: {e}\",\n                        adapter=\"postgres\",\n                        url=obj[\"engine_url\"],\n                    ) from e\n                # Re-raise the original exception\n                raise\n\n        except ConnectionError:\n            # Re-raise ConnectionError\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ConnectionError(\n                f\"Unexpected error in PostgreSQL adapter: {e}\",\n                adapter=\"postgres\",\n                url=obj.get(\"engine_url\", cls.DEFAULT),\n            ) from e\n\n    @classmethod\n    def to_obj(cls, subj, /, **kw):\n        try:\n            # Set default connection string if not provided\n            kw.setdefault(\"engine_url\", cls.DEFAULT)\n\n            # Add PostgreSQL-specific error handling\n            try:\n                return super().to_obj(subj, **kw)\n            except Exception as e:\n                # Check for common PostgreSQL-specific errors\n                error_str = str(e).lower()\n                if \"authentication\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL authentication failed: {e}\",\n                        adapter=\"postgres\",\n                        url=kw[\"engine_url\"],\n                    ) from e\n                elif \"connection\" in error_str and \"refused\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL connection refused: {e}\",\n                        adapter=\"postgres\",\n                        url=kw[\"engine_url\"],\n                    ) from e\n                elif \"does not exist\" in error_str and \"database\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL database does not exist: {e}\",\n                        adapter=\"postgres\",\n                        url=kw[\"engine_url\"],\n                    ) from e\n                # Re-raise the original exception\n                raise\n\n        except ConnectionError:\n            # Re-raise ConnectionError\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ConnectionError(\n                f\"Unexpected error in PostgreSQL adapter: {e}\",\n                adapter=\"postgres\",\n                url=kw.get(\"engine_url\", cls.DEFAULT),\n            ) from e\n</code></pre>"},{"location":"api/extras/#mongodb-adapter","title":"MongoDB Adapter","text":""},{"location":"api/extras/#pydapter.extras.mongo_","title":"<code>pydapter.extras.mongo_</code>","text":"<p>MongoDB adapter (requires <code>pymongo</code>).</p>"},{"location":"api/extras/#pydapter.extras.mongo_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.mongo_.MongoAdapter","title":"<code>MongoAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>MongoDB adapter for converting between Pydantic models and MongoDB documents.</p> <p>This adapter provides methods to: - Query MongoDB collections and convert documents to Pydantic models - Insert Pydantic models as documents into MongoDB collections - Handle MongoDB connection management and error handling - Support for various MongoDB operations (find, insert, update, delete)</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"mongo\")</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.extras.mongo_ import MongoAdapter\n\nclass User(BaseModel):\n    name: str\n    email: str\n    age: int\n\n# Query from MongoDB\nquery_config = {\n    \"url\": \"mongodb://localhost:27017\",\n    \"database\": \"myapp\",\n    \"collection\": \"users\",\n    \"filter\": {\"age\": {\"$gte\": 18}}\n}\nusers = MongoAdapter.from_obj(User, query_config, many=True)\n\n# Insert to MongoDB\ninsert_config = {\n    \"url\": \"mongodb://localhost:27017\",\n    \"database\": \"myapp\",\n    \"collection\": \"users\"\n}\nnew_users = [User(name=\"John\", email=\"john@example.com\", age=30)]\nMongoAdapter.to_obj(new_users, insert_config, many=True)\n</code></pre> Source code in <code>src/pydapter/extras/mongo_.py</code> <pre><code>class MongoAdapter(Adapter[T]):\n    \"\"\"\n    MongoDB adapter for converting between Pydantic models and MongoDB documents.\n\n    This adapter provides methods to:\n    - Query MongoDB collections and convert documents to Pydantic models\n    - Insert Pydantic models as documents into MongoDB collections\n    - Handle MongoDB connection management and error handling\n    - Support for various MongoDB operations (find, insert, update, delete)\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"mongo\")\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.extras.mongo_ import MongoAdapter\n\n        class User(BaseModel):\n            name: str\n            email: str\n            age: int\n\n        # Query from MongoDB\n        query_config = {\n            \"url\": \"mongodb://localhost:27017\",\n            \"database\": \"myapp\",\n            \"collection\": \"users\",\n            \"filter\": {\"age\": {\"$gte\": 18}}\n        }\n        users = MongoAdapter.from_obj(User, query_config, many=True)\n\n        # Insert to MongoDB\n        insert_config = {\n            \"url\": \"mongodb://localhost:27017\",\n            \"database\": \"myapp\",\n            \"collection\": \"users\"\n        }\n        new_users = [User(name=\"John\", email=\"john@example.com\", age=30)]\n        MongoAdapter.to_obj(new_users, insert_config, many=True)\n        ```\n    \"\"\"\n\n    obj_key = \"mongo\"\n\n    @classmethod\n    def _client(cls, url: str) -&gt; pymongo.MongoClient:\n        \"\"\"\n        Create a MongoDB client with proper error handling.\n\n        Args:\n            url: MongoDB connection string\n\n        Returns:\n            pymongo.MongoClient instance\n\n        Raises:\n            ConnectionError: If connection cannot be established\n        \"\"\"\n        try:\n            return pymongo.MongoClient(url, serverSelectionTimeoutMS=5000)\n        except pymongo.errors.ConfigurationError as e:\n            raise ConnectionError(\n                f\"Invalid MongoDB connection string: {e}\", adapter=\"mongo\", url=url\n            ) from e\n        except Exception as e:\n            raise ConnectionError(\n                f\"Failed to create MongoDB client: {e}\", adapter=\"mongo\", url=url\n            ) from e\n\n    @classmethod\n    def _validate_connection(cls, client: pymongo.MongoClient) -&gt; None:\n        \"\"\"Validate that the MongoDB connection is working.\"\"\"\n        try:\n            # This will raise an exception if the connection fails\n            client.admin.command(\"ping\")\n        except pymongo.errors.ServerSelectionTimeoutError as e:\n            raise ConnectionError(\n                f\"MongoDB server selection timeout: {e}\", adapter=\"mongo\"\n            ) from e\n        except pymongo.errors.OperationFailure as e:\n            if \"auth failed\" in str(e).lower():\n                raise ConnectionError(\n                    f\"MongoDB authentication failed: {e}\", adapter=\"mongo\"\n                ) from e\n            raise QueryError(f\"MongoDB operation failure: {e}\", adapter=\"mongo\") from e\n        except Exception as e:\n            raise ConnectionError(\n                f\"Failed to connect to MongoDB: {e}\", adapter=\"mongo\"\n            ) from e\n\n    # incoming\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=True, **kw):\n        try:\n            # Validate required parameters\n            if \"url\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'url'\", data=obj\n                )\n            if \"db\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'db'\", data=obj\n                )\n            if \"collection\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'collection'\", data=obj\n                )\n\n            # Create client and validate connection\n            client = cls._client(obj[\"url\"])\n            cls._validate_connection(client)\n\n            # Get collection and execute query\n            try:\n                coll = client[obj[\"db\"]][obj[\"collection\"]]\n                filter_query = obj.get(\"filter\") or {}\n\n                # Validate filter query if provided\n                if filter_query and not isinstance(filter_query, dict):\n                    raise AdapterValidationError(\n                        \"Filter must be a dictionary\",\n                        data=filter_query,\n                    )\n\n                docs = list(coll.find(filter_query))\n            except pymongo.errors.OperationFailure as e:\n                if \"not authorized\" in str(e).lower():\n                    raise ConnectionError(\n                        f\"Not authorized to access {obj['db']}.{obj['collection']}: {e}\",\n                        adapter=\"mongo\",\n                        url=obj[\"url\"],\n                    ) from e\n                raise QueryError(\n                    f\"MongoDB query error: {e}\",\n                    query=filter_query,\n                    adapter=\"mongo\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Error executing MongoDB query: {e}\",\n                    query=filter_query,\n                    adapter=\"mongo\",\n                ) from e\n\n            # Handle empty result set\n            if not docs:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No documents found matching the query\",\n                    resource=f\"{obj['db']}.{obj['collection']}\",\n                    filter=filter_query,\n                )\n\n            # Convert documents to model instances\n            try:\n                if many:\n                    return [subj_cls.model_validate(d) for d in docs]\n                return subj_cls.model_validate(docs[0])\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=docs[0] if not many else docs,\n                    errors=e.errors(),\n                ) from e\n\n        except AdapterError:\n            raise\n\n        except Exception as e:\n            raise QueryError(\n                f\"Unexpected error in MongoDB adapter: {e}\", adapter=\"mongo\"\n            )\n\n    # outgoing\n    @classmethod\n    def to_obj(cls, subj: T | Sequence[T], /, *, url, db, collection, many=True, **kw):\n        try:\n            # Validate required parameters\n            if not url:\n                raise AdapterValidationError(\"Missing required parameter 'url'\")\n            if not db:\n                raise AdapterValidationError(\"Missing required parameter 'db'\")\n            if not collection:\n                raise AdapterValidationError(\"Missing required parameter 'collection'\")\n\n            # Create client and validate connection\n            client = cls._client(url)\n            cls._validate_connection(client)\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return None  # Nothing to insert\n\n            payload = [i.model_dump() for i in items]\n\n            # Execute insert\n            try:\n                result = client[db][collection].insert_many(payload)\n                return {\"inserted_count\": result.inserted_ids}\n            except pymongo.errors.BulkWriteError as e:\n                raise QueryError(\n                    f\"MongoDB bulk write error: {e}\",\n                    adapter=\"mongo\",\n                ) from e\n            except pymongo.errors.OperationFailure as e:\n                if \"not authorized\" in str(e).lower():\n                    raise ConnectionError(\n                        f\"Not authorized to write to {db}.{collection}: {e}\",\n                        adapter=\"mongo\",\n                        url=url,\n                    ) from e\n                raise QueryError(\n                    f\"MongoDB operation failure: {e}\",\n                    adapter=\"mongo\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Error inserting documents into MongoDB: {e}\",\n                    adapter=\"mongo\",\n                ) from e\n\n        except AdapterError:\n            raise\n\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(\n                f\"Unexpected error in MongoDB adapter: {e}\", adapter=\"mongo\"\n            )\n</code></pre>"},{"location":"api/extras/#neo4j-adapter","title":"Neo4j Adapter","text":""},{"location":"api/extras/#pydapter.extras.neo4j_","title":"<code>pydapter.extras.neo4j_</code>","text":"<p>Neo4j adapter (requires <code>neo4j</code>).</p>"},{"location":"api/extras/#pydapter.extras.neo4j_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.neo4j_.Neo4jAdapter","title":"<code>Neo4jAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Neo4j graph database adapter for converting between Pydantic models and Neo4j nodes/relationships.</p> <p>This adapter provides methods to: - Execute Cypher queries and convert results to Pydantic models - Create nodes and relationships from Pydantic models - Handle Neo4j connection management and error handling - Support for complex graph operations and traversals</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"neo4j\")</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.extras.neo4j_ import Neo4jAdapter\nfrom neo4j import basic_auth\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    city: str\n\n# Query from Neo4j\nquery_config = {\n    \"url\": \"bolt://localhost:7687\",\n    \"auth\": basic_auth(\"neo4j\", \"password\"),\n    \"query\": \"MATCH (p:Person) WHERE p.age &gt;= 18 RETURN p.name, p.age, p.city\"\n}\npeople = Neo4jAdapter.from_obj(Person, query_config, many=True)\n\n# Create nodes in Neo4j\ncreate_config = {\n    \"url\": \"bolt://localhost:7687\",\n    \"auth\": basic_auth(\"neo4j\", \"password\"),\n    \"query\": \"CREATE (p:Person {name: $name, age: $age, city: $city})\"\n}\nnew_people = [Person(name=\"John\", age=30, city=\"NYC\")]\nNeo4jAdapter.to_obj(new_people, create_config, many=True)\n</code></pre> Source code in <code>src/pydapter/extras/neo4j_.py</code> <pre><code>class Neo4jAdapter(Adapter[T]):\n    \"\"\"\n    Neo4j graph database adapter for converting between Pydantic models and Neo4j nodes/relationships.\n\n    This adapter provides methods to:\n    - Execute Cypher queries and convert results to Pydantic models\n    - Create nodes and relationships from Pydantic models\n    - Handle Neo4j connection management and error handling\n    - Support for complex graph operations and traversals\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"neo4j\")\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.extras.neo4j_ import Neo4jAdapter\n        from neo4j import basic_auth\n\n        class Person(BaseModel):\n            name: str\n            age: int\n            city: str\n\n        # Query from Neo4j\n        query_config = {\n            \"url\": \"bolt://localhost:7687\",\n            \"auth\": basic_auth(\"neo4j\", \"password\"),\n            \"query\": \"MATCH (p:Person) WHERE p.age &gt;= 18 RETURN p.name, p.age, p.city\"\n        }\n        people = Neo4jAdapter.from_obj(Person, query_config, many=True)\n\n        # Create nodes in Neo4j\n        create_config = {\n            \"url\": \"bolt://localhost:7687\",\n            \"auth\": basic_auth(\"neo4j\", \"password\"),\n            \"query\": \"CREATE (p:Person {name: $name, age: $age, city: $city})\"\n        }\n        new_people = [Person(name=\"John\", age=30, city=\"NYC\")]\n        Neo4jAdapter.to_obj(new_people, create_config, many=True)\n        ```\n    \"\"\"\n\n    obj_key = \"neo4j\"\n\n    @classmethod\n    def _create_driver(cls, url: str, auth=None) -&gt; neo4j.Driver:\n        \"\"\"\n        Create a Neo4j driver with proper error handling.\n\n        Args:\n            url: Neo4j connection URL (e.g., \"bolt://localhost:7687\")\n            auth: Authentication tuple or None for no auth\n\n        Returns:\n            neo4j.Driver instance\n\n        Raises:\n            ConnectionError: If connection cannot be established or auth fails\n        \"\"\"\n        try:\n            if auth:\n                return GraphDatabase.driver(url, auth=auth)\n            else:\n                return GraphDatabase.driver(url)\n        except neo4j.exceptions.ServiceUnavailable as e:\n            raise ConnectionError(\n                f\"Neo4j service unavailable: {e}\", adapter=\"neo4j\", url=url\n            ) from e\n        except neo4j.exceptions.AuthError as e:\n            raise ConnectionError(\n                f\"Neo4j authentication failed: {e}\", adapter=\"neo4j\", url=url\n            ) from e\n        except Exception as e:\n            raise ConnectionError(\n                f\"Failed to create Neo4j driver: {e}\", adapter=\"neo4j\", url=url\n            ) from e\n\n    @classmethod\n    def _validate_cypher(cls, cypher: str) -&gt; None:\n        \"\"\"Basic validation for Cypher queries to prevent injection.\"\"\"\n        # Check for unescaped backticks in label names\n        if re.search(r\"`[^`]*`[^`]*`\", cypher):\n            raise QueryError(\n                \"Invalid Cypher query: Possible injection in label name\",\n                query=cypher,\n                adapter=\"neo4j\",\n            )\n\n    # incoming\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=True, **kw):\n        try:\n            # Validate required parameters\n            if \"url\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'url'\", data=obj\n                )\n\n            # Create driver\n            auth = obj.get(\"auth\")\n            driver = cls._create_driver(obj[\"url\"], auth=auth)\n\n            # Prepare Cypher query\n            label = obj.get(\"label\", subj_cls.__name__)\n            where = f\"WHERE {obj['where']}\" if \"where\" in obj else \"\"\n            cypher = f\"MATCH (n:`{label}`) {where} RETURN n\"\n\n            # Validate Cypher query\n            cls._validate_cypher(cypher)\n\n            # Execute query\n            try:\n                with driver.session() as s:\n                    result = s.run(cypher)\n                    rows = [r[\"n\"]._properties for r in result]\n            except neo4j.exceptions.CypherSyntaxError as e:\n                raise QueryError(\n                    f\"Neo4j Cypher syntax error: {e}\",\n                    query=cypher,\n                    adapter=\"neo4j\",\n                ) from e\n            except neo4j.exceptions.ClientError as e:\n                if \"not found\" in str(e).lower():\n                    raise ResourceError(\n                        f\"Neo4j resource not found: {e}\",\n                        resource=label,\n                    ) from e\n                raise QueryError(\n                    f\"Neo4j client error: {e}\",\n                    query=cypher,\n                    adapter=\"neo4j\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Error executing Neo4j query: {e}\",\n                    query=cypher,\n                    adapter=\"neo4j\",\n                ) from e\n            finally:\n                driver.close()\n\n            # Handle empty result set\n            if not rows:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No nodes found matching the query\",\n                    resource=label,\n                    where=obj.get(\"where\", \"\"),\n                )\n\n            # Convert rows to model instances\n            try:\n                if many:\n                    return [subj_cls.model_validate(r) for r in rows]\n                return subj_cls.model_validate(rows[0])\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=rows[0] if not many else rows,\n                    errors=e.errors(),\n                ) from e\n\n        except (ConnectionError, QueryError, ResourceError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(f\"Unexpected error in Neo4j adapter: {e}\", adapter=\"neo4j\")\n\n    # outgoing\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | Sequence[T],\n        /,\n        *,\n        url,\n        auth=None,\n        label=None,\n        merge_on=\"id\",\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if not url:\n                raise AdapterValidationError(\"Missing required parameter 'url'\")\n            if not merge_on:\n                raise AdapterValidationError(\"Missing required parameter 'merge_on'\")\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return None  # Nothing to insert\n\n            # Get label from first item if not provided\n            label = label or items[0].__class__.__name__\n\n            # Create driver\n            driver = cls._create_driver(url, auth=auth)\n\n            try:\n                with driver.session() as s:\n                    results = []\n                    for it in items:\n                        props = it.model_dump()\n\n                        # Check if merge_on property exists\n                        if merge_on not in props:\n                            raise AdapterValidationError(\n                                f\"Merge property '{merge_on}' not found in model\",\n                                data=props,\n                            )\n\n                        # Prepare and validate Cypher query\n                        cypher = (\n                            f\"MERGE (n:`{label}` {{{merge_on}: $val}}) SET n += $props\"\n                        )\n                        cls._validate_cypher(cypher)\n\n                        # Execute query\n                        try:\n                            result = s.run(cypher, val=props[merge_on], props=props)\n                            results.append(result)\n                        except neo4j.exceptions.CypherSyntaxError as e:\n                            raise QueryError(\n                                f\"Neo4j Cypher syntax error: {e}\",\n                                query=cypher,\n                                adapter=\"neo4j\",\n                            ) from e\n                        except neo4j.exceptions.ConstraintError as e:\n                            raise QueryError(\n                                f\"Neo4j constraint violation: {e}\",\n                                query=cypher,\n                                adapter=\"neo4j\",\n                            ) from e\n                        except Exception as e:\n                            raise QueryError(\n                                f\"Error executing Neo4j query: {e}\",\n                                query=cypher,\n                                adapter=\"neo4j\",\n                            ) from e\n\n                    return {\"merged_count\": len(results)}\n            finally:\n                driver.close()\n\n        except (ConnectionError, QueryError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(f\"Unexpected error in Neo4j adapter: {e}\", adapter=\"neo4j\")\n</code></pre>"},{"location":"api/extras/#qdrant-adapter","title":"Qdrant Adapter","text":""},{"location":"api/extras/#pydapter.extras.qdrant_","title":"<code>pydapter.extras.qdrant_</code>","text":"<p>Qdrant vector-store adapter (requires <code>qdrant-client</code>).</p>"},{"location":"api/extras/#pydapter.extras.qdrant_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.qdrant_.QdrantAdapter","title":"<code>QdrantAdapter</code>","text":"<p>               Bases: <code>Adapter[T]</code></p> <p>Qdrant vector database adapter for converting between Pydantic models and vector embeddings.</p> <p>This adapter provides methods to: - Search for similar vectors and convert results to Pydantic models - Insert Pydantic models as vector points into Qdrant collections - Handle vector similarity operations and metadata filtering - Support for both cloud and self-hosted Qdrant instances</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"qdrant\")</p> Example <pre><code>from pydantic import BaseModel\nfrom pydapter.extras.qdrant_ import QdrantAdapter\n\nclass Document(BaseModel):\n    id: str\n    text: str\n    embedding: list[float]\n    category: str\n\n# Search for similar vectors\nsearch_config = {\n    \"url\": \"http://localhost:6333\",\n    \"collection_name\": \"documents\",\n    \"query_vector\": [0.1, 0.2, 0.3, ...],  # 768-dim vector\n    \"limit\": 10,\n    \"score_threshold\": 0.8\n}\nsimilar_docs = QdrantAdapter.from_obj(Document, search_config, many=True)\n\n# Insert documents with vectors\ninsert_config = {\n    \"url\": \"http://localhost:6333\",\n    \"collection_name\": \"documents\"\n}\nnew_docs = [Document(\n    id=\"doc1\",\n    text=\"Sample text\",\n    embedding=[0.1, 0.2, 0.3, ...],\n    category=\"tech\"\n)]\nQdrantAdapter.to_obj(new_docs, insert_config, many=True)\n</code></pre> Source code in <code>src/pydapter/extras/qdrant_.py</code> <pre><code>class QdrantAdapter(Adapter[T]):\n    \"\"\"\n    Qdrant vector database adapter for converting between Pydantic models and vector embeddings.\n\n    This adapter provides methods to:\n    - Search for similar vectors and convert results to Pydantic models\n    - Insert Pydantic models as vector points into Qdrant collections\n    - Handle vector similarity operations and metadata filtering\n    - Support for both cloud and self-hosted Qdrant instances\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"qdrant\")\n\n    Example:\n        ```python\n        from pydantic import BaseModel\n        from pydapter.extras.qdrant_ import QdrantAdapter\n\n        class Document(BaseModel):\n            id: str\n            text: str\n            embedding: list[float]\n            category: str\n\n        # Search for similar vectors\n        search_config = {\n            \"url\": \"http://localhost:6333\",\n            \"collection_name\": \"documents\",\n            \"query_vector\": [0.1, 0.2, 0.3, ...],  # 768-dim vector\n            \"limit\": 10,\n            \"score_threshold\": 0.8\n        }\n        similar_docs = QdrantAdapter.from_obj(Document, search_config, many=True)\n\n        # Insert documents with vectors\n        insert_config = {\n            \"url\": \"http://localhost:6333\",\n            \"collection_name\": \"documents\"\n        }\n        new_docs = [Document(\n            id=\"doc1\",\n            text=\"Sample text\",\n            embedding=[0.1, 0.2, 0.3, ...],\n            category=\"tech\"\n        )]\n        QdrantAdapter.to_obj(new_docs, insert_config, many=True)\n        ```\n    \"\"\"\n\n    obj_key = \"qdrant\"\n\n    @staticmethod\n    def _client(url: str | None):\n        \"\"\"\n        Create a Qdrant client with proper error handling.\n\n        Args:\n            url: Qdrant server URL or None for in-memory instance\n\n        Returns:\n            QdrantClient instance\n\n        Raises:\n            ConnectionError: If connection cannot be established\n        \"\"\"\n        try:\n            return QdrantClient(url=url) if url else QdrantClient(\":memory:\")\n        except UnexpectedResponse as e:\n            raise ConnectionError(\n                f\"Failed to connect to Qdrant: {e}\", adapter=\"qdrant\", url=url\n            ) from e\n        except (ConnectionRefusedError, OSError, grpc.RpcError) as e:\n            # Catch specific network-related errors like DNS resolution failures\n            # Include grpc.RpcError to handle gRPC-specific connection issues\n            raise ConnectionError(\n                f\"Failed to connect to Qdrant: {e}\", adapter=\"qdrant\", url=url\n            ) from e\n        except Exception as e:\n            # Check for DNS resolution errors in the exception message\n            if (\n                \"nodename nor servname provided\" in str(e)\n                or \"Name or service not known\" in str(e)\n                or \"getaddrinfo failed\" in str(e)\n            ):\n                raise ConnectionError(\n                    f\"DNS resolution failed for Qdrant: {e}\", adapter=\"qdrant\", url=url\n                ) from e\n            raise ConnectionError(\n                f\"Unexpected error connecting to Qdrant: {e}\", adapter=\"qdrant\", url=url\n            ) from e\n\n    def _validate_vector_dimensions(vector, expected_dim=None):\n        \"\"\"Validate that the vector has the correct dimensions.\"\"\"\n        if not isinstance(vector, (list, tuple)) or not all(\n            isinstance(x, (int, float)) for x in vector\n        ):\n            raise AdapterValidationError(\n                \"Vector must be a list or tuple of numbers\",\n                data=vector,\n            )\n\n        if expected_dim is not None and len(vector) != expected_dim:\n            raise AdapterValidationError(\n                f\"Vector dimension mismatch: expected {expected_dim}, got {len(vector)}\",\n                data=vector,\n            )\n\n    # outgoing\n    @classmethod\n    def to_obj(\n        cls,\n        subj: T | Sequence[T],\n        /,\n        *,\n        collection,\n        vector_field=\"embedding\",\n        id_field=\"id\",\n        url=None,\n        **kw,\n    ) -&gt; None:\n        try:\n            # Validate required parameters\n            if not collection:\n                raise AdapterValidationError(\"Missing required parameter 'collection'\")\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return None  # Nothing to insert\n\n            # Validate vector field exists\n            if not hasattr(items[0], vector_field):\n                raise AdapterValidationError(\n                    f\"Vector field '{vector_field}' not found in model\",\n                    data=items[0].model_dump(),\n                )\n\n            # Validate ID field exists\n            if not hasattr(items[0], id_field):\n                raise AdapterValidationError(\n                    f\"ID field '{id_field}' not found in model\",\n                    data=items[0].model_dump(),\n                )\n\n            # Create client\n            client = cls._client(url)\n\n            # Get vector dimension\n            vector = getattr(items[0], vector_field)\n            cls._validate_vector_dimensions(vector)\n            dim = len(vector)\n\n            # Create or recreate collection\n            try:\n                client.recreate_collection(\n                    collection,\n                    vectors_config=qd.VectorParams(size=dim, distance=\"Cosine\"),\n                )\n            except UnexpectedResponse as e:\n                raise QueryError(\n                    f\"Failed to create Qdrant collection: {e}\",\n                    adapter=\"qdrant\",\n                ) from e\n            except Exception as e:\n                # Check for various DNS and connection-related error messages\n                if (\n                    \"nodename nor servname provided\" in str(e)\n                    or \"connection\" in str(e).lower()\n                    or \"Name or service not known\" in str(e)\n                    or \"getaddrinfo failed\" in str(e)\n                ):\n                    raise ConnectionError(\n                        f\"Failed to connect to Qdrant: {e}\",\n                        adapter=\"qdrant\",\n                        url=url,\n                    ) from e\n                else:\n                    raise QueryError(\n                        f\"Unexpected error creating Qdrant collection: {e}\",\n                        adapter=\"qdrant\",\n                    ) from e\n\n            # Create points\n            try:\n                points = []\n                for i, item in enumerate(items):\n                    vector = getattr(item, vector_field)\n                    cls._validate_vector_dimensions(vector, dim)\n\n                    # Create payload with all fields\n                    # The test_qdrant_to_obj_with_custom_vector_field test expects\n                    # the embedding field to be excluded, but other integration tests\n                    # expect it to be included. We'll include it for now and handle\n                    # the test case separately.\n                    payload = item.model_dump()\n\n                    points.append(\n                        qd.PointStruct(\n                            id=getattr(item, id_field),\n                            vector=vector,\n                            payload=payload,\n                        )\n                    )\n            except AdapterValidationError:\n                # Re-raise validation errors\n                raise\n            except Exception as e:\n                raise AdapterValidationError(\n                    f\"Error creating Qdrant points: {e}\",\n                    data=items,\n                ) from e\n\n            # Upsert points\n            try:\n                client.upsert(collection, points)\n                return {\"upserted_count\": len(points)}\n            except UnexpectedResponse as e:\n                raise QueryError(\n                    f\"Failed to upsert points to Qdrant: {e}\",\n                    adapter=\"qdrant\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Unexpected error upserting points to Qdrant: {e}\",\n                    adapter=\"qdrant\",\n                ) from e\n\n        except (ConnectionError, QueryError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(\n                f\"Unexpected error in Qdrant adapter: {e}\", adapter=\"qdrant\"\n            )\n\n    # incoming\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=True, **kw):\n        try:\n            # Validate required parameters\n            if \"collection\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'collection'\", data=obj\n                )\n            if \"query_vector\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'query_vector'\", data=obj\n                )\n\n            # Validate query vector\n            cls._validate_vector_dimensions(obj[\"query_vector\"])\n\n            # Create client\n            client = cls._client(obj.get(\"url\"))\n\n            # Execute search\n            try:\n                # Set a high score threshold to ensure we get enough results\n                res = client.search(\n                    obj[\"collection\"],\n                    obj[\"query_vector\"],\n                    limit=obj.get(\"top_k\", 5),\n                    with_payload=True,\n                    score_threshold=0.0,  # Return all results regardless of similarity\n                )\n            except UnexpectedResponse as e:\n                if \"not found\" in str(e).lower():\n                    raise ResourceError(\n                        f\"Qdrant collection not found: {e}\",\n                        resource=obj[\"collection\"],\n                    ) from e\n                raise QueryError(\n                    f\"Failed to search Qdrant: {e}\",\n                    adapter=\"qdrant\",\n                ) from e\n            except grpc.RpcError as e:\n                raise ConnectionError(\n                    f\"Qdrant RPC error: {e}\",\n                    adapter=\"qdrant\",\n                    url=obj.get(\"url\"),\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Unexpected error searching Qdrant: {e}\",\n                    adapter=\"qdrant\",\n                ) from e\n\n            # Extract payloads\n            docs = [r.payload for r in res]\n\n            # Handle empty result set\n            if not docs:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No points found matching the query vector\",\n                    resource=obj[\"collection\"],\n                )\n\n            # Convert documents to model instances\n            try:\n                if many:\n                    return [subj_cls.model_validate(d) for d in docs]\n                return subj_cls.model_validate(docs[0])\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=docs[0] if not many else docs,\n                    errors=e.errors(),\n                ) from e\n\n        except (ConnectionError, QueryError, ResourceError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(\n                f\"Unexpected error in Qdrant adapter: {e}\", adapter=\"qdrant\"\n            )\n</code></pre>"},{"location":"api/extras/#async-sql-adapter","title":"Async SQL Adapter","text":""},{"location":"api/extras/#pydapter.extras.async_sql_","title":"<code>pydapter.extras.async_sql_</code>","text":"<p>Generic async SQL adapter - SQLAlchemy 2.x asyncio + asyncpg driver.</p>"},{"location":"api/extras/#pydapter.extras.async_sql_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.async_sql_.AsyncSQLAdapter","title":"<code>AsyncSQLAdapter</code>","text":"<p>               Bases: <code>AsyncAdapter[T]</code></p> <p>Asynchronous SQL adapter using SQLAlchemy 2.x asyncio for database operations.</p> <p>This adapter provides async methods to: - Execute SQL queries asynchronously and convert results to Pydantic models - Insert Pydantic models as rows into database tables asynchronously - Support for various async SQL databases through SQLAlchemy - Handle connection pooling and async context management</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"async_sql\")</p> Example <pre><code>import asyncio\nimport sqlalchemy as sa\nfrom pydantic import BaseModel\nfrom pydapter.extras.async_sql_ import AsyncSQLAdapter\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\nasync def main():\n    # Query from database\n    query_config = {\n        \"engine_url\": \"postgresql+asyncpg://user:pass@localhost/db\",\n        \"query\": \"SELECT id, name, email FROM users WHERE active = true\"\n    }\n    users = await AsyncSQLAdapter.from_obj(User, query_config, many=True)\n\n    # Insert to database\n    insert_config = {\n        \"engine_url\": \"postgresql+asyncpg://user:pass@localhost/db\",\n        \"table\": \"users\"\n    }\n    new_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\n    await AsyncSQLAdapter.to_obj(new_users, insert_config, many=True)\n\nasyncio.run(main())\n</code></pre> Source code in <code>src/pydapter/extras/async_sql_.py</code> <pre><code>class AsyncSQLAdapter(AsyncAdapter[T]):\n    \"\"\"\n    Asynchronous SQL adapter using SQLAlchemy 2.x asyncio for database operations.\n\n    This adapter provides async methods to:\n    - Execute SQL queries asynchronously and convert results to Pydantic models\n    - Insert Pydantic models as rows into database tables asynchronously\n    - Support for various async SQL databases through SQLAlchemy\n    - Handle connection pooling and async context management\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"async_sql\")\n\n    Example:\n        ```python\n        import asyncio\n        import sqlalchemy as sa\n        from pydantic import BaseModel\n        from pydapter.extras.async_sql_ import AsyncSQLAdapter\n\n        class User(BaseModel):\n            id: int\n            name: str\n            email: str\n\n        async def main():\n            # Query from database\n            query_config = {\n                \"engine_url\": \"postgresql+asyncpg://user:pass@localhost/db\",\n                \"query\": \"SELECT id, name, email FROM users WHERE active = true\"\n            }\n            users = await AsyncSQLAdapter.from_obj(User, query_config, many=True)\n\n            # Insert to database\n            insert_config = {\n                \"engine_url\": \"postgresql+asyncpg://user:pass@localhost/db\",\n                \"table\": \"users\"\n            }\n            new_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\n            await AsyncSQLAdapter.to_obj(new_users, insert_config, many=True)\n\n        asyncio.run(main())\n        ```\n    \"\"\"\n\n    obj_key = \"async_sql\"\n\n    @staticmethod\n    def _table(meta: sa.MetaData, name: str) -&gt; sa.Table:\n        \"\"\"\n        Helper method to get a SQLAlchemy Table object for async operations.\n\n        Args:\n            meta: SQLAlchemy MetaData instance\n            name: Name of the table to load\n\n        Returns:\n            SQLAlchemy Table object\n\n        Raises:\n            ResourceError: If table is not found or cannot be accessed\n        \"\"\"\n        try:\n            # In SQLAlchemy 2.x, we should use the connection directly\n            return sa.Table(name, meta, autoload_with=meta.bind)\n        except sa_exc.NoSuchTableError as e:\n            raise ResourceError(f\"Table '{name}' not found\", resource=name) from e\n        except Exception as e:\n            raise ResourceError(\n                f\"Error accessing table '{name}': {e}\", resource=name\n            ) from e\n\n    # incoming\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=True, **kw):\n        try:\n            # Validate required parameters\n            if \"engine_url\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'engine_url'\", data=obj\n                )\n            if \"table\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'table'\", data=obj\n                )\n\n            # Create engine\n            try:\n                eng = create_async_engine(obj[\"engine_url\"], future=True)\n            except Exception as e:\n                raise ConnectionError(\n                    f\"Failed to create async database engine: {e}\",\n                    adapter=\"async_sql\",\n                    url=obj[\"engine_url\"],\n                ) from e\n\n            # Execute query\n            try:\n                # Use a try-except block to handle both real and mocked engines\n                try:\n                    async with eng.begin() as conn:\n                        meta = sa.MetaData()\n                        meta.bind = conn\n                        tbl = cls._table(meta, obj[\"table\"])\n                        stmt = sa.select(tbl).filter_by(**obj.get(\"selectors\", {}))\n                        rows = (await conn.execute(stmt)).fetchall()\n                except TypeError:\n                    # Handle case where eng.begin() is a coroutine in tests\n                    if hasattr(eng.begin, \"__self__\") and hasattr(\n                        eng.begin.__self__, \"__aenter__\"\n                    ):\n                        # This is for test mocks\n                        conn = await eng.begin().__aenter__()\n                        meta = sa.MetaData()\n                        meta.bind = conn\n                        tbl = cls._table(meta, obj[\"table\"])\n                        stmt = sa.select(tbl).filter_by(**obj.get(\"selectors\", {}))\n                        rows = (await conn.execute(stmt)).fetchall()\n                    else:\n                        raise\n            except ResourceError:\n                # Re-raise ResourceError from _table\n                raise\n            except sa_exc.SQLAlchemyError as e:\n                raise QueryError(\n                    f\"Error executing async SQL query: {e}\",\n                    query=str(obj.get(\"selectors\", {})),\n                    adapter=\"async_sql\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Unexpected error in async SQL query: {e}\",\n                    adapter=\"async_sql\",\n                ) from e\n\n            # Handle empty result set\n            if not rows:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No rows found matching the query\",\n                    resource=obj[\"table\"],\n                    selectors=obj.get(\"selectors\", {}),\n                )\n\n            # Convert rows to model instances\n            try:\n                records = [dict(r) for r in rows]\n                if many:\n                    return [subj_cls.model_validate(r) for r in records]\n                return subj_cls.model_validate(records[0])\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=records[0] if not many else records,\n                    errors=e.errors(),\n                ) from e\n\n        except AdapterError:\n            raise\n\n        except Exception as e:\n            raise QueryError(\n                f\"Unexpected error in async SQL adapter: {e}\", adapter=\"async_sql\"\n            )\n\n    # outgoing\n    @classmethod\n    async def to_obj(\n        cls,\n        subj: T | Sequence[T],\n        /,\n        *,\n        engine_url: str,\n        table: str,\n        many=True,\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if not engine_url:\n                raise AdapterValidationError(\"Missing required parameter 'engine_url'\")\n            if not table:\n                raise AdapterValidationError(\"Missing required parameter 'table'\")\n\n            # Create engine\n            try:\n                eng = create_async_engine(engine_url, future=True)\n            except Exception as e:\n                raise ConnectionError(\n                    f\"Failed to create async database engine: {e}\",\n                    adapter=\"async_sql\",\n                    url=engine_url,\n                ) from e\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return None  # Nothing to insert\n\n            rows = [i.model_dump() for i in items]\n\n            # Execute insert\n            try:\n                # Use a try-except block to handle both real and mocked engines\n                try:\n                    async with eng.begin() as conn:\n                        meta = sa.MetaData()\n                        meta.bind = conn\n                        tbl = cls._table(meta, table)\n                        await conn.execute(sa.insert(tbl), rows)\n                        return {\"inserted_count\": len(rows)}\n                except TypeError:\n                    # Handle case where eng.begin() is a coroutine in tests\n                    if hasattr(eng.begin, \"__self__\") and hasattr(\n                        eng.begin.__self__, \"__aenter__\"\n                    ):\n                        # This is for test mocks\n                        conn = await eng.begin().__aenter__()\n                        meta = sa.MetaData()\n                        meta.bind = conn\n                        tbl = cls._table(meta, table)\n                        await conn.execute(sa.insert(tbl), rows)\n                        return {\"inserted_count\": len(rows)}\n                    else:\n                        raise\n            except ResourceError:\n                raise\n\n            except sa_exc.SQLAlchemyError as e:\n                raise QueryError(\n                    f\"Error executing async SQL insert: {e}\",\n                    query=f\"INSERT INTO {table}\",\n                    adapter=\"async_sql\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Unexpected error in async SQL insert: {e}\",\n                    adapter=\"async_sql\",\n                ) from e\n\n        except AdapterError:\n            raise\n\n        except Exception as e:\n            raise QueryError(\n                f\"Unexpected error in async SQL adapter: {e}\", adapter=\"async_sql\"\n            )\n</code></pre>"},{"location":"api/extras/#async-postgresql-adapter","title":"Async PostgreSQL Adapter","text":""},{"location":"api/extras/#pydapter.extras.async_postgres_","title":"<code>pydapter.extras.async_postgres_</code>","text":"<p>AsyncPostgresAdapter - presets AsyncSQLAdapter for PostgreSQL/pgvector.</p>"},{"location":"api/extras/#pydapter.extras.async_postgres_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.async_postgres_.AsyncPostgresAdapter","title":"<code>AsyncPostgresAdapter</code>","text":"<p>               Bases: <code>AsyncSQLAdapter[T]</code></p> <p>Asynchronous PostgreSQL adapter extending AsyncSQLAdapter with PostgreSQL-specific optimizations.</p> <p>This adapter provides: - Async PostgreSQL operations using asyncpg driver - Enhanced error handling for PostgreSQL-specific issues - Support for pgvector when vector columns are present - Default PostgreSQL connection string management</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"async_pg\")</p> <code>DEFAULT</code> <p>Default PostgreSQL+asyncpg connection string</p> Example <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\nasync def main():\n    # Query with custom connection\n    query_config = {\n        \"query\": \"SELECT id, name, email FROM users WHERE active = true\",\n        \"dsn\": \"postgresql+asyncpg://user:pass@localhost/mydb\"\n    }\n    users = await AsyncPostgresAdapter.from_obj(User, query_config, many=True)\n\n    # Insert with default connection\n    insert_config = {\n        \"table\": \"users\"\n    }\n    new_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\n    await AsyncPostgresAdapter.to_obj(new_users, insert_config, many=True)\n\nasyncio.run(main())\n</code></pre> Source code in <code>src/pydapter/extras/async_postgres_.py</code> <pre><code>class AsyncPostgresAdapter(AsyncSQLAdapter[T]):\n    \"\"\"\n    Asynchronous PostgreSQL adapter extending AsyncSQLAdapter with PostgreSQL-specific optimizations.\n\n    This adapter provides:\n    - Async PostgreSQL operations using asyncpg driver\n    - Enhanced error handling for PostgreSQL-specific issues\n    - Support for pgvector when vector columns are present\n    - Default PostgreSQL connection string management\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"async_pg\")\n        DEFAULT: Default PostgreSQL+asyncpg connection string\n\n    Example:\n        ```python\n        import asyncio\n        from pydantic import BaseModel\n        from pydapter.extras.async_postgres_ import AsyncPostgresAdapter\n\n        class User(BaseModel):\n            id: int\n            name: str\n            email: str\n\n        async def main():\n            # Query with custom connection\n            query_config = {\n                \"query\": \"SELECT id, name, email FROM users WHERE active = true\",\n                \"dsn\": \"postgresql+asyncpg://user:pass@localhost/mydb\"\n            }\n            users = await AsyncPostgresAdapter.from_obj(User, query_config, many=True)\n\n            # Insert with default connection\n            insert_config = {\n                \"table\": \"users\"\n            }\n            new_users = [User(id=1, name=\"John\", email=\"john@example.com\")]\n            await AsyncPostgresAdapter.to_obj(new_users, insert_config, many=True)\n\n        asyncio.run(main())\n        ```\n    \"\"\"\n\n    obj_key = \"async_pg\"\n    DEFAULT = \"postgresql+asyncpg://test:test@localhost/test\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls, obj: dict, /, **kw):\n        try:\n            # Use the provided DSN if available, otherwise use the default\n            engine_url = kw.get(\"dsn\", cls.DEFAULT)\n            if \"dsn\" in kw:\n                # Convert the PostgreSQL URL to SQLAlchemy format\n                if not engine_url.startswith(\"postgresql+asyncpg://\"):\n                    engine_url = engine_url.replace(\n                        \"postgresql://\", \"postgresql+asyncpg://\"\n                    )\n            obj.setdefault(\"engine_url\", engine_url)\n\n            # Add PostgreSQL-specific error handling\n            try:\n                return await super().from_obj(subj_cls, obj, **kw)\n            except Exception as e:\n                # Check for common PostgreSQL-specific errors\n                error_str = str(e).lower()\n                if \"authentication\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL authentication failed: {e}\",\n                        adapter=\"async_pg\",\n                        url=engine_url,\n                    ) from e\n                elif \"connection\" in error_str and \"refused\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL connection refused: {e}\",\n                        adapter=\"async_pg\",\n                        url=engine_url,\n                    ) from e\n                elif \"does not exist\" in error_str and \"database\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL database does not exist: {e}\",\n                        adapter=\"async_pg\",\n                        url=engine_url,\n                    ) from e\n                # Re-raise the original exception\n                raise\n\n        except ConnectionError:\n            # Re-raise ConnectionError\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ConnectionError(\n                f\"Unexpected error in async PostgreSQL adapter: {e}\",\n                adapter=\"async_pg\",\n                url=obj.get(\"engine_url\", cls.DEFAULT),\n            ) from e\n\n    @classmethod\n    async def to_obj(cls, subj, /, **kw):\n        try:\n            # Use the provided DSN if available, otherwise use the default\n            engine_url = kw.get(\"dsn\", cls.DEFAULT)\n            if \"dsn\" in kw:\n                # Convert the PostgreSQL URL to SQLAlchemy format\n                if not engine_url.startswith(\"postgresql+asyncpg://\"):\n                    engine_url = engine_url.replace(\n                        \"postgresql://\", \"postgresql+asyncpg://\"\n                    )\n            kw.setdefault(\"engine_url\", engine_url)\n\n            # Add PostgreSQL-specific error handling\n            try:\n                return await super().to_obj(subj, **kw)\n            except Exception as e:\n                # Check for common PostgreSQL-specific errors\n                error_str = str(e).lower()\n                if \"authentication\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL authentication failed: {e}\",\n                        adapter=\"async_pg\",\n                        url=engine_url,\n                    ) from e\n                elif \"connection\" in error_str and \"refused\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL connection refused: {e}\",\n                        adapter=\"async_pg\",\n                        url=engine_url,\n                    ) from e\n                elif \"does not exist\" in error_str and \"database\" in error_str:\n                    raise ConnectionError(\n                        f\"PostgreSQL database does not exist: {e}\",\n                        adapter=\"async_pg\",\n                        url=engine_url,\n                    ) from e\n                # Re-raise the original exception\n                raise\n\n        except ConnectionError:\n            # Re-raise ConnectionError\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise ConnectionError(\n                f\"Unexpected error in async PostgreSQL adapter: {e}\",\n                adapter=\"async_pg\",\n                url=kw.get(\"engine_url\", cls.DEFAULT),\n            ) from e\n</code></pre>"},{"location":"api/extras/#async-mongodb-adapter","title":"Async MongoDB Adapter","text":""},{"location":"api/extras/#pydapter.extras.async_mongo_","title":"<code>pydapter.extras.async_mongo_</code>","text":"<p>AsyncMongoAdapter - uses <code>motor.motor_asyncio</code>.</p>"},{"location":"api/extras/#pydapter.extras.async_mongo_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.async_mongo_.AsyncMongoAdapter","title":"<code>AsyncMongoAdapter</code>","text":"<p>               Bases: <code>AsyncAdapter[T]</code></p> <p>Asynchronous MongoDB adapter for converting between Pydantic models and MongoDB documents.</p> <p>This adapter provides async methods to: - Query MongoDB collections asynchronously and convert documents to Pydantic models - Insert Pydantic models as documents into MongoDB collections asynchronously - Handle async MongoDB operations using Motor (async MongoDB driver) - Support for various async MongoDB operations (find, insert, update, delete)</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"async_mongo\")</p> Example <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom pydapter.extras.async_mongo_ import AsyncMongoAdapter\n\nclass User(BaseModel):\n    name: str\n    email: str\n    age: int\n\nasync def main():\n    # Query from MongoDB\n    query_config = {\n        \"url\": \"mongodb://localhost:27017\",\n        \"database\": \"myapp\",\n        \"collection\": \"users\",\n        \"filter\": {\"age\": {\"$gte\": 18}}\n    }\n    users = await AsyncMongoAdapter.from_obj(User, query_config, many=True)\n\n    # Insert to MongoDB\n    insert_config = {\n        \"url\": \"mongodb://localhost:27017\",\n        \"database\": \"myapp\",\n        \"collection\": \"users\"\n    }\n    new_users = [User(name=\"John\", email=\"john@example.com\", age=30)]\n    await AsyncMongoAdapter.to_obj(new_users, insert_config, many=True)\n\nasyncio.run(main())\n</code></pre> Source code in <code>src/pydapter/extras/async_mongo_.py</code> <pre><code>class AsyncMongoAdapter(AsyncAdapter[T]):\n    \"\"\"\n    Asynchronous MongoDB adapter for converting between Pydantic models and MongoDB documents.\n\n    This adapter provides async methods to:\n    - Query MongoDB collections asynchronously and convert documents to Pydantic models\n    - Insert Pydantic models as documents into MongoDB collections asynchronously\n    - Handle async MongoDB operations using Motor (async MongoDB driver)\n    - Support for various async MongoDB operations (find, insert, update, delete)\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"async_mongo\")\n\n    Example:\n        ```python\n        import asyncio\n        from pydantic import BaseModel\n        from pydapter.extras.async_mongo_ import AsyncMongoAdapter\n\n        class User(BaseModel):\n            name: str\n            email: str\n            age: int\n\n        async def main():\n            # Query from MongoDB\n            query_config = {\n                \"url\": \"mongodb://localhost:27017\",\n                \"database\": \"myapp\",\n                \"collection\": \"users\",\n                \"filter\": {\"age\": {\"$gte\": 18}}\n            }\n            users = await AsyncMongoAdapter.from_obj(User, query_config, many=True)\n\n            # Insert to MongoDB\n            insert_config = {\n                \"url\": \"mongodb://localhost:27017\",\n                \"database\": \"myapp\",\n                \"collection\": \"users\"\n            }\n            new_users = [User(name=\"John\", email=\"john@example.com\", age=30)]\n            await AsyncMongoAdapter.to_obj(new_users, insert_config, many=True)\n\n        asyncio.run(main())\n        ```\n    \"\"\"\n\n    obj_key = \"async_mongo\"\n\n    @classmethod\n    def _client(cls, url: str) -&gt; AsyncIOMotorClient:\n        try:\n            return AsyncIOMotorClient(url, serverSelectionTimeoutMS=5000)\n        except pymongo.errors.ConfigurationError as e:\n            raise ConnectionError(\n                f\"Invalid MongoDB connection string: {e}\",\n                adapter=\"async_mongo\",\n                url=url,\n            ) from e\n        except Exception as e:\n            raise ConnectionError(\n                f\"Failed to create MongoDB client: {e}\", adapter=\"async_mongo\", url=url\n            ) from e\n\n    @classmethod\n    async def _validate_connection(cls, client: AsyncIOMotorClient) -&gt; None:\n        \"\"\"Validate that the MongoDB connection is working.\"\"\"\n        try:\n            # This will raise an exception if the connection fails\n            await client.admin.command(\"ping\")\n        except pymongo.errors.ServerSelectionTimeoutError as e:\n            raise ConnectionError(\n                f\"MongoDB server selection timeout: {e}\", adapter=\"async_mongo\"\n            ) from e\n        except pymongo.errors.OperationFailure as e:\n            if \"auth failed\" in str(e).lower():\n                raise ConnectionError(\n                    f\"MongoDB authentication failed: {e}\", adapter=\"async_mongo\"\n                ) from e\n            raise QueryError(\n                f\"MongoDB operation failure: {e}\", adapter=\"async_mongo\"\n            ) from e\n        except Exception as e:\n            raise ConnectionError(\n                f\"Failed to connect to MongoDB: {e}\", adapter=\"async_mongo\"\n            ) from e\n\n    # incoming\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=True, **kw):\n        try:\n            # Validate required parameters\n            if \"url\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'url'\", data=obj\n                )\n            if \"db\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'db'\", data=obj\n                )\n            if \"collection\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'collection'\", data=obj\n                )\n\n            # Create client and validate connection\n            client = cls._client(obj[\"url\"])\n            await cls._validate_connection(client)\n\n            # Get collection and execute query\n            try:\n                coll = client[obj[\"db\"]][obj[\"collection\"]]\n                filter_query = obj.get(\"filter\") or {}\n\n                # Validate filter query if provided\n                if filter_query and not isinstance(filter_query, dict):\n                    raise AdapterValidationError(\n                        \"Filter must be a dictionary\",\n                        data=filter_query,\n                    )\n\n                docs = await coll.find(filter_query).to_list(length=None)\n            except pymongo.errors.OperationFailure as e:\n                if \"not authorized\" in str(e).lower():\n                    raise ConnectionError(\n                        f\"Not authorized to access {obj['db']}.{obj['collection']}: {e}\",\n                        adapter=\"async_mongo\",\n                        url=obj[\"url\"],\n                    ) from e\n                raise QueryError(\n                    f\"MongoDB query error: {e}\",\n                    query=filter_query,\n                    adapter=\"async_mongo\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Error executing MongoDB query: {e}\",\n                    query=filter_query,\n                    adapter=\"async_mongo\",\n                ) from e\n\n            # Handle empty result set\n            if not docs:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No documents found matching the query\",\n                    resource=f\"{obj['db']}.{obj['collection']}\",\n                    filter=filter_query,\n                )\n\n            # Convert documents to model instances\n            try:\n                if many:\n                    return [subj_cls.model_validate(d) for d in docs]\n                return subj_cls.model_validate(docs[0])\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=docs[0] if not many else docs,\n                    errors=e.errors(),\n                ) from e\n\n        except (ConnectionError, QueryError, ResourceError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(\n                f\"Unexpected error in async MongoDB adapter: {e}\", adapter=\"async_mongo\"\n            )\n\n    # outgoing\n    @classmethod\n    async def to_obj(\n        cls, subj: T | Sequence[T], /, *, url, db, collection, many=True, **kw\n    ):\n        try:\n            # Validate required parameters\n            if not url:\n                raise AdapterValidationError(\"Missing required parameter 'url'\")\n            if not db:\n                raise AdapterValidationError(\"Missing required parameter 'db'\")\n            if not collection:\n                raise AdapterValidationError(\"Missing required parameter 'collection'\")\n\n            # Create client and validate connection\n            client = cls._client(url)\n            await cls._validate_connection(client)\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return None  # Nothing to insert\n\n            payload = [i.model_dump() for i in items]\n\n            # Execute insert\n            try:\n                result = await client[db][collection].insert_many(payload)\n                return {\"inserted_count\": len(result.inserted_ids)}\n            except pymongo.errors.BulkWriteError as e:\n                raise QueryError(\n                    f\"MongoDB bulk write error: {e}\",\n                    adapter=\"async_mongo\",\n                ) from e\n            except pymongo.errors.OperationFailure as e:\n                if \"not authorized\" in str(e).lower():\n                    raise ConnectionError(\n                        f\"Not authorized to write to {db}.{collection}: {e}\",\n                        adapter=\"async_mongo\",\n                        url=url,\n                    ) from e\n                raise QueryError(\n                    f\"MongoDB operation failure: {e}\",\n                    adapter=\"async_mongo\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Error inserting documents into MongoDB: {e}\",\n                    adapter=\"async_mongo\",\n                ) from e\n\n        except (ConnectionError, QueryError, AdapterValidationError):\n            # Re-raise our custom exceptions\n            raise\n        except Exception as e:\n            # Wrap other exceptions\n            raise QueryError(\n                f\"Unexpected error in async MongoDB adapter: {e}\", adapter=\"async_mongo\"\n            )\n</code></pre>"},{"location":"api/extras/#async-qdrant-adapter","title":"Async Qdrant Adapter","text":""},{"location":"api/extras/#pydapter.extras.async_qdrant_","title":"<code>pydapter.extras.async_qdrant_</code>","text":"<p>AsyncQdrantAdapter - vector upsert / search using AsyncQdrantClient.</p>"},{"location":"api/extras/#pydapter.extras.async_qdrant_-classes","title":"Classes","text":""},{"location":"api/extras/#pydapter.extras.async_qdrant_.AsyncQdrantAdapter","title":"<code>AsyncQdrantAdapter</code>","text":"<p>               Bases: <code>AsyncAdapter[T]</code></p> <p>Asynchronous Qdrant vector database adapter for async vector operations.</p> <p>This adapter provides async methods to: - Search for similar vectors asynchronously and convert results to Pydantic models - Insert Pydantic models as vector points into Qdrant collections asynchronously - Handle async vector similarity operations and metadata filtering - Support for both cloud and self-hosted Qdrant instances with async operations</p> <p>Attributes:</p> Name Type Description <code>obj_key</code> <p>The key identifier for this adapter type (\"async_qdrant\")</p> Example <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\nclass Document(BaseModel):\n    id: str\n    text: str\n    embedding: list[float]\n    category: str\n\nasync def main():\n    # Search for similar vectors\n    search_config = {\n        \"url\": \"http://localhost:6333\",\n        \"collection_name\": \"documents\",\n        \"query_vector\": [0.1, 0.2, 0.3, ...],  # 768-dim vector\n        \"limit\": 10,\n        \"score_threshold\": 0.8\n    }\n    similar_docs = await AsyncQdrantAdapter.from_obj(Document, search_config, many=True)\n\n    # Insert documents with vectors\n    insert_config = {\n        \"url\": \"http://localhost:6333\",\n        \"collection_name\": \"documents\"\n    }\n    new_docs = [Document(\n        id=\"doc1\",\n        text=\"Sample text\",\n        embedding=[0.1, 0.2, 0.3, ...],\n        category=\"tech\"\n    )]\n    await AsyncQdrantAdapter.to_obj(new_docs, insert_config, many=True)\n\nasyncio.run(main())\n</code></pre> Source code in <code>src/pydapter/extras/async_qdrant_.py</code> <pre><code>class AsyncQdrantAdapter(AsyncAdapter[T]):\n    \"\"\"\n    Asynchronous Qdrant vector database adapter for async vector operations.\n\n    This adapter provides async methods to:\n    - Search for similar vectors asynchronously and convert results to Pydantic models\n    - Insert Pydantic models as vector points into Qdrant collections asynchronously\n    - Handle async vector similarity operations and metadata filtering\n    - Support for both cloud and self-hosted Qdrant instances with async operations\n\n    Attributes:\n        obj_key: The key identifier for this adapter type (\"async_qdrant\")\n\n    Example:\n        ```python\n        import asyncio\n        from pydantic import BaseModel\n        from pydapter.extras.async_qdrant_ import AsyncQdrantAdapter\n\n        class Document(BaseModel):\n            id: str\n            text: str\n            embedding: list[float]\n            category: str\n\n        async def main():\n            # Search for similar vectors\n            search_config = {\n                \"url\": \"http://localhost:6333\",\n                \"collection_name\": \"documents\",\n                \"query_vector\": [0.1, 0.2, 0.3, ...],  # 768-dim vector\n                \"limit\": 10,\n                \"score_threshold\": 0.8\n            }\n            similar_docs = await AsyncQdrantAdapter.from_obj(Document, search_config, many=True)\n\n            # Insert documents with vectors\n            insert_config = {\n                \"url\": \"http://localhost:6333\",\n                \"collection_name\": \"documents\"\n            }\n            new_docs = [Document(\n                id=\"doc1\",\n                text=\"Sample text\",\n                embedding=[0.1, 0.2, 0.3, ...],\n                category=\"tech\"\n            )]\n            await AsyncQdrantAdapter.to_obj(new_docs, insert_config, many=True)\n\n        asyncio.run(main())\n        ```\n    \"\"\"\n\n    obj_key = \"async_qdrant\"\n\n    @staticmethod\n    def _client(url: str | None):\n        \"\"\"\n        Create an async Qdrant client with proper error handling.\n\n        Args:\n            url: Qdrant server URL or None for in-memory instance\n\n        Returns:\n            AsyncQdrantClient instance\n\n        Raises:\n            ConnectionError: If connection cannot be established\n        \"\"\"\n        try:\n            return AsyncQdrantClient(url=url) if url else AsyncQdrantClient(\":memory:\")\n        except UnexpectedResponse as e:\n            raise ConnectionError(\n                f\"Failed to connect to Qdrant: {e}\", adapter=\"async_qdrant\", url=url\n            ) from e\n        except Exception as e:\n            raise ConnectionError(\n                f\"Unexpected error connecting to Qdrant: {e}\",\n                adapter=\"async_qdrant\",\n                url=url,\n            ) from e\n\n    @staticmethod\n    def _validate_vector_dimensions(vector, expected_dim=None):\n        \"\"\"Validate that the vector has the correct dimensions.\"\"\"\n        if not isinstance(vector, (list, tuple)) or not all(\n            isinstance(x, (int, float)) for x in vector\n        ):\n            raise AdapterValidationError(\n                \"Vector must be a list or tuple of numbers\",\n                data=vector,\n            )\n\n        if expected_dim is not None and len(vector) != expected_dim:\n            raise AdapterValidationError(\n                f\"Vector dimension mismatch: expected {expected_dim}, got {len(vector)}\",\n                data=vector,\n            )\n\n    # outgoing\n    @classmethod\n    async def to_obj(\n        cls,\n        subj: T | Sequence[T],\n        /,\n        *,\n        collection,\n        vector_field=\"embedding\",\n        id_field=\"id\",\n        url=None,\n        **kw,\n    ):\n        try:\n            # Validate required parameters\n            if not collection:\n                raise AdapterValidationError(\"Missing required parameter 'collection'\")\n\n            # Prepare data\n            items = subj if isinstance(subj, Sequence) else [subj]\n            if not items:\n                return None  # Nothing to insert\n\n            # Validate vector field exists\n            if not hasattr(items[0], vector_field):\n                raise AdapterValidationError(\n                    f\"Vector field '{vector_field}' not found in model\",\n                    data=items[0].model_dump(),\n                )\n\n            # Validate ID field exists\n            if not hasattr(items[0], id_field):\n                raise AdapterValidationError(\n                    f\"ID field '{id_field}' not found in model\",\n                    data=items[0].model_dump(),\n                )\n\n            # Get vector dimension\n            vector = getattr(items[0], vector_field)\n            cls._validate_vector_dimensions(vector)\n            dim = len(vector)\n\n            # Create client\n            client = cls._client(url)\n\n            # Create or recreate collection\n            try:\n                await client.recreate_collection(\n                    collection,\n                    vectors_config=qd.VectorParams(size=dim, distance=\"Cosine\"),\n                )\n            except UnexpectedResponse as e:\n                raise QueryError(\n                    f\"Failed to create Qdrant collection: {e}\",\n                    adapter=\"async_qdrant\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Unexpected error creating Qdrant collection: {e}\",\n                    adapter=\"async_qdrant\",\n                ) from e\n\n            # Create points\n            try:\n                points = []\n                for i, item in enumerate(items):\n                    vector = getattr(item, vector_field)\n                    cls._validate_vector_dimensions(vector, dim)\n\n                    points.append(\n                        qd.PointStruct(\n                            id=getattr(item, id_field),\n                            vector=vector,\n                            payload=item.model_dump(exclude={vector_field}),\n                        )\n                    )\n            except AdapterValidationError:\n                # Re-raise validation errors\n                raise\n            except Exception as e:\n                raise AdapterValidationError(\n                    f\"Error creating Qdrant points: {e}\",\n                    data=items,\n                ) from e\n\n            # Upsert points\n            try:\n                await client.upsert(collection, points)\n                return {\"upserted_count\": len(points)}\n            except UnexpectedResponse as e:\n                raise QueryError(\n                    f\"Failed to upsert points to Qdrant: {e}\",\n                    adapter=\"async_qdrant\",\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Unexpected error upserting points to Qdrant: {e}\",\n                    adapter=\"async_qdrant\",\n                ) from e\n\n        except AdapterError:\n            raise\n\n        except Exception as e:\n            raise QueryError(\n                f\"Unexpected error in async Qdrant adapter: {e}\", adapter=\"async_qdrant\"\n            )\n\n    # incoming\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=True, **kw):\n        try:\n            if \"collection\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'collection'\", data=obj\n                )\n            if \"query_vector\" not in obj:\n                raise AdapterValidationError(\n                    \"Missing required parameter 'query_vector'\", data=obj\n                )\n\n            # Validate query vector &amp; Create client\n            cls._validate_vector_dimensions(obj[\"query_vector\"])\n            client = cls._client(obj.get(\"url\"))\n\n            # Execute search\n            try:\n                res = await client.search(\n                    obj[\"collection\"],\n                    obj[\"query_vector\"],\n                    limit=obj.get(\"top_k\", 5),\n                    with_payload=True,\n                )\n            except UnexpectedResponse as e:\n                if \"not found\" in str(e).lower():\n                    raise ResourceError(\n                        f\"Qdrant collection not found: {e}\",\n                        resource=obj[\"collection\"],\n                    ) from e\n                raise QueryError(\n                    f\"Failed to search Qdrant: {e}\",\n                    adapter=\"async_qdrant\",\n                ) from e\n            except grpc.RpcError as e:\n                raise ConnectionError(\n                    f\"Qdrant RPC error: {e}\",\n                    adapter=\"async_qdrant\",\n                    url=obj.get(\"url\"),\n                ) from e\n            except Exception as e:\n                raise QueryError(\n                    f\"Unexpected error searching Qdrant: {e}\",\n                    adapter=\"async_qdrant\",\n                ) from e\n\n            # Extract payloads\n            docs = [r.payload for r in res]\n\n            # Handle empty result set\n            if not docs:\n                if many:\n                    return []\n                raise ResourceError(\n                    \"No points found matching the query vector\",\n                    resource=obj[\"collection\"],\n                )\n\n            # Convert documents to model instances\n            try:\n                if many:\n                    return [subj_cls.model_validate(d) for d in docs]\n                return subj_cls.model_validate(docs[0])\n            except ValidationError as e:\n                raise AdapterValidationError(\n                    f\"Validation error: {e}\",\n                    data=docs[0] if not many else docs,\n                    errors=e.errors(),\n                ) from e\n\n        except AdapterError:\n            raise\n\n        except Exception as e:\n            raise QueryError(\n                f\"Unexpected error in async Qdrant adapter: {e}\", adapter=\"async_qdrant\"\n            )\n</code></pre>"},{"location":"api/fields/","title":"Fields API Reference","text":"<p>The <code>pydapter.fields</code> module provides a robust system for defining and managing data fields with enhanced validation, type transformation, and protocol integration.</p>"},{"location":"api/fields/#installation","title":"Installation","text":"<pre><code>pip install pydapter\n</code></pre>"},{"location":"api/fields/#overview","title":"Overview","text":"<p>The fields module extends Pydantic's field system with additional features:</p> <ul> <li>Enhanced Field Descriptors: More powerful than standard Pydantic fields</li> <li>Type Transformation: Convert fields to nullable or listable variants</li> <li>Pre-defined Field Types: Common field patterns for consistency</li> <li>Validation Integration: Flexible validator attachment</li> <li>Protocol Support: Seamless integration with pydapter protocols</li> </ul>"},{"location":"api/fields/#core-classes","title":"Core Classes","text":""},{"location":"api/fields/#field","title":"Field","text":"<p>Module: <code>pydapter.fields.types</code></p> <p>Enhanced field descriptor that provides advanced functionality over Pydantic's standard fields.</p> <p>Constructor:</p> <pre><code>class Field:\n    def __init__(\n        self,\n        name: str,\n        annotation: type | UndefinedType = Undefined,\n        default: Any = Undefined,\n        default_factory: Callable | UndefinedType = Undefined,\n        title: str | UndefinedType = Undefined,\n        description: str | UndefinedType = Undefined,\n        examples: list | UndefinedType = Undefined,\n        exclude: bool | UndefinedType = Undefined,\n        frozen: bool | UndefinedType = Undefined,\n        validator: Callable | UndefinedType = Undefined,\n        validator_kwargs: dict = Undefined,\n        alias: str | UndefinedType = Undefined,\n        immutable: bool = False,\n        **extra_info: Any,\n    )\n</code></pre> <p>Key Methods:</p> <ul> <li><code>copy(**kwargs)</code>: Create a copy with updated values</li> <li><code>as_nullable()</code>: Create nullable variant with None default</li> <li><code>as_listable(strict=False)</code>: Create list variant</li> <li><code>field_info</code> property: Returns Pydantic FieldInfo object</li> <li><code>field_validator</code> property: Returns validator dictionary</li> </ul> <p>Basic Usage:</p> <pre><code>from pydapter.fields import Field\n\n# Define a validated field\nname_field = Field(\n    name=\"name\",\n    annotation=str,\n    title=\"User Name\",\n    description=\"The user's full name\",\n    validator=lambda cls, v: v.strip().title()\n)\n\n# Create variants\noptional_name = name_field.as_nullable()\nname_list = name_field.as_listable()\n</code></pre> <p>Advanced Usage:</p> <pre><code># Custom email field with validation\nemail_field = Field(\n    name=\"email\",\n    annotation=str,\n    validator=lambda cls, v: v.lower() if \"@\" in v else ValueError(\"Invalid email\"),\n    title=\"Email Address\",\n    description=\"User's email address\",\n    examples=[\"user@example.com\"]\n)\n\n# Immutable field\nid_field = Field(\n    name=\"id\",\n    annotation=str,\n    immutable=True,\n    frozen=True,\n    description=\"Unique identifier\"\n)\n</code></pre>"},{"location":"api/fields/#undefinedtype","title":"UndefinedType","text":"<p>Module: <code>pydapter.fields.types</code></p> <p>Sentinel type for undefined values in field definitions.</p> <pre><code>class UndefinedType:\n    def __bool__(self) -&gt; Literal[False]:\n        return False\n\n    def __repr__(self) -&gt; Literal[\"UNDEFINED\"]:\n        return \"UNDEFINED\"\n\nUndefined = UndefinedType()  # Singleton instance\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.fields.types import Undefined\n\n# Check if value is undefined\nif field.default is not Undefined:\n    print(\"Field has a default value\")\n</code></pre>"},{"location":"api/fields/#pre-defined-fields","title":"Pre-defined Fields","text":""},{"location":"api/fields/#identifier-fields","title":"Identifier Fields","text":"<p>Module: <code>pydapter.fields.ids</code></p>"},{"location":"api/fields/#id_frozen","title":"ID_FROZEN","text":"<p>Immutable UUID field for entity identification.</p> <pre><code>ID_FROZEN = Field(\n    name=\"id\",\n    annotation=UUID,\n    default_factory=uuid4,\n    frozen=True,\n    validator=validate_uuid,\n    immutable=True\n)\n</code></pre>"},{"location":"api/fields/#id_mutable","title":"ID_MUTABLE","text":"<p>Mutable UUID field that can be updated.</p> <pre><code>ID_MUTABLE = Field(\n    name=\"id\",\n    annotation=UUID,\n    default_factory=uuid4,\n    validator=validate_uuid\n)\n</code></pre>"},{"location":"api/fields/#id_nullable","title":"ID_NULLABLE","text":"<p>Optional UUID field that can be None.</p> <pre><code>ID_NULLABLE = Field(\n    name=\"id\",\n    annotation=UUID | None,\n    default=None,\n    validator=validate_uuid\n)\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.fields import ID_FROZEN, create_model\n\n# Use in model creation\nUser = create_model(\n    \"User\",\n    fields=[\n        ID_FROZEN.copy(name=\"user_id\"),\n        Field(name=\"name\", annotation=str)\n    ]\n)\n</code></pre>"},{"location":"api/fields/#datetime-fields","title":"DateTime Fields","text":"<p>Module: <code>pydapter.fields.dts</code></p>"},{"location":"api/fields/#datetime","title":"DATETIME","text":"<p>Standard datetime field with UTC timezone.</p> <pre><code>DATETIME = Field(\n    name=\"timestamp\",\n    annotation=datetime,\n    default_factory=lambda: datetime.now(timezone.utc),\n    validator=validate_datetime,\n    serializer=datetime_serializer\n)\n</code></pre>"},{"location":"api/fields/#datetime_nullable","title":"DATETIME_NULLABLE","text":"<p>Optional datetime field.</p> <pre><code>DATETIME_NULLABLE = DATETIME.as_nullable()\n</code></pre> <p>Validation Function:</p> <pre><code>def validate_datetime(cls, v) -&gt; datetime:\n    \"\"\"Validates and ensures timezone-aware datetime\"\"\"\n    if isinstance(v, str):\n        v = datetime.fromisoformat(v)\n    if v.tzinfo is None:\n        v = v.replace(tzinfo=timezone.utc)\n    return v\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.fields import DATETIME, DATETIME_NULLABLE\n\n# In model creation\nArticle = create_model(\n    \"Article\",\n    fields=[\n        DATETIME.copy(name=\"created_at\"),\n        DATETIME_NULLABLE.copy(name=\"published_at\")\n    ]\n)\n</code></pre>"},{"location":"api/fields/#embedding-fields","title":"Embedding Fields","text":"<p>Module: <code>pydapter.fields.embedding</code></p>"},{"location":"api/fields/#embedding","title":"EMBEDDING","text":"<p>Vector embedding field for AI/ML applications.</p> <pre><code>EMBEDDING = Field(\n    name=\"embedding\",\n    annotation=list[float] | None,\n    default=None,\n    validator=validate_embedding,\n    title=\"Vector Embedding\",\n    description=\"High-dimensional vector representation\"\n)\n</code></pre> <p>Validation Function:</p> <pre><code>def validate_embedding(cls, v) -&gt; list[float] | None:\n    \"\"\"Validates embedding vectors\"\"\"\n    if v is None:\n        return v\n    if not isinstance(v, list):\n        raise ValueError(\"Embedding must be a list\")\n    if not all(isinstance(x, (int, float)) for x in v):\n        raise ValueError(\"Embedding values must be numeric\")\n    return [float(x) for x in v]\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.fields import EMBEDDING\n\n# Document with embedding\nDocument = create_model(\n    \"Document\",\n    fields=[\n        Field(name=\"content\", annotation=str),\n        EMBEDDING.copy(name=\"content_embedding\")\n    ]\n)\n</code></pre>"},{"location":"api/fields/#execution-fields","title":"Execution Fields","text":"<p>Module: <code>pydapter.fields.execution</code></p>"},{"location":"api/fields/#execution","title":"EXECUTION","text":"<p>Execution state tracking field.</p> <pre><code>EXECUTION = Field(\n    name=\"execution\",\n    annotation=Execution,\n    default_factory=Execution,\n    validator=lambda cls, v: v or Execution(),\n    validator_kwargs={\"mode\": \"before\"},\n    immutable=True\n)\n</code></pre> <p>Execution Model:</p> <pre><code>class ExecutionStatus(str, Enum):\n    PENDING = \"pending\"\n    PROCESSING = \"processing\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\nclass Execution(BaseModel):\n    duration: float | None = None           # Execution time in seconds\n    response: dict | None = None            # Serialized response\n    status: ExecutionStatus = PENDING       # Current status\n    error: str | None = None               # Error message if failed\n    response_obj: Any = Field(None, exclude=True)  # Original response\n    updated_at: datetime | None = Field(    # Last update timestamp\n        default_factory=lambda: datetime.now(tz=timezone.utc),\n        exclude=True\n    )\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.fields import EXECUTION\nfrom pydapter.fields.execution import ExecutionStatus\n\n# Task with execution tracking\nTask = create_model(\n    \"Task\",\n    fields=[\n        Field(name=\"name\", annotation=str),\n        EXECUTION.copy(name=\"execution\")\n    ]\n)\n\ntask = Task(name=\"Process Data\", execution=Execution())\ntask.execution.status = ExecutionStatus.PROCESSING\n</code></pre>"},{"location":"api/fields/#parameter-fields","title":"Parameter Fields","text":"<p>Module: <code>pydapter.fields.params</code></p>"},{"location":"api/fields/#params","title":"PARAMS","text":"<p>General parameter dictionary field.</p> <pre><code>PARAMS = Field(\n    name=\"params\",\n    annotation=dict,\n    default_factory=dict,\n    validator=validate_model_to_params,\n    title=\"Parameters\",\n    description=\"Key-value parameter mapping\"\n)\n</code></pre>"},{"location":"api/fields/#param_type","title":"PARAM_TYPE","text":"<p>Parameter type field.</p> <pre><code>PARAM_TYPE = Field(\n    name=\"param_type\",\n    annotation=type,\n    validator=validate_model_to_type,\n    title=\"Parameter Type\"\n)\n</code></pre>"},{"location":"api/fields/#param_type_nullable","title":"PARAM_TYPE_NULLABLE","text":"<p>Optional parameter type field.</p> <pre><code>PARAM_TYPE_NULLABLE = PARAM_TYPE.as_nullable()\n</code></pre> <p>Validation Functions:</p> <pre><code>def validate_model_to_params(v) -&gt; dict:\n    \"\"\"Converts models to parameter dictionaries\"\"\"\n    if hasattr(v, 'model_dump'):\n        return v.model_dump()\n    elif isinstance(v, dict):\n        return v\n    else:\n        return {\"value\": v}\n\ndef validate_model_to_type(v) -&gt; type:\n    \"\"\"Validates and extracts type information\"\"\"\n    if isinstance(v, type):\n        return v\n    return type(v)\n</code></pre>"},{"location":"api/fields/#type-definitions","title":"Type Definitions","text":""},{"location":"api/fields/#core-types","title":"Core Types","text":"<p>Module: <code>pydapter.fields.types</code></p> <pre><code># Basic type aliases\nID = UUID                    # Unique identifier type\nEmbedding = list[float]      # Vector embedding type\nMetadata = dict             # General metadata type\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.fields.types import ID, Embedding, Metadata\n\n# Type-annotated model\nclass Document(BaseModel):\n    id: ID\n    embedding: Embedding\n    metadata: Metadata\n</code></pre>"},{"location":"api/fields/#utility-functions","title":"Utility Functions","text":""},{"location":"api/fields/#model-creation","title":"Model Creation","text":""},{"location":"api/fields/#create_model","title":"create_model","text":"<p>Function: <code>create_model</code></p> <p>Enhanced model creation that integrates with the Field system.</p> <p>Signature:</p> <pre><code>def create_model(\n    model_name: str,\n    config: dict[str, Any] = None,\n    doc: str = None,\n    base: type[BaseModel] = None,\n    fields: list[Field] = None,\n    frozen: bool = False,\n) -&gt; type[BaseModel]\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.fields import Field, create_model, ID_FROZEN, DATETIME\n\n# Define fields\nuser_fields = [\n    ID_FROZEN.copy(name=\"id\"),\n    Field(name=\"name\", annotation=str, title=\"User Name\"),\n    Field(name=\"email\", annotation=str, validator=validate_email),\n    DATETIME.copy(name=\"created_at\")\n]\n\n# Create model\nUser = create_model(\n    model_name=\"User\",\n    doc=\"User model with validation\",\n    fields=user_fields,\n    frozen=True\n)\n\n# Use the model\nuser = User(name=\"John Doe\", email=\"john@example.com\")\n</code></pre> <p>Advanced Usage:</p> <pre><code># With base class\nclass BaseEntity(BaseModel):\n    created_by: str\n\n# Custom configuration\nconfig = {\"str_strip_whitespace\": True}\n\n# Create enhanced model\nProduct = create_model(\n    model_name=\"Product\",\n    config=config,\n    base=BaseEntity,\n    fields=[\n        Field(name=\"name\", annotation=str),\n        Field(name=\"price\", annotation=float, validator=lambda cls, v: max(0, v))\n    ]\n)\n</code></pre>"},{"location":"api/fields/#validation-functions","title":"Validation Functions","text":""},{"location":"api/fields/#uuid-validation","title":"UUID Validation","text":"<p>Module: <code>pydapter.fields.ids</code></p> <pre><code>def validate_uuid(cls, v) -&gt; UUID:\n    \"\"\"Validates and converts to UUID\"\"\"\n    if isinstance(v, str):\n        return UUID(v)\n    elif isinstance(v, UUID):\n        return v\n    else:\n        raise ValueError(\"Invalid UUID format\")\n\ndef serialize_uuid(v: UUID) -&gt; str:\n    \"\"\"Serializes UUID to string\"\"\"\n    return str(v)\n</code></pre>"},{"location":"api/fields/#datetime-validation","title":"DateTime Validation","text":"<p>Module: <code>pydapter.fields.dts</code></p> <pre><code>def validate_datetime(cls, v) -&gt; datetime:\n    \"\"\"Validates and normalizes datetime\"\"\"\n    if isinstance(v, str):\n        v = datetime.fromisoformat(v)\n    if v.tzinfo is None:\n        v = v.replace(tzinfo=timezone.utc)\n    return v\n\ndef datetime_serializer(v: datetime) -&gt; str:\n    \"\"\"Serializes datetime to ISO format\"\"\"\n    return v.isoformat()\n</code></pre>"},{"location":"api/fields/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/fields/#custom-field-types","title":"Custom Field Types","text":"<p>Create reusable field factories:</p> <pre><code>from pydapter.fields import Field\n\ndef create_email_field(name: str, required: bool = True) -&gt; Field:\n    \"\"\"Factory for email fields\"\"\"\n    def validate_email(cls, v):\n        if \"@\" not in v or \".\" not in v:\n            raise ValueError(\"Invalid email format\")\n        return v.lower().strip()\n\n    return Field(\n        name=name,\n        annotation=str if required else str | None,\n        default=None if not required else Undefined,\n        validator=validate_email,\n        title=\"Email Address\",\n        description=\"A valid email address\"\n    )\n\n# Usage\nuser_email = create_email_field(\"email\")\ncontact_email = create_email_field(\"contact_email\", required=False)\n</code></pre>"},{"location":"api/fields/#field-transformation","title":"Field Transformation","text":"<p>Use transformation methods for field variants:</p> <pre><code># Base field\nname_field = Field(name=\"name\", annotation=str)\n\n# Create variants\noptional_name = name_field.as_nullable()        # str | None with default=None\nname_list = name_field.as_listable(strict=True) # list[str]\nflexible_names = name_field.as_listable()       # list[str] | str\n\n# Use in models\nUserModel = create_model(\"User\", fields=[\n    name_field,\n    optional_name.copy(name=\"nickname\"),\n    name_list.copy(name=\"aliases\")\n])\n</code></pre>"},{"location":"api/fields/#protocol-integration","title":"Protocol Integration","text":"<p>Fields integrate seamlessly with protocols:</p> <pre><code>from pydapter.fields import ID_FROZEN, DATETIME, EMBEDDING, EXECUTION\nfrom pydapter.protocols.event import BASE_EVENT_FIELDS\n\n# Event uses pre-defined fields\nprint([f.name for f in BASE_EVENT_FIELDS])\n# ['id', 'created_at', 'updated_at', 'embedding', 'execution', 'request', 'content', 'event_type']\n\n# Custom protocol with standard fields\ncustom_fields = [\n    ID_FROZEN.copy(name=\"id\"),\n    DATETIME.copy(name=\"timestamp\"),\n    Field(name=\"priority\", annotation=int, default=1),\n    Field(name=\"category\", annotation=str)\n]\n\nCustomEvent = create_model(\"CustomEvent\", fields=custom_fields)\n</code></pre>"},{"location":"api/fields/#validation-patterns","title":"Validation Patterns","text":"<p>Common validation patterns:</p> <pre><code># Range validation\ndef create_range_field(name: str, min_val: int, max_val: int) -&gt; Field:\n    def validate_range(cls, v):\n        if not min_val &lt;= v &lt;= max_val:\n            raise ValueError(f\"Value must be between {min_val} and {max_val}\")\n        return v\n\n    return Field(\n        name=name,\n        annotation=int,\n        validator=validate_range,\n        description=f\"Integer value between {min_val} and {max_val}\"\n    )\n\n# String normalization\ndef create_normalized_string_field(name: str) -&gt; Field:\n    def normalize_string(cls, v):\n        return v.strip().lower() if v else v\n\n    return Field(\n        name=name,\n        annotation=str,\n        validator=normalize_string\n    )\n\n# List validation\ndef create_validated_list_field(name: str, item_validator: callable) -&gt; Field:\n    def validate_list(cls, v):\n        if not isinstance(v, list):\n            raise ValueError(\"Value must be a list\")\n        return [item_validator(cls, item) for item in v]\n\n    return Field(\n        name=name,\n        annotation=list,\n        validator=validate_list\n    )\n</code></pre>"},{"location":"api/fields/#best-practices","title":"Best Practices","text":""},{"location":"api/fields/#field-design","title":"Field Design","text":"<ol> <li>Use Pre-defined Fields: Leverage existing field definitions for    consistency</li> <li>Immutability: Use <code>immutable=True</code> for fields that shouldn't change</li> <li>Validation: Always include appropriate validators for data integrity</li> <li>Documentation: Provide clear titles and descriptions</li> <li>Type Safety: Use proper type annotations</li> </ol>"},{"location":"api/fields/#model-creation_1","title":"Model Creation","text":"<ol> <li>Field Lists: Organize fields in logical lists for reuse</li> <li>Base Classes: Use base classes for common functionality</li> <li>Configuration: Apply consistent model configuration</li> <li>Frozen Models: Use <code>frozen=True</code> for immutable data structures</li> </ol>"},{"location":"api/fields/#performance","title":"Performance","text":"<ol> <li>Validator Efficiency: Keep validators fast and simple</li> <li>Field Reuse: Reuse field definitions instead of recreating</li> <li>Lazy Evaluation: Use default factories for expensive computations</li> <li>Minimal Fields: Only include fields you actually need</li> </ol>"},{"location":"api/fields/#integration","title":"Integration","text":"<ol> <li>Protocol Compatibility: Design fields to work with protocols</li> <li>Adapter Support: Ensure fields work with adapters</li> <li>Serialization: Test field serialization/deserialization</li> <li>Migration: Plan for field evolution and backward compatibility</li> </ol>"},{"location":"api/fields/#migration-guide","title":"Migration Guide","text":"<p>When upgrading from previous versions:</p> <ol> <li>Field Definitions: Update to use new Field class</li> <li>Validation: Migrate custom validators to new format</li> <li>Model Creation: Use <code>create_model</code> function</li> <li>Type Annotations: Add proper type hints</li> <li>Protocol Integration: Leverage pre-defined fields for protocols</li> </ol> <p>For detailed migration instructions, see the Migration Guide.</p>"},{"location":"api/migrations/","title":"Migrations API Reference","text":"<p>This page provides detailed API documentation for the <code>pydapter.migrations</code> module.</p>"},{"location":"api/migrations/#installation","title":"Installation","text":"<p>The migrations module is available as optional dependencies:</p> <pre><code># Core migrations functionality\npip install \"pydapter[migrations-core]\"\n\n# SQL migrations with Alembic support\npip install \"pydapter[migrations-sql]\"\n\n# All migrations components\npip install \"pydapter[migrations]\"\n</code></pre>"},{"location":"api/migrations/#module-overview","title":"Module Overview","text":"<p>The migrations module provides a framework for managing database schema changes, following the adapter pattern:</p> <pre><code>MigrationProtocol\n       \u2502\n       \u25bc\nBaseMigrationAdapter\n       \u2502\n       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502                     \u2502\n       \u25bc                     \u25bc\nSyncMigrationAdapter    AsyncMigrationAdapter\n       \u2502                     \u2502\n       \u25bc                     \u25bc\n AlembicAdapter        AsyncAlembicAdapter\n</code></pre>"},{"location":"api/migrations/#protocols","title":"Protocols","text":""},{"location":"api/migrations/#migrationprotocol","title":"MigrationProtocol","text":""},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol","title":"<code>pydapter.migrations.protocols.MigrationProtocol</code>","text":"<p>               Bases: <code>Protocol[T]</code></p> <p>Protocol defining synchronous migration operations.</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@runtime_checkable\nclass MigrationProtocol(Protocol[T]):\n    \"\"\"Protocol defining synchronous migration operations.\"\"\"\n\n    migration_key: ClassVar[str]\n\n    @classmethod\n    def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize migration environment in the specified directory.\n\n        Args:\n            directory: Path to the directory where migrations will be stored\n            **kwargs: Additional adapter-specific arguments\n        \"\"\"\n        ...\n\n    @classmethod\n    def create_migration(\n        cls, message: str, autogenerate: bool = True, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"\n        Create a new migration.\n\n        Args:\n            message: Description of the migration\n            autogenerate: Whether to auto-generate the migration based on model changes\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The revision identifier of the created migration\n        \"\"\"\n        ...\n\n    @classmethod\n    def upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n        \"\"\"\n        Upgrade to the specified revision.\n\n        Args:\n            revision: The target revision to upgrade to (default: \"head\")\n            **kwargs: Additional adapter-specific arguments\n        \"\"\"\n        ...\n\n    @classmethod\n    def downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Downgrade to the specified revision.\n\n        Args:\n            revision: The target revision to downgrade to\n            **kwargs: Additional adapter-specific arguments\n        \"\"\"\n        ...\n\n    @classmethod\n    def get_current_revision(cls, **kwargs: Any) -&gt; str | None:\n        \"\"\"\n        Get the current migration revision.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The current revision identifier, or None if no migrations have been applied\n        \"\"\"\n        ...\n\n    @classmethod\n    def get_migration_history(cls, **kwargs: Any) -&gt; list[dict]:\n        \"\"\"\n        Get the migration history.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            A list of dictionaries containing migration information\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol.create_migration","title":"<code>create_migration(message, autogenerate=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new migration.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Description of the migration</p> required <code>autogenerate</code> <code>bool</code> <p>Whether to auto-generate the migration based on model changes</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The revision identifier of the created migration</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\ndef create_migration(\n    cls, message: str, autogenerate: bool = True, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Create a new migration.\n\n    Args:\n        message: Description of the migration\n        autogenerate: Whether to auto-generate the migration based on model changes\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The revision identifier of the created migration\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol.downgrade","title":"<code>downgrade(revision, **kwargs)</code>  <code>classmethod</code>","text":"<p>Downgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to downgrade to</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\ndef downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Downgrade to the specified revision.\n\n    Args:\n        revision: The target revision to downgrade to\n        **kwargs: Additional adapter-specific arguments\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol.get_current_revision","title":"<code>get_current_revision(**kwargs)</code>  <code>classmethod</code>","text":"<p>Get the current migration revision.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str | None</code> <p>The current revision identifier, or None if no migrations have been applied</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\ndef get_current_revision(cls, **kwargs: Any) -&gt; str | None:\n    \"\"\"\n    Get the current migration revision.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The current revision identifier, or None if no migrations have been applied\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol.get_migration_history","title":"<code>get_migration_history(**kwargs)</code>  <code>classmethod</code>","text":"<p>Get the migration history.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries containing migration information</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\ndef get_migration_history(cls, **kwargs: Any) -&gt; list[dict]:\n    \"\"\"\n    Get the migration history.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        A list of dictionaries containing migration information\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol.init_migrations","title":"<code>init_migrations(directory, **kwargs)</code>  <code>classmethod</code>","text":"<p>Initialize migration environment in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory where migrations will be stored</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\ndef init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize migration environment in the specified directory.\n\n    Args:\n        directory: Path to the directory where migrations will be stored\n        **kwargs: Additional adapter-specific arguments\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.MigrationProtocol.upgrade","title":"<code>upgrade(revision='head', **kwargs)</code>  <code>classmethod</code>","text":"<p>Upgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to upgrade to (default: \"head\")</p> <code>'head'</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\ndef upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n    \"\"\"\n    Upgrade to the specified revision.\n\n    Args:\n        revision: The target revision to upgrade to (default: \"head\")\n        **kwargs: Additional adapter-specific arguments\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#asyncmigrationprotocol","title":"AsyncMigrationProtocol","text":""},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol","title":"<code>pydapter.migrations.protocols.AsyncMigrationProtocol</code>","text":"<p>               Bases: <code>Protocol[T]</code></p> <p>Protocol defining asynchronous migration operations.</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@runtime_checkable\nclass AsyncMigrationProtocol(Protocol[T]):\n    \"\"\"Protocol defining asynchronous migration operations.\"\"\"\n\n    migration_key: ClassVar[str]\n\n    @classmethod\n    async def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize migration environment in the specified directory.\n\n        Args:\n            directory: Path to the directory where migrations will be stored\n            **kwargs: Additional adapter-specific arguments\n        \"\"\"\n        ...\n\n    @classmethod\n    async def create_migration(\n        cls, message: str, autogenerate: bool = True, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"\n        Create a new migration.\n\n        Args:\n            message: Description of the migration\n            autogenerate: Whether to auto-generate the migration based on model changes\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The revision identifier of the created migration\n        \"\"\"\n        ...\n\n    @classmethod\n    async def upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n        \"\"\"\n        Upgrade to the specified revision.\n\n        Args:\n            revision: The target revision to upgrade to (default: \"head\")\n            **kwargs: Additional adapter-specific arguments\n        \"\"\"\n        ...\n\n    @classmethod\n    async def downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Downgrade to the specified revision.\n\n        Args:\n            revision: The target revision to downgrade to\n            **kwargs: Additional adapter-specific arguments\n        \"\"\"\n        ...\n\n    @classmethod\n    async def get_current_revision(cls, **kwargs: Any) -&gt; str | None:\n        \"\"\"\n        Get the current migration revision.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The current revision identifier, or None if no migrations have been applied\n        \"\"\"\n        ...\n\n    @classmethod\n    async def get_migration_history(cls, **kwargs: Any) -&gt; list[dict]:\n        \"\"\"\n        Get the migration history.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            A list of dictionaries containing migration information\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol.create_migration","title":"<code>create_migration(message, autogenerate=True, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Create a new migration.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Description of the migration</p> required <code>autogenerate</code> <code>bool</code> <p>Whether to auto-generate the migration based on model changes</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The revision identifier of the created migration</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\nasync def create_migration(\n    cls, message: str, autogenerate: bool = True, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Create a new migration.\n\n    Args:\n        message: Description of the migration\n        autogenerate: Whether to auto-generate the migration based on model changes\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The revision identifier of the created migration\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol.downgrade","title":"<code>downgrade(revision, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Downgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to downgrade to</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\nasync def downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Downgrade to the specified revision.\n\n    Args:\n        revision: The target revision to downgrade to\n        **kwargs: Additional adapter-specific arguments\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol.get_current_revision","title":"<code>get_current_revision(**kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Get the current migration revision.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str | None</code> <p>The current revision identifier, or None if no migrations have been applied</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\nasync def get_current_revision(cls, **kwargs: Any) -&gt; str | None:\n    \"\"\"\n    Get the current migration revision.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The current revision identifier, or None if no migrations have been applied\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol.get_migration_history","title":"<code>get_migration_history(**kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Get the migration history.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries containing migration information</p> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\nasync def get_migration_history(cls, **kwargs: Any) -&gt; list[dict]:\n    \"\"\"\n    Get the migration history.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        A list of dictionaries containing migration information\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol.init_migrations","title":"<code>init_migrations(directory, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Initialize migration environment in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory where migrations will be stored</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\nasync def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize migration environment in the specified directory.\n\n    Args:\n        directory: Path to the directory where migrations will be stored\n        **kwargs: Additional adapter-specific arguments\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.protocols.AsyncMigrationProtocol.upgrade","title":"<code>upgrade(revision='head', **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Upgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to upgrade to (default: \"head\")</p> <code>'head'</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> Source code in <code>src/pydapter/migrations/protocols.py</code> <pre><code>@classmethod\nasync def upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n    \"\"\"\n    Upgrade to the specified revision.\n\n    Args:\n        revision: The target revision to upgrade to (default: \"head\")\n        **kwargs: Additional adapter-specific arguments\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/migrations/#base-classes","title":"Base Classes","text":""},{"location":"api/migrations/#basemigrationadapter","title":"BaseMigrationAdapter","text":""},{"location":"api/migrations/#pydapter.migrations.base.BaseMigrationAdapter","title":"<code>pydapter.migrations.base.BaseMigrationAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for migration adapters.</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>class BaseMigrationAdapter(ABC):\n    \"\"\"Base class for migration adapters.\"\"\"\n\n    migration_key: ClassVar[str]\n\n    def __init__(self, connection_string: str, models_module: Any = None):\n        \"\"\"\n        Initialize the migration adapter.\n\n        Args:\n            connection_string: Database connection string\n            models_module: Optional module containing model definitions\n        \"\"\"\n        self.connection_string = connection_string\n        self.models_module = models_module\n        self._initialized = False\n        self._migrations_dir = None\n\n    def _ensure_directory(self, directory: str) -&gt; None:\n        \"\"\"\n        Ensure the directory exists.\n\n        Args:\n            directory: Directory path\n        \"\"\"\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n    def _check_initialized(self) -&gt; None:\n        \"\"\"\n        Check if migrations have been initialized.\n\n        Raises:\n            MigrationError: If migrations have not been initialized\n        \"\"\"\n        if not self._initialized:\n            raise MigrationError(\n                \"Migrations have not been initialized. Call init_migrations first.\",\n                adapter=self.__class__.migration_key,\n            )\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.BaseMigrationAdapter-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.base.BaseMigrationAdapter.__init__","title":"<code>__init__(connection_string, models_module=None)</code>","text":"<p>Initialize the migration adapter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_string</code> <code>str</code> <p>Database connection string</p> required <code>models_module</code> <code>Any</code> <p>Optional module containing model definitions</p> <code>None</code> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>def __init__(self, connection_string: str, models_module: Any = None):\n    \"\"\"\n    Initialize the migration adapter.\n\n    Args:\n        connection_string: Database connection string\n        models_module: Optional module containing model definitions\n    \"\"\"\n    self.connection_string = connection_string\n    self.models_module = models_module\n    self._initialized = False\n    self._migrations_dir = None\n</code></pre>"},{"location":"api/migrations/#syncmigrationadapter","title":"SyncMigrationAdapter","text":""},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter","title":"<code>pydapter.migrations.base.SyncMigrationAdapter</code>","text":"<p>               Bases: <code>BaseMigrationAdapter</code>, <code>MigrationProtocol</code></p> <p>Base class for synchronous migration adapters.</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>class SyncMigrationAdapter(BaseMigrationAdapter, MigrationProtocol):\n    \"\"\"Base class for synchronous migration adapters.\"\"\"\n\n    @classmethod\n    def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize migration environment in the specified directory.\n\n        Args:\n            directory: Path to the directory where migrations will be stored\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationInitError: If initialization fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement init_migrations\")\n\n    @classmethod\n    def create_migration(\n        cls, message: str, autogenerate: bool = True, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"\n        Create a new migration.\n\n        Args:\n            message: Description of the migration\n            autogenerate: Whether to auto-generate the migration based on model changes\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The revision identifier of the created migration\n\n        Raises:\n            MigrationCreationError: If creation fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement create_migration\")\n\n    @classmethod\n    def upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n        \"\"\"\n        Upgrade to the specified revision.\n\n        Args:\n            revision: The target revision to upgrade to (default: \"head\")\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationUpgradeError: If upgrade fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement upgrade\")\n\n    @classmethod\n    def downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Downgrade to the specified revision.\n\n        Args:\n            revision: The target revision to downgrade to\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationDowngradeError: If downgrade fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement downgrade\")\n\n    @classmethod\n    def get_current_revision(cls, **kwargs: Any) -&gt; Optional[str]:\n        \"\"\"\n        Get the current migration revision.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The current revision identifier, or None if no migrations have been applied\n\n        Raises:\n            MigrationError: If getting the current revision fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement get_current_revision\")\n\n    @classmethod\n    def get_migration_history(cls, **kwargs: Any) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Get the migration history.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            A list of dictionaries containing migration information\n\n        Raises:\n            MigrationError: If getting the migration history fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement get_migration_history\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter.create_migration","title":"<code>create_migration(message, autogenerate=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new migration.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Description of the migration</p> required <code>autogenerate</code> <code>bool</code> <p>Whether to auto-generate the migration based on model changes</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The revision identifier of the created migration</p> <p>Raises:</p> Type Description <code>MigrationCreationError</code> <p>If creation fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\ndef create_migration(\n    cls, message: str, autogenerate: bool = True, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Create a new migration.\n\n    Args:\n        message: Description of the migration\n        autogenerate: Whether to auto-generate the migration based on model changes\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The revision identifier of the created migration\n\n    Raises:\n        MigrationCreationError: If creation fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement create_migration\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter.downgrade","title":"<code>downgrade(revision, **kwargs)</code>  <code>classmethod</code>","text":"<p>Downgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to downgrade to</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationDowngradeError</code> <p>If downgrade fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\ndef downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Downgrade to the specified revision.\n\n    Args:\n        revision: The target revision to downgrade to\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationDowngradeError: If downgrade fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement downgrade\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter.get_current_revision","title":"<code>get_current_revision(**kwargs)</code>  <code>classmethod</code>","text":"<p>Get the current migration revision.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The current revision identifier, or None if no migrations have been applied</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the current revision fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\ndef get_current_revision(cls, **kwargs: Any) -&gt; Optional[str]:\n    \"\"\"\n    Get the current migration revision.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The current revision identifier, or None if no migrations have been applied\n\n    Raises:\n        MigrationError: If getting the current revision fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement get_current_revision\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter.get_migration_history","title":"<code>get_migration_history(**kwargs)</code>  <code>classmethod</code>","text":"<p>Get the migration history.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of dictionaries containing migration information</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the migration history fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\ndef get_migration_history(cls, **kwargs: Any) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Get the migration history.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        A list of dictionaries containing migration information\n\n    Raises:\n        MigrationError: If getting the migration history fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement get_migration_history\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter.init_migrations","title":"<code>init_migrations(directory, **kwargs)</code>  <code>classmethod</code>","text":"<p>Initialize migration environment in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory where migrations will be stored</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationInitError</code> <p>If initialization fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\ndef init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize migration environment in the specified directory.\n\n    Args:\n        directory: Path to the directory where migrations will be stored\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationInitError: If initialization fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement init_migrations\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.SyncMigrationAdapter.upgrade","title":"<code>upgrade(revision='head', **kwargs)</code>  <code>classmethod</code>","text":"<p>Upgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to upgrade to (default: \"head\")</p> <code>'head'</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationUpgradeError</code> <p>If upgrade fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\ndef upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n    \"\"\"\n    Upgrade to the specified revision.\n\n    Args:\n        revision: The target revision to upgrade to (default: \"head\")\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationUpgradeError: If upgrade fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement upgrade\")\n</code></pre>"},{"location":"api/migrations/#asyncmigrationadapter","title":"AsyncMigrationAdapter","text":""},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter","title":"<code>pydapter.migrations.base.AsyncMigrationAdapter</code>","text":"<p>               Bases: <code>BaseMigrationAdapter</code>, <code>AsyncMigrationProtocol</code></p> <p>Base class for asynchronous migration adapters.</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>class AsyncMigrationAdapter(BaseMigrationAdapter, AsyncMigrationProtocol):\n    \"\"\"Base class for asynchronous migration adapters.\"\"\"\n\n    @classmethod\n    async def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize migration environment in the specified directory.\n\n        Args:\n            directory: Path to the directory where migrations will be stored\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationInitError: If initialization fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement init_migrations\")\n\n    @classmethod\n    async def create_migration(\n        cls, message: str, autogenerate: bool = True, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"\n        Create a new migration.\n\n        Args:\n            message: Description of the migration\n            autogenerate: Whether to auto-generate the migration based on model changes\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The revision identifier of the created migration\n\n        Raises:\n            MigrationCreationError: If creation fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement create_migration\")\n\n    @classmethod\n    async def upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n        \"\"\"\n        Upgrade to the specified revision.\n\n        Args:\n            revision: The target revision to upgrade to (default: \"head\")\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationUpgradeError: If upgrade fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement upgrade\")\n\n    @classmethod\n    async def downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Downgrade to the specified revision.\n\n        Args:\n            revision: The target revision to downgrade to\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationDowngradeError: If downgrade fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement downgrade\")\n\n    @classmethod\n    async def get_current_revision(cls, **kwargs: Any) -&gt; Optional[str]:\n        \"\"\"\n        Get the current migration revision.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The current revision identifier, or None if no migrations have been applied\n\n        Raises:\n            MigrationError: If getting the current revision fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement get_current_revision\")\n\n    @classmethod\n    async def get_migration_history(cls, **kwargs: Any) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Get the migration history.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            A list of dictionaries containing migration information\n\n        Raises:\n            MigrationError: If getting the migration history fails\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement get_migration_history\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter.create_migration","title":"<code>create_migration(message, autogenerate=True, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Create a new migration.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Description of the migration</p> required <code>autogenerate</code> <code>bool</code> <p>Whether to auto-generate the migration based on model changes</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The revision identifier of the created migration</p> <p>Raises:</p> Type Description <code>MigrationCreationError</code> <p>If creation fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\nasync def create_migration(\n    cls, message: str, autogenerate: bool = True, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Create a new migration.\n\n    Args:\n        message: Description of the migration\n        autogenerate: Whether to auto-generate the migration based on model changes\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The revision identifier of the created migration\n\n    Raises:\n        MigrationCreationError: If creation fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement create_migration\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter.downgrade","title":"<code>downgrade(revision, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Downgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to downgrade to</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationDowngradeError</code> <p>If downgrade fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\nasync def downgrade(cls, revision: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Downgrade to the specified revision.\n\n    Args:\n        revision: The target revision to downgrade to\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationDowngradeError: If downgrade fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement downgrade\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter.get_current_revision","title":"<code>get_current_revision(**kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Get the current migration revision.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The current revision identifier, or None if no migrations have been applied</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the current revision fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\nasync def get_current_revision(cls, **kwargs: Any) -&gt; Optional[str]:\n    \"\"\"\n    Get the current migration revision.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The current revision identifier, or None if no migrations have been applied\n\n    Raises:\n        MigrationError: If getting the current revision fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement get_current_revision\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter.get_migration_history","title":"<code>get_migration_history(**kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Get the migration history.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of dictionaries containing migration information</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the migration history fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\nasync def get_migration_history(cls, **kwargs: Any) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Get the migration history.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        A list of dictionaries containing migration information\n\n    Raises:\n        MigrationError: If getting the migration history fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement get_migration_history\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter.init_migrations","title":"<code>init_migrations(directory, **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Initialize migration environment in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory where migrations will be stored</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationInitError</code> <p>If initialization fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\nasync def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize migration environment in the specified directory.\n\n    Args:\n        directory: Path to the directory where migrations will be stored\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationInitError: If initialization fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement init_migrations\")\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.base.AsyncMigrationAdapter.upgrade","title":"<code>upgrade(revision='head', **kwargs)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Upgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>The target revision to upgrade to (default: \"head\")</p> <code>'head'</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationUpgradeError</code> <p>If upgrade fails</p> Source code in <code>src/pydapter/migrations/base.py</code> <pre><code>@classmethod\nasync def upgrade(cls, revision: str = \"head\", **kwargs: Any) -&gt; None:\n    \"\"\"\n    Upgrade to the specified revision.\n\n    Args:\n        revision: The target revision to upgrade to (default: \"head\")\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationUpgradeError: If upgrade fails\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement upgrade\")\n</code></pre>"},{"location":"api/migrations/#sql-adapters","title":"SQL Adapters","text":""},{"location":"api/migrations/#alembicadapter","title":"AlembicAdapter","text":""},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter","title":"<code>pydapter.migrations.sql.alembic_adapter.AlembicAdapter</code>","text":"<p>               Bases: <code>SyncMigrationAdapter</code></p> <p>Alembic implementation of the MigrationAdapter interface.</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>class AlembicAdapter(SyncMigrationAdapter):\n    \"\"\"Alembic implementation of the MigrationAdapter interface.\"\"\"\n\n    migration_key: ClassVar[str] = \"alembic\"\n\n    def __init__(self, connection_string: str, models_module: Any = None):\n        \"\"\"\n        Initialize the Alembic migration adapter.\n\n        Args:\n            connection_string: Database connection string\n            models_module: Optional module containing SQLAlchemy models\n        \"\"\"\n        super().__init__(connection_string, models_module)\n        self.engine = sa.create_engine(connection_string)\n        self.alembic_cfg = None\n\n    @classmethod\n    def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize migration environment in the specified directory.\n\n        Args:\n            directory: Path to the directory where migrations will be stored\n            **kwargs: Additional adapter-specific arguments\n                connection_string: Database connection string\n                models_module: Optional module containing SQLAlchemy models\n                template: Optional template to use for migration environment\n        \"\"\"\n        try:\n            # Create a new instance with the provided connection string\n            connection_string = kwargs.get(\"connection_string\")\n            if not connection_string:\n                raise MigrationInitError(\n                    \"Connection string is required for Alembic initialization\",\n                    directory=directory,\n                )\n\n            adapter = cls(connection_string, kwargs.get(\"models_module\"))\n\n            # Check if the directory exists and is not empty\n            force_clean = kwargs.get(\"force_clean\", False)\n            if os.path.exists(directory) and os.listdir(directory):\n                if force_clean:\n                    # If force_clean is specified, remove the directory and recreate it\n                    shutil.rmtree(directory)\n                    os.makedirs(directory)\n            else:\n                # Create the directory if it doesn't exist\n                adapter._ensure_directory(directory)\n\n            # Initialize Alembic directory structure\n            template = kwargs.get(\"template\", \"generic\")\n\n            # Create a temporary config file\n            ini_path = os.path.join(directory, \"alembic.ini\")\n            with open(ini_path, \"w\") as f:\n                f.write(\n                    f\"\"\"\n[alembic]\nscript_location = {directory}\nsqlalchemy.url = {connection_string}\n\n[loggers]\nkeys = root,sqlalchemy,alembic\n\n[handlers]\nkeys = console\n\n[formatters]\nkeys = generic\n\n[logger_root]\nlevel = WARN\nhandlers = console\nqualname =\n\n[logger_sqlalchemy]\nlevel = WARN\nhandlers =\nqualname = sqlalchemy.engine\n\n[logger_alembic]\nlevel = INFO\nhandlers =\nqualname = alembic\n\n[handler_console]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatter_generic]\nformat = %(levelname)-5.5s [%(name)s] %(message)s\ndatefmt = %H:%M:%S\n\"\"\"\n                )\n\n            # Initialize Alembic in the specified directory\n            adapter.alembic_cfg = config.Config(ini_path)\n            adapter.alembic_cfg.set_main_option(\"script_location\", directory)\n            adapter.alembic_cfg.set_main_option(\"sqlalchemy.url\", connection_string)\n\n            # Initialize Alembic directory structure\n            try:\n                command.init(adapter.alembic_cfg, directory, template=template)\n            except Exception as e:\n                if \"already exists and is not empty\" in str(e) and force_clean:\n                    # If the directory exists and is not empty, and force_clean is True,\n                    # try to clean it up again and retry\n                    shutil.rmtree(directory)\n                    os.makedirs(directory)\n                    command.init(adapter.alembic_cfg, directory, template=template)\n                else:\n                    raise\n\n            # Update env.py to use the models_module for autogeneration if provided\n            if adapter.models_module:\n                env_path = os.path.join(directory, \"env.py\")\n                adapter._update_env_py(env_path)\n\n            adapter._migrations_dir = directory\n            adapter._initialized = True\n\n            # Return the adapter instance\n            return adapter\n\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationInitError(\n                f\"Failed to initialize Alembic migrations: {str(exc)}\",\n                directory=directory,\n                original_error=str(exc),\n            ) from exc\n\n    def _update_env_py(self, env_path: str) -&gt; None:\n        \"\"\"\n        Update the env.py file to use the models_module for autogeneration.\n\n        Args:\n            env_path: Path to the env.py file\n        \"\"\"\n        if not os.path.exists(env_path):\n            return\n\n        # Read the env.py file\n        with open(env_path) as f:\n            env_content = f.read()\n\n        # Update the target_metadata\n        if \"target_metadata = None\" in env_content:\n            # Import the models module\n            import_statement = f\"from {self.models_module.__name__} import Base\\n\"\n            env_content = env_content.replace(\n                \"from alembic import context\",\n                \"from alembic import context\\n\" + import_statement,\n            )\n\n            # Update the target_metadata\n            env_content = env_content.replace(\n                \"target_metadata = None\", \"target_metadata = Base.metadata\"\n            )\n\n            # Write the updated env.py file\n            with open(env_path, \"w\") as f:\n                f.write(env_content)\n\n    def create_migration(\n        self, message: str, autogenerate: bool = True, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"\n        Create a new migration.\n\n        Args:\n            message: Description of the migration\n            autogenerate: Whether to auto-generate the migration based on model changes\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The revision identifier of the created migration\n\n        Raises:\n            MigrationCreationError: If creation fails\n        \"\"\"\n        try:\n            if not self._initialized:\n                raise MigrationCreationError(\n                    \"Migrations have not been initialized. Call init_migrations first.\"\n                )\n\n            # Create the migration\n            command.revision(\n                self.alembic_cfg,\n                message=message,\n                autogenerate=autogenerate,\n            )\n\n            # Get the revision ID from the latest revision\n            from alembic.script import ScriptDirectory\n\n            script = ScriptDirectory.from_config(self.alembic_cfg)\n            revision = script.get_current_head()\n\n            return revision\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationCreationError(\n                f\"Failed to create migration: {str(exc)}\",\n                autogenerate=autogenerate,\n                original_error=str(exc),\n            ) from exc\n\n    def upgrade(self, revision: str = \"head\", **kwargs: Any) -&gt; None:\n        \"\"\"\n        Upgrade to the specified revision.\n\n        Args:\n            revision: Revision to upgrade to (default: \"head\" for latest)\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationUpgradeError: If upgrade fails\n        \"\"\"\n        try:\n            if not self._initialized:\n                raise MigrationUpgradeError(\n                    \"Migrations have not been initialized. Call init_migrations first.\"\n                )\n\n            # Upgrade to the specified revision\n            command.upgrade(self.alembic_cfg, revision)\n\n            return None\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationUpgradeError(\n                f\"Failed to upgrade: {str(exc)}\",\n                revision=revision,\n                original_error=str(exc),\n            ) from exc\n\n    def downgrade(self, revision: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Downgrade to the specified revision.\n\n        Args:\n            revision: Revision to downgrade to\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationDowngradeError: If downgrade fails\n        \"\"\"\n        try:\n            if not self._initialized:\n                raise MigrationDowngradeError(\n                    \"Migrations have not been initialized. Call init_migrations first.\"\n                )\n\n            # Downgrade to the specified revision\n            command.downgrade(self.alembic_cfg, revision)\n\n            return None\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationDowngradeError(\n                f\"Failed to downgrade: {str(exc)}\",\n                revision=revision,\n                original_error=str(exc),\n            ) from exc\n\n    def get_current_revision(self, **kwargs: Any) -&gt; Optional[str]:\n        \"\"\"\n        Get the current revision.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The current revision, or None if no migrations have been applied\n\n        Raises:\n            MigrationError: If getting the current revision fails\n        \"\"\"\n        try:\n            if not self._initialized:\n                raise MigrationError(\n                    \"Migrations have not been initialized. Call init_migrations first.\"\n                )\n\n            # Get the current revision\n            from alembic.migration import MigrationContext\n\n            # Get the database connection\n            connection = self.engine.connect()\n\n            # Create a migration context\n            migration_context = MigrationContext.configure(connection)\n\n            # Get the current revision\n            current_revision = migration_context.get_current_revision()\n\n            # Close the connection\n            connection.close()\n\n            return current_revision\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationError(\n                f\"Failed to get current revision: {str(exc)}\",\n                original_error=exc,\n            ) from exc\n\n    def get_migration_history(self, **kwargs: Any) -&gt; list[dict]:\n        \"\"\"\n        Get the migration history.\n\n        Args:\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            A list of dictionaries containing migration information\n\n        Raises:\n            MigrationError: If getting the migration history fails\n        \"\"\"\n        try:\n            if not self._initialized:\n                raise MigrationError(\n                    \"Migrations have not been initialized. Call init_migrations first.\"\n                )\n\n            # Get the migration history\n            from alembic.script import ScriptDirectory\n\n            script = ScriptDirectory.from_config(self.alembic_cfg)\n\n            # Get all revisions\n            revisions = []\n            for revision in script.walk_revisions():\n                revisions.append(\n                    {\n                        \"revision\": revision.revision,\n                        \"message\": revision.doc,\n                        \"created\": None,  # Alembic doesn't store creation dates\n                    }\n                )\n\n            return revisions\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationError(\n                f\"Failed to get migration history: {str(exc)}\",\n                original_error=exc,\n            ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.__init__","title":"<code>__init__(connection_string, models_module=None)</code>","text":"<p>Initialize the Alembic migration adapter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_string</code> <code>str</code> <p>Database connection string</p> required <code>models_module</code> <code>Any</code> <p>Optional module containing SQLAlchemy models</p> <code>None</code> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def __init__(self, connection_string: str, models_module: Any = None):\n    \"\"\"\n    Initialize the Alembic migration adapter.\n\n    Args:\n        connection_string: Database connection string\n        models_module: Optional module containing SQLAlchemy models\n    \"\"\"\n    super().__init__(connection_string, models_module)\n    self.engine = sa.create_engine(connection_string)\n    self.alembic_cfg = None\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.create_migration","title":"<code>create_migration(message, autogenerate=True, **kwargs)</code>","text":"<p>Create a new migration.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Description of the migration</p> required <code>autogenerate</code> <code>bool</code> <p>Whether to auto-generate the migration based on model changes</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The revision identifier of the created migration</p> <p>Raises:</p> Type Description <code>MigrationCreationError</code> <p>If creation fails</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def create_migration(\n    self, message: str, autogenerate: bool = True, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Create a new migration.\n\n    Args:\n        message: Description of the migration\n        autogenerate: Whether to auto-generate the migration based on model changes\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The revision identifier of the created migration\n\n    Raises:\n        MigrationCreationError: If creation fails\n    \"\"\"\n    try:\n        if not self._initialized:\n            raise MigrationCreationError(\n                \"Migrations have not been initialized. Call init_migrations first.\"\n            )\n\n        # Create the migration\n        command.revision(\n            self.alembic_cfg,\n            message=message,\n            autogenerate=autogenerate,\n        )\n\n        # Get the revision ID from the latest revision\n        from alembic.script import ScriptDirectory\n\n        script = ScriptDirectory.from_config(self.alembic_cfg)\n        revision = script.get_current_head()\n\n        return revision\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationCreationError(\n            f\"Failed to create migration: {str(exc)}\",\n            autogenerate=autogenerate,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.downgrade","title":"<code>downgrade(revision, **kwargs)</code>","text":"<p>Downgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>Revision to downgrade to</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationDowngradeError</code> <p>If downgrade fails</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def downgrade(self, revision: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Downgrade to the specified revision.\n\n    Args:\n        revision: Revision to downgrade to\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationDowngradeError: If downgrade fails\n    \"\"\"\n    try:\n        if not self._initialized:\n            raise MigrationDowngradeError(\n                \"Migrations have not been initialized. Call init_migrations first.\"\n            )\n\n        # Downgrade to the specified revision\n        command.downgrade(self.alembic_cfg, revision)\n\n        return None\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationDowngradeError(\n            f\"Failed to downgrade: {str(exc)}\",\n            revision=revision,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.get_current_revision","title":"<code>get_current_revision(**kwargs)</code>","text":"<p>Get the current revision.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The current revision, or None if no migrations have been applied</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the current revision fails</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def get_current_revision(self, **kwargs: Any) -&gt; Optional[str]:\n    \"\"\"\n    Get the current revision.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The current revision, or None if no migrations have been applied\n\n    Raises:\n        MigrationError: If getting the current revision fails\n    \"\"\"\n    try:\n        if not self._initialized:\n            raise MigrationError(\n                \"Migrations have not been initialized. Call init_migrations first.\"\n            )\n\n        # Get the current revision\n        from alembic.migration import MigrationContext\n\n        # Get the database connection\n        connection = self.engine.connect()\n\n        # Create a migration context\n        migration_context = MigrationContext.configure(connection)\n\n        # Get the current revision\n        current_revision = migration_context.get_current_revision()\n\n        # Close the connection\n        connection.close()\n\n        return current_revision\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationError(\n            f\"Failed to get current revision: {str(exc)}\",\n            original_error=exc,\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.get_migration_history","title":"<code>get_migration_history(**kwargs)</code>","text":"<p>Get the migration history.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries containing migration information</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the migration history fails</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def get_migration_history(self, **kwargs: Any) -&gt; list[dict]:\n    \"\"\"\n    Get the migration history.\n\n    Args:\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        A list of dictionaries containing migration information\n\n    Raises:\n        MigrationError: If getting the migration history fails\n    \"\"\"\n    try:\n        if not self._initialized:\n            raise MigrationError(\n                \"Migrations have not been initialized. Call init_migrations first.\"\n            )\n\n        # Get the migration history\n        from alembic.script import ScriptDirectory\n\n        script = ScriptDirectory.from_config(self.alembic_cfg)\n\n        # Get all revisions\n        revisions = []\n        for revision in script.walk_revisions():\n            revisions.append(\n                {\n                    \"revision\": revision.revision,\n                    \"message\": revision.doc,\n                    \"created\": None,  # Alembic doesn't store creation dates\n                }\n            )\n\n        return revisions\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationError(\n            f\"Failed to get migration history: {str(exc)}\",\n            original_error=exc,\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.init_migrations","title":"<code>init_migrations(directory, **kwargs)</code>  <code>classmethod</code>","text":"<p>Initialize migration environment in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory where migrations will be stored</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments connection_string: Database connection string models_module: Optional module containing SQLAlchemy models template: Optional template to use for migration environment</p> <code>{}</code> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>    @classmethod\n    def init_migrations(cls, directory: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize migration environment in the specified directory.\n\n        Args:\n            directory: Path to the directory where migrations will be stored\n            **kwargs: Additional adapter-specific arguments\n                connection_string: Database connection string\n                models_module: Optional module containing SQLAlchemy models\n                template: Optional template to use for migration environment\n        \"\"\"\n        try:\n            # Create a new instance with the provided connection string\n            connection_string = kwargs.get(\"connection_string\")\n            if not connection_string:\n                raise MigrationInitError(\n                    \"Connection string is required for Alembic initialization\",\n                    directory=directory,\n                )\n\n            adapter = cls(connection_string, kwargs.get(\"models_module\"))\n\n            # Check if the directory exists and is not empty\n            force_clean = kwargs.get(\"force_clean\", False)\n            if os.path.exists(directory) and os.listdir(directory):\n                if force_clean:\n                    # If force_clean is specified, remove the directory and recreate it\n                    shutil.rmtree(directory)\n                    os.makedirs(directory)\n            else:\n                # Create the directory if it doesn't exist\n                adapter._ensure_directory(directory)\n\n            # Initialize Alembic directory structure\n            template = kwargs.get(\"template\", \"generic\")\n\n            # Create a temporary config file\n            ini_path = os.path.join(directory, \"alembic.ini\")\n            with open(ini_path, \"w\") as f:\n                f.write(\n                    f\"\"\"\n[alembic]\nscript_location = {directory}\nsqlalchemy.url = {connection_string}\n\n[loggers]\nkeys = root,sqlalchemy,alembic\n\n[handlers]\nkeys = console\n\n[formatters]\nkeys = generic\n\n[logger_root]\nlevel = WARN\nhandlers = console\nqualname =\n\n[logger_sqlalchemy]\nlevel = WARN\nhandlers =\nqualname = sqlalchemy.engine\n\n[logger_alembic]\nlevel = INFO\nhandlers =\nqualname = alembic\n\n[handler_console]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatter_generic]\nformat = %(levelname)-5.5s [%(name)s] %(message)s\ndatefmt = %H:%M:%S\n\"\"\"\n                )\n\n            # Initialize Alembic in the specified directory\n            adapter.alembic_cfg = config.Config(ini_path)\n            adapter.alembic_cfg.set_main_option(\"script_location\", directory)\n            adapter.alembic_cfg.set_main_option(\"sqlalchemy.url\", connection_string)\n\n            # Initialize Alembic directory structure\n            try:\n                command.init(adapter.alembic_cfg, directory, template=template)\n            except Exception as e:\n                if \"already exists and is not empty\" in str(e) and force_clean:\n                    # If the directory exists and is not empty, and force_clean is True,\n                    # try to clean it up again and retry\n                    shutil.rmtree(directory)\n                    os.makedirs(directory)\n                    command.init(adapter.alembic_cfg, directory, template=template)\n                else:\n                    raise\n\n            # Update env.py to use the models_module for autogeneration if provided\n            if adapter.models_module:\n                env_path = os.path.join(directory, \"env.py\")\n                adapter._update_env_py(env_path)\n\n            adapter._migrations_dir = directory\n            adapter._initialized = True\n\n            # Return the adapter instance\n            return adapter\n\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationInitError(\n                f\"Failed to initialize Alembic migrations: {str(exc)}\",\n                directory=directory,\n                original_error=str(exc),\n            ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AlembicAdapter.upgrade","title":"<code>upgrade(revision='head', **kwargs)</code>","text":"<p>Upgrade to the specified revision.</p> <p>Parameters:</p> Name Type Description Default <code>revision</code> <code>str</code> <p>Revision to upgrade to (default: \"head\" for latest)</p> <code>'head'</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationUpgradeError</code> <p>If upgrade fails</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def upgrade(self, revision: str = \"head\", **kwargs: Any) -&gt; None:\n    \"\"\"\n    Upgrade to the specified revision.\n\n    Args:\n        revision: Revision to upgrade to (default: \"head\" for latest)\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationUpgradeError: If upgrade fails\n    \"\"\"\n    try:\n        if not self._initialized:\n            raise MigrationUpgradeError(\n                \"Migrations have not been initialized. Call init_migrations first.\"\n            )\n\n        # Upgrade to the specified revision\n        command.upgrade(self.alembic_cfg, revision)\n\n        return None\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationUpgradeError(\n            f\"Failed to upgrade: {str(exc)}\",\n            revision=revision,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#asyncalembicadapter","title":"AsyncAlembicAdapter","text":""},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AsyncAlembicAdapter","title":"<code>pydapter.migrations.sql.alembic_adapter.AsyncAlembicAdapter</code>","text":"<p>               Bases: <code>AsyncMigrationAdapter</code></p> <p>Async implementation of the Alembic migration adapter.</p> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>class AsyncAlembicAdapter(AsyncMigrationAdapter):\n    \"\"\"Async implementation of the Alembic migration adapter.\"\"\"\n\n    migration_key: ClassVar[str] = \"async_alembic\"\n\n    def __init__(self, connection_string: str, models_module: Any = None):\n        \"\"\"\n        Initialize the async Alembic migration adapter.\n\n        Args:\n            connection_string: Database connection string\n            models_module: Optional module containing SQLAlchemy models\n        \"\"\"\n        super().__init__(connection_string, models_module)\n        self.engine = create_async_engine(connection_string)\n        self.alembic_cfg = None\n\n    async def _update_env_py_async(self, env_path: str) -&gt; None:\n        \"\"\"\n        Update the env.py file to use the models_module for autogeneration.\n\n        Args:\n            env_path: Path to the env.py file\n        \"\"\"\n        if not os.path.exists(env_path):\n            return\n\n        # Read the env.py file\n        with open(env_path) as f:\n            env_content = f.read()\n\n        # Update the target_metadata\n        if \"target_metadata = None\" in env_content:\n            # Import the models module\n            import_statement = f\"from {self.models_module.__name__} import Base\\n\"\n            env_content = env_content.replace(\n                \"from alembic import context\",\n                \"from alembic import context\\n\" + import_statement,\n            )\n\n            # Update the target_metadata\n            env_content = env_content.replace(\n                \"target_metadata = None\", \"target_metadata = Base.metadata\"\n            )\n\n            # Write the updated env.py file\n            with open(env_path, \"w\") as f:\n                f.write(env_content)\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AsyncAlembicAdapter-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.sql.alembic_adapter.AsyncAlembicAdapter.__init__","title":"<code>__init__(connection_string, models_module=None)</code>","text":"<p>Initialize the async Alembic migration adapter.</p> <p>Parameters:</p> Name Type Description Default <code>connection_string</code> <code>str</code> <p>Database connection string</p> required <code>models_module</code> <code>Any</code> <p>Optional module containing SQLAlchemy models</p> <code>None</code> Source code in <code>src/pydapter/migrations/sql/alembic_adapter.py</code> <pre><code>def __init__(self, connection_string: str, models_module: Any = None):\n    \"\"\"\n    Initialize the async Alembic migration adapter.\n\n    Args:\n        connection_string: Database connection string\n        models_module: Optional module containing SQLAlchemy models\n    \"\"\"\n    super().__init__(connection_string, models_module)\n    self.engine = create_async_engine(connection_string)\n    self.alembic_cfg = None\n</code></pre>"},{"location":"api/migrations/#registry","title":"Registry","text":""},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry","title":"<code>pydapter.migrations.registry.MigrationRegistry</code>","text":"<p>Registry for migration adapters.</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>class MigrationRegistry:\n    \"\"\"Registry for migration adapters.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._reg: dict[str, type[MigrationProtocol]] = {}\n\n    def register(self, adapter_cls: type[MigrationProtocol]) -&gt; None:\n        \"\"\"\n        Register a migration adapter.\n\n        Args:\n            adapter_cls: The adapter class to register\n\n        Raises:\n            ConfigurationError: If the adapter does not define a migration_key\n        \"\"\"\n        key = getattr(adapter_cls, \"migration_key\", None)\n        if not key:\n            raise ConfigurationError(\n                \"Migration adapter must define 'migration_key'\",\n                adapter_cls=adapter_cls.__name__,\n            )\n        self._reg[key] = adapter_cls\n\n    def get(self, migration_key: str) -&gt; type[MigrationProtocol]:\n        \"\"\"\n        Get a migration adapter by key.\n\n        Args:\n            migration_key: The key of the adapter to retrieve\n\n        Returns:\n            The adapter class\n\n        Raises:\n            AdapterNotFoundError: If no adapter is registered for the given key\n        \"\"\"\n        try:\n            return self._reg[migration_key]\n        except KeyError as exc:\n            raise AdapterNotFoundError(\n                f\"No migration adapter registered for '{migration_key}'\",\n                obj_key=migration_key,\n            ) from exc\n\n    # Convenience methods for migration operations\n\n    def init_migrations(\n        self, migration_key: str, directory: str, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        Initialize migrations for the specified adapter.\n\n        Args:\n            migration_key: The key of the adapter to use\n            directory: The directory to initialize migrations in\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationInitError: If initialization fails\n        \"\"\"\n        try:\n            adapter_cls = self.get(migration_key)\n            adapter_cls.init_migrations(directory, **kwargs)\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationInitError(\n                f\"Failed to initialize migrations for '{migration_key}'\",\n                directory=directory,\n                adapter=migration_key,\n                original_error=str(exc),\n            ) from exc\n\n    def create_migration(\n        self, migration_key: str, message: str, autogenerate: bool = True, **kwargs: Any\n    ) -&gt; str:\n        \"\"\"\n        Create a migration for the specified adapter.\n\n        Args:\n            migration_key: The key of the adapter to use\n            message: The migration message\n            autogenerate: Whether to auto-generate the migration\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The revision identifier of the created migration\n\n        Raises:\n            MigrationCreationError: If creation fails\n        \"\"\"\n        try:\n            adapter_cls = self.get(migration_key)\n            return adapter_cls.create_migration(message, autogenerate, **kwargs)\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationCreationError(\n                f\"Failed to create migration for '{migration_key}'\",\n                message_text=message,\n                autogenerate=autogenerate,\n                adapter=migration_key,\n                original_error=str(exc),\n            ) from exc\n\n    def upgrade(\n        self, migration_key: str, revision: str = \"head\", **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        Upgrade migrations for the specified adapter.\n\n        Args:\n            migration_key: The key of the adapter to use\n            revision: The target revision to upgrade to\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationUpgradeError: If upgrade fails\n        \"\"\"\n        try:\n            adapter_cls = self.get(migration_key)\n            adapter_cls.upgrade(revision, **kwargs)\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationUpgradeError(\n                f\"Failed to upgrade migrations for '{migration_key}'\",\n                revision=revision,\n                adapter=migration_key,\n                original_error=str(exc),\n            ) from exc\n\n    def downgrade(self, migration_key: str, revision: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Downgrade migrations for the specified adapter.\n\n        Args:\n            migration_key: The key of the adapter to use\n            revision: The target revision to downgrade to\n            **kwargs: Additional adapter-specific arguments\n\n        Raises:\n            MigrationDowngradeError: If downgrade fails\n        \"\"\"\n        try:\n            adapter_cls = self.get(migration_key)\n            adapter_cls.downgrade(revision, **kwargs)\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationDowngradeError(\n                f\"Failed to downgrade migrations for '{migration_key}'\",\n                revision=revision,\n                adapter=migration_key,\n                original_error=str(exc),\n            ) from exc\n\n    def get_current_revision(self, migration_key: str, **kwargs: Any) -&gt; Optional[str]:\n        \"\"\"\n        Get the current revision for the specified adapter.\n\n        Args:\n            migration_key: The key of the adapter to use\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            The current revision identifier, or None if no migrations have been applied\n\n        Raises:\n            MigrationError: If getting the current revision fails\n        \"\"\"\n        try:\n            adapter_cls = self.get(migration_key)\n            return adapter_cls.get_current_revision(**kwargs)\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationError(\n                f\"Failed to get current revision for '{migration_key}'\",\n                adapter=migration_key,\n                original_error=exc,\n            ) from exc\n\n    def get_migration_history(self, migration_key: str, **kwargs: Any) -&gt; list[dict]:\n        \"\"\"\n        Get the migration history for the specified adapter.\n\n        Args:\n            migration_key: The key of the adapter to use\n            **kwargs: Additional adapter-specific arguments\n\n        Returns:\n            A list of dictionaries containing migration information\n\n        Raises:\n            MigrationError: If getting the migration history fails\n        \"\"\"\n        try:\n            adapter_cls = self.get(migration_key)\n            return adapter_cls.get_migration_history(**kwargs)\n        except Exception as exc:\n            if isinstance(exc, MigrationError):\n                raise\n            raise MigrationError(\n                f\"Failed to get migration history for '{migration_key}'\",\n                adapter=migration_key,\n                original_error=exc,\n            ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.create_migration","title":"<code>create_migration(migration_key, message, autogenerate=True, **kwargs)</code>","text":"<p>Create a migration for the specified adapter.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to use</p> required <code>message</code> <code>str</code> <p>The migration message</p> required <code>autogenerate</code> <code>bool</code> <p>Whether to auto-generate the migration</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The revision identifier of the created migration</p> <p>Raises:</p> Type Description <code>MigrationCreationError</code> <p>If creation fails</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def create_migration(\n    self, migration_key: str, message: str, autogenerate: bool = True, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Create a migration for the specified adapter.\n\n    Args:\n        migration_key: The key of the adapter to use\n        message: The migration message\n        autogenerate: Whether to auto-generate the migration\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The revision identifier of the created migration\n\n    Raises:\n        MigrationCreationError: If creation fails\n    \"\"\"\n    try:\n        adapter_cls = self.get(migration_key)\n        return adapter_cls.create_migration(message, autogenerate, **kwargs)\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationCreationError(\n            f\"Failed to create migration for '{migration_key}'\",\n            message_text=message,\n            autogenerate=autogenerate,\n            adapter=migration_key,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.downgrade","title":"<code>downgrade(migration_key, revision, **kwargs)</code>","text":"<p>Downgrade migrations for the specified adapter.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to use</p> required <code>revision</code> <code>str</code> <p>The target revision to downgrade to</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationDowngradeError</code> <p>If downgrade fails</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def downgrade(self, migration_key: str, revision: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Downgrade migrations for the specified adapter.\n\n    Args:\n        migration_key: The key of the adapter to use\n        revision: The target revision to downgrade to\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationDowngradeError: If downgrade fails\n    \"\"\"\n    try:\n        adapter_cls = self.get(migration_key)\n        adapter_cls.downgrade(revision, **kwargs)\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationDowngradeError(\n            f\"Failed to downgrade migrations for '{migration_key}'\",\n            revision=revision,\n            adapter=migration_key,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.get","title":"<code>get(migration_key)</code>","text":"<p>Get a migration adapter by key.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to retrieve</p> required <p>Returns:</p> Type Description <code>type[MigrationProtocol]</code> <p>The adapter class</p> <p>Raises:</p> Type Description <code>AdapterNotFoundError</code> <p>If no adapter is registered for the given key</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def get(self, migration_key: str) -&gt; type[MigrationProtocol]:\n    \"\"\"\n    Get a migration adapter by key.\n\n    Args:\n        migration_key: The key of the adapter to retrieve\n\n    Returns:\n        The adapter class\n\n    Raises:\n        AdapterNotFoundError: If no adapter is registered for the given key\n    \"\"\"\n    try:\n        return self._reg[migration_key]\n    except KeyError as exc:\n        raise AdapterNotFoundError(\n            f\"No migration adapter registered for '{migration_key}'\",\n            obj_key=migration_key,\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.get_current_revision","title":"<code>get_current_revision(migration_key, **kwargs)</code>","text":"<p>Get the current revision for the specified adapter.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to use</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The current revision identifier, or None if no migrations have been applied</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the current revision fails</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def get_current_revision(self, migration_key: str, **kwargs: Any) -&gt; Optional[str]:\n    \"\"\"\n    Get the current revision for the specified adapter.\n\n    Args:\n        migration_key: The key of the adapter to use\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        The current revision identifier, or None if no migrations have been applied\n\n    Raises:\n        MigrationError: If getting the current revision fails\n    \"\"\"\n    try:\n        adapter_cls = self.get(migration_key)\n        return adapter_cls.get_current_revision(**kwargs)\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationError(\n            f\"Failed to get current revision for '{migration_key}'\",\n            adapter=migration_key,\n            original_error=exc,\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.get_migration_history","title":"<code>get_migration_history(migration_key, **kwargs)</code>","text":"<p>Get the migration history for the specified adapter.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to use</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries containing migration information</p> <p>Raises:</p> Type Description <code>MigrationError</code> <p>If getting the migration history fails</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def get_migration_history(self, migration_key: str, **kwargs: Any) -&gt; list[dict]:\n    \"\"\"\n    Get the migration history for the specified adapter.\n\n    Args:\n        migration_key: The key of the adapter to use\n        **kwargs: Additional adapter-specific arguments\n\n    Returns:\n        A list of dictionaries containing migration information\n\n    Raises:\n        MigrationError: If getting the migration history fails\n    \"\"\"\n    try:\n        adapter_cls = self.get(migration_key)\n        return adapter_cls.get_migration_history(**kwargs)\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationError(\n            f\"Failed to get migration history for '{migration_key}'\",\n            adapter=migration_key,\n            original_error=exc,\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.init_migrations","title":"<code>init_migrations(migration_key, directory, **kwargs)</code>","text":"<p>Initialize migrations for the specified adapter.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to use</p> required <code>directory</code> <code>str</code> <p>The directory to initialize migrations in</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationInitError</code> <p>If initialization fails</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def init_migrations(\n    self, migration_key: str, directory: str, **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Initialize migrations for the specified adapter.\n\n    Args:\n        migration_key: The key of the adapter to use\n        directory: The directory to initialize migrations in\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationInitError: If initialization fails\n    \"\"\"\n    try:\n        adapter_cls = self.get(migration_key)\n        adapter_cls.init_migrations(directory, **kwargs)\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationInitError(\n            f\"Failed to initialize migrations for '{migration_key}'\",\n            directory=directory,\n            adapter=migration_key,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.register","title":"<code>register(adapter_cls)</code>","text":"<p>Register a migration adapter.</p> <p>Parameters:</p> Name Type Description Default <code>adapter_cls</code> <code>type[MigrationProtocol]</code> <p>The adapter class to register</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If the adapter does not define a migration_key</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def register(self, adapter_cls: type[MigrationProtocol]) -&gt; None:\n    \"\"\"\n    Register a migration adapter.\n\n    Args:\n        adapter_cls: The adapter class to register\n\n    Raises:\n        ConfigurationError: If the adapter does not define a migration_key\n    \"\"\"\n    key = getattr(adapter_cls, \"migration_key\", None)\n    if not key:\n        raise ConfigurationError(\n            \"Migration adapter must define 'migration_key'\",\n            adapter_cls=adapter_cls.__name__,\n        )\n    self._reg[key] = adapter_cls\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.registry.MigrationRegistry.upgrade","title":"<code>upgrade(migration_key, revision='head', **kwargs)</code>","text":"<p>Upgrade migrations for the specified adapter.</p> <p>Parameters:</p> Name Type Description Default <code>migration_key</code> <code>str</code> <p>The key of the adapter to use</p> required <code>revision</code> <code>str</code> <p>The target revision to upgrade to</p> <code>'head'</code> <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>MigrationUpgradeError</code> <p>If upgrade fails</p> Source code in <code>src/pydapter/migrations/registry.py</code> <pre><code>def upgrade(\n    self, migration_key: str, revision: str = \"head\", **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Upgrade migrations for the specified adapter.\n\n    Args:\n        migration_key: The key of the adapter to use\n        revision: The target revision to upgrade to\n        **kwargs: Additional adapter-specific arguments\n\n    Raises:\n        MigrationUpgradeError: If upgrade fails\n    \"\"\"\n    try:\n        adapter_cls = self.get(migration_key)\n        adapter_cls.upgrade(revision, **kwargs)\n    except Exception as exc:\n        if isinstance(exc, MigrationError):\n            raise\n        raise MigrationUpgradeError(\n            f\"Failed to upgrade migrations for '{migration_key}'\",\n            revision=revision,\n            adapter=migration_key,\n            original_error=str(exc),\n        ) from exc\n</code></pre>"},{"location":"api/migrations/#exceptions","title":"Exceptions","text":""},{"location":"api/migrations/#migrationerror","title":"MigrationError","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationError","title":"<code>pydapter.migrations.exceptions.MigrationError</code>","text":"<p>               Bases: <code>AdapterError</code></p> <p>Base exception for all migration-related errors.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>class MigrationError(AdapterError):\n    \"\"\"Base exception for all migration-related errors.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        original_error: Optional[Exception] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, **context)\n        self.original_error = original_error\n        self.adapter = adapter\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the error.\"\"\"\n        result = super().__str__()\n        if hasattr(self, \"original_error\") and self.original_error is not None:\n            result += f\" (original_error='{self.original_error}')\"\n        return result\n</code></pre>"},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationError-functions","title":"Functions","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationError.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the error.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a string representation of the error.\"\"\"\n    result = super().__str__()\n    if hasattr(self, \"original_error\") and self.original_error is not None:\n        result += f\" (original_error='{self.original_error}')\"\n    return result\n</code></pre>"},{"location":"api/migrations/#migrationiniterror","title":"MigrationInitError","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationInitError","title":"<code>pydapter.migrations.exceptions.MigrationInitError</code>","text":"<p>               Bases: <code>MigrationError</code></p> <p>Exception raised when migration initialization fails.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>class MigrationInitError(MigrationError):\n    \"\"\"Exception raised when migration initialization fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        directory: Optional[str] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, directory=directory, adapter=adapter, **context)\n        self.directory = directory\n        self.adapter = adapter\n        # Ensure original_error is set even if not passed through super().__init__\n        if \"original_error\" in context:\n            self.original_error = context[\"original_error\"]\n</code></pre>"},{"location":"api/migrations/#migrationcreationerror","title":"MigrationCreationError","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationCreationError","title":"<code>pydapter.migrations.exceptions.MigrationCreationError</code>","text":"<p>               Bases: <code>MigrationError</code></p> <p>Exception raised when migration creation fails.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>class MigrationCreationError(MigrationError):\n    \"\"\"Exception raised when migration creation fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        message_text: Optional[str] = None,\n        autogenerate: Optional[bool] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(\n            message,\n            message_text=message_text,\n            autogenerate=autogenerate,\n            adapter=adapter,\n            **context,\n        )\n        self.message_text = message_text\n        self.autogenerate = autogenerate\n        self.adapter = adapter\n        # Ensure original_error is set even if not passed through super().__init__\n        if \"original_error\" in context:\n            self.original_error = context[\"original_error\"]\n</code></pre>"},{"location":"api/migrations/#migrationupgradeerror","title":"MigrationUpgradeError","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationUpgradeError","title":"<code>pydapter.migrations.exceptions.MigrationUpgradeError</code>","text":"<p>               Bases: <code>MigrationError</code></p> <p>Exception raised when migration upgrade fails.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>class MigrationUpgradeError(MigrationError):\n    \"\"\"Exception raised when migration upgrade fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        revision: Optional[str] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, revision=revision, adapter=adapter, **context)\n        self.revision = revision\n        self.adapter = adapter\n        # Ensure original_error is set even if not passed through super().__init__\n        if \"original_error\" in context:\n            self.original_error = context[\"original_error\"]\n</code></pre>"},{"location":"api/migrations/#migrationdowngradeerror","title":"MigrationDowngradeError","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationDowngradeError","title":"<code>pydapter.migrations.exceptions.MigrationDowngradeError</code>","text":"<p>               Bases: <code>MigrationError</code></p> <p>Exception raised when migration downgrade fails.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>class MigrationDowngradeError(MigrationError):\n    \"\"\"Exception raised when migration downgrade fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        revision: Optional[str] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, revision=revision, adapter=adapter, **context)\n        self.revision = revision\n        self.adapter = adapter\n        # Ensure original_error is set even if not passed through super().__init__\n        if \"original_error\" in context:\n            self.original_error = context[\"original_error\"]\n</code></pre>"},{"location":"api/migrations/#migrationnotfounderror","title":"MigrationNotFoundError","text":""},{"location":"api/migrations/#pydapter.migrations.exceptions.MigrationNotFoundError","title":"<code>pydapter.migrations.exceptions.MigrationNotFoundError</code>","text":"<p>               Bases: <code>MigrationError</code></p> <p>Exception raised when a migration is not found.</p> Source code in <code>src/pydapter/migrations/exceptions.py</code> <pre><code>class MigrationNotFoundError(MigrationError):\n    \"\"\"Exception raised when a migration is not found.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        revision: Optional[str] = None,\n        adapter: Optional[str] = None,\n        **context: Any,\n    ):\n        super().__init__(message, revision=revision, adapter=adapter, **context)\n        self.revision = revision\n        self.adapter = adapter\n        # Ensure original_error is set even if not passed through super().__init__\n        if \"original_error\" in context:\n            self.original_error = context[\"original_error\"]\n</code></pre>"},{"location":"api/protocols/","title":"Protocols API Reference","text":"<p>The <code>pydapter.protocols</code> module provides independent, composable interfaces for models with specialized functionality.</p>"},{"location":"api/protocols/#installation","title":"Installation","text":"<pre><code>pip install pydapter\n</code></pre>"},{"location":"api/protocols/#overview","title":"Overview","text":"<p>Protocols in pydapter are independent, composable interfaces that can be mixed and matched:</p> <pre><code>Independent Protocols:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Identifiable  \u2502  \u2502    Temporal     \u2502  \u2502   Embeddable    \u2502\n\u2502   (id: UUID)    \u2502  \u2502 (timestamps)    \u2502  \u2502 (content +      \u2502\n\u2502                 \u2502  \u2502                 \u2502  \u2502  embedding)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Invokable    \u2502  \u2502 Cryptographical \u2502\n\u2502 (execution      \u2502  \u2502 (hashing)       \u2502\n\u2502  tracking)      \u2502  \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nEvent = Identifiable + Temporal + Embeddable + Invokable\n</code></pre> <p>Each protocol defines specific fields and can be used independently or combined through multiple inheritance.</p>"},{"location":"api/protocols/#core-protocols","title":"Core Protocols","text":""},{"location":"api/protocols/#identifiable","title":"Identifiable","text":"<p>Module: <code>pydapter.protocols.identifiable</code></p> <p>Provides unique identification using UUID.</p> <p>Protocol Interface:</p> <pre><code>@runtime_checkable\nclass Identifiable(Protocol):\n    id: UUID\n</code></pre> <p>Mixin Class: <code>IdentifiableMixin</code></p> <ul> <li>Provides UUID serialization to string</li> <li>Implements <code>__hash__</code> based on ID</li> </ul> <p>Usage:</p> <pre><code>from pydapter.protocols.identifiable import IdentifiableMixin\nfrom pydantic import BaseModel\n\nclass User(BaseModel, IdentifiableMixin):\n    name: str\n    email: str\n\nuser = User(name=\"John Doe\", email=\"john@example.com\")\nprint(user.id)  # UUID field must be provided or use field defaults\n</code></pre>"},{"location":"api/protocols/#temporal","title":"Temporal","text":"<p>Module: <code>pydapter.protocols.temporal</code></p> <p>Adds timestamp tracking capabilities.</p> <p>Protocol Interface:</p> <pre><code>@runtime_checkable\nclass Temporal(Protocol):\n    created_at: datetime\n    updated_at: datetime\n</code></pre> <p>Mixin Class: <code>TemporalMixin</code></p> <ul> <li><code>update_timestamp()</code>: Updates <code>updated_at</code> to current UTC time</li> <li>Provides datetime serialization to ISO format</li> </ul> <p>Usage:</p> <pre><code>from pydapter.protocols.temporal import TemporalMixin\nfrom pydantic import BaseModel\n\nclass Article(BaseModel, TemporalMixin):\n    title: str\n    content: str\n\narticle = Article(title=\"Hello World\", content=\"...\")\narticle.update_timestamp()  # Updates updated_at field\n</code></pre>"},{"location":"api/protocols/#embeddable","title":"Embeddable","text":"<p>Module: <code>pydapter.protocols.embeddable</code></p> <p>Provides content and vector embedding support.</p> <p>Protocol Interface:</p> <pre><code>@runtime_checkable\nclass Embeddable(Protocol):\n    content: str | None\n    embedding: Embedding  # list[float]\n</code></pre> <p>Mixin Class: <code>EmbeddableMixin</code></p> <ul> <li><code>n_dim</code> property: Returns embedding dimensions</li> <li><code>parse_embedding_response()</code>: Parses various embedding API response formats</li> </ul> <p>Usage:</p> <pre><code>from pydapter.protocols.embeddable import EmbeddableMixin\nfrom pydantic import BaseModel\n\nclass Document(BaseModel, EmbeddableMixin):\n    title: str\n\ndoc = Document(title=\"AI Research\")\ndoc.content = \"Machine learning research paper\"\ndoc.embedding = [0.1, 0.2, 0.3, ...]\nprint(doc.n_dim)  # Returns embedding length\n</code></pre>"},{"location":"api/protocols/#invokable","title":"Invokable","text":"<p>Module: <code>pydapter.protocols.invokable</code></p> <p>Enables asynchronous execution with state tracking.</p> <p>Protocol Interface:</p> <pre><code>@runtime_checkable\nclass Invokable(Protocol):\n    request: dict | None\n    execution: Execution\n    _handler: Callable | None\n    _handler_args: tuple[Any, ...]\n    _handler_kwargs: dict[str, Any]\n</code></pre> <p>Mixin Class: <code>InvokableMixin</code></p> <ul> <li><code>invoke()</code>: Executes the handler and tracks execution state</li> <li><code>has_invoked</code> property: Returns True if execution completed or failed</li> <li>Private attributes for handler management</li> </ul> <p>Usage:</p> <pre><code>from pydapter.protocols.invokable import InvokableMixin\nfrom pydapter.fields.execution import Execution\nfrom pydantic import BaseModel, PrivateAttr\n\nclass Task(BaseModel, InvokableMixin):\n    name: str\n    execution: Execution\n    _handler: callable = PrivateAttr()\n    _handler_args: tuple = PrivateAttr(default=())\n    _handler_kwargs: dict = PrivateAttr(default_factory=dict)\n\nasync def process_data():\n    return {\"result\": \"success\"}\n\ntask = Task(name=\"Process\", execution=Execution())\ntask._handler = process_data\nawait task.invoke()\nprint(task.execution.status)  # ExecutionStatus.COMPLETED\n</code></pre>"},{"location":"api/protocols/#cryptographical","title":"Cryptographical","text":"<p>Module: <code>pydapter.protocols.cryptographical</code></p> <p>Provides content hashing capabilities.</p> <p>Protocol Interface:</p> <pre><code>@runtime_checkable\nclass Cryptographical(Protocol):\n    content: JsonValue\n    sha256: str | None = None\n</code></pre> <p>Mixin Class: <code>CryptographicalMixin</code></p> <ul> <li><code>hash_content()</code>: Generates SHA-256 hash of content</li> </ul> <p>Usage:</p> <pre><code>from pydapter.protocols.cryptographical import CryptographicalMixin\nfrom pydantic import BaseModel\n\nclass SecureData(BaseModel, CryptographicalMixin):\n    title: str\n    content: str\n\ndata = SecureData(title=\"Secret\", content=\"classified information\")\ndata.hash_content()\nprint(data.sha256)  # SHA-256 hash of content\n</code></pre>"},{"location":"api/protocols/#event-protocol","title":"Event Protocol","text":""},{"location":"api/protocols/#event","title":"Event","text":"<p>Module: <code>pydapter.protocols.event</code></p> <p>The Event class combines multiple protocols into a comprehensive event tracking system.</p> <p>Inheritance:</p> <pre><code>class Event(_BaseEvent, IdentifiableMixin, InvokableMixin, TemporalMixin, EmbeddableMixin):\n    # Combines all major protocols\n</code></pre> <p>Event Fields (from BASE_EVENT_FIELDS):</p> <ul> <li><code>id</code>: Unique identifier (UUID, frozen)</li> <li><code>created_at</code>: Creation timestamp</li> <li><code>updated_at</code>: Last update timestamp</li> <li><code>embedding</code>: Vector representation</li> <li><code>execution</code>: Execution state tracking</li> <li><code>request</code>: Request parameters (dict)</li> <li><code>content</code>: Event content (str | dict | JsonValue | None)</li> <li><code>event_type</code>: Event classification (str | None)</li> </ul> <p>Constructor:</p> <pre><code>def __init__(\n    self,\n    handler: Callable,\n    handler_arg: tuple[Any, ...],\n    handler_kwargs: dict[str, Any],\n    **data,\n):\n</code></pre> <p>Usage:</p> <pre><code>from pydapter.protocols.event import Event\n\nasync def process_user_data(user_id: str):\n    return {\"user_id\": user_id, \"processed\": True}\n\nevent = Event(\n    handler=process_user_data,\n    handler_arg=(\"user123\",),\n    handler_kwargs={},\n    content=\"Processing user data\",\n    event_type=\"data_processing\"\n)\n\nawait event.invoke()\nprint(event.execution.status)  # ExecutionStatus.COMPLETED\n</code></pre>"},{"location":"api/protocols/#event-decorator-as_event","title":"Event Decorator: as_event","text":"<p>Function: <code>as_event</code></p> <p>Transforms functions into event-tracked operations.</p> <p>Signature:</p> <pre><code>def as_event(\n    *,\n    event_type: str | None = None,\n    request_arg: str | None = None,\n    embed_content: bool = False,\n    embed_function: Callable[..., Embedding] | None = None,\n    adapt: bool = False,\n    adapter: type[Adapter | AsyncAdapter] | None = None,\n    content_parser: Callable | None = None,\n    strict_content: bool = False,\n    **kw\n) -&gt; Callable\n</code></pre> <p>Basic Usage:</p> <pre><code>from pydapter.protocols.event import as_event\n\n@as_event(event_type=\"api_call\")\nasync def process_request(data: dict) -&gt; dict:\n    return {\"result\": \"processed\", \"input\": data}\n\n# Returns an Event object\nevent = await process_request({\"user_id\": 123})\nprint(event.event_type)  # \"api_call\"\n</code></pre> <p>Advanced Usage with Embedding and Persistence:</p> <pre><code>from pydapter.protocols.event import as_event\nfrom pydapter.extras import AsyncPostgresAdapter\n\ndef my_embedding_function(text: str) -&gt; list[float]:\n    # Your embedding logic\n    return [0.1, 0.2, 0.3]\n\n@as_event(\n    event_type=\"ml_inference\",\n    embed_content=True,\n    embed_function=my_embedding_function,\n    adapt=True,\n    adapter=AsyncPostgresAdapter,\n    content_parser=lambda response: response.get(\"prediction\"),\n    database_url=\"postgresql://...\"\n)\nasync def run_model(input_data):\n    prediction = {\"prediction\": \"positive\", \"confidence\": 0.95}\n    return prediction\n\n# Event is automatically created, embedded, and stored\nevent = await run_model({\"text\": \"This is great!\"})\n</code></pre>"},{"location":"api/protocols/#protocol-composition","title":"Protocol Composition","text":""},{"location":"api/protocols/#multiple-protocol-inheritance","title":"Multiple Protocol Inheritance","text":"<p>Protocols are designed to be composed through multiple inheritance:</p> <pre><code>from pydapter.protocols import (\n    IdentifiableMixin,\n    TemporalMixin,\n    EmbeddableMixin,\n    CryptographicalMixin\n)\nfrom pydantic import BaseModel\n\n# Combine multiple protocols\nclass RichDocument(\n    BaseModel,\n    IdentifiableMixin,     # Adds: id\n    TemporalMixin,         # Adds: created_at, updated_at\n    EmbeddableMixin,       # Adds: content, embedding\n    CryptographicalMixin   # Adds: sha256\n):\n    title: str\n    category: str\n\n# Use all protocol features\ndoc = RichDocument(\n    title=\"Research Paper\",\n    category=\"AI\",\n    content=\"Deep learning research...\",\n    embedding=[0.1, 0.2, 0.3]\n)\n\ndoc.update_timestamp()    # Temporal\ndoc.hash_content()        # Cryptographical\nprint(doc.n_dim)          # Embeddable\nprint(doc.id)             # Identifiable\n</code></pre>"},{"location":"api/protocols/#selective-protocol-usage","title":"Selective Protocol Usage","text":"<p>Use only the protocols you need:</p> <pre><code># Just identification and timestamps\nclass SimpleEvent(BaseModel, IdentifiableMixin, TemporalMixin):\n    action: str\n    user_id: str\n\n# Just embedding capability\nclass EmbeddedText(BaseModel, EmbeddableMixin):\n    text: str\n\n# Just execution tracking\nclass ExecutableTask(BaseModel, InvokableMixin):\n    task_name: str\n    execution: Execution\n</code></pre>"},{"location":"api/protocols/#field-integration","title":"Field Integration","text":"<p>Protocols integrate with the <code>pydapter.fields</code> system through pre-defined field definitions:</p> <pre><code>from pydapter.protocols.event import BASE_EVENT_FIELDS\n\n# Standard event field definitions\nBASE_EVENT_FIELDS = [\n    ID_FROZEN.copy(name=\"id\"),              # From pydapter.fields\n    DATETIME.copy(name=\"created_at\"),       # From pydapter.fields\n    DATETIME.copy(name=\"updated_at\"),       # From pydapter.fields\n    EMBEDDING.copy(name=\"embedding\"),       # From pydapter.fields\n    EXECUTION.copy(name=\"execution\"),       # From pydapter.fields\n    PARAMS.copy(name=\"request\"),            # From pydapter.fields\n    # ... additional fields\n]\n</code></pre>"},{"location":"api/protocols/#best-practices","title":"Best Practices","text":""},{"location":"api/protocols/#protocol-selection","title":"Protocol Selection","text":"<ol> <li>Use Minimal Sets: Only include protocols you actually need</li> <li>Composition Over Inheritance: Prefer multiple protocol mixins over    complex hierarchies</li> <li>Field Consistency: Use standard field definitions from <code>pydapter.fields</code></li> </ol>"},{"location":"api/protocols/#event-tracking","title":"Event Tracking","text":"<ol> <li>Strategic Decoration: Use <code>@as_event</code> for important business operations</li> <li>Content Management: Implement robust content parsing for complex    responses</li> <li>Error Handling: Handle execution failures gracefully</li> <li>Performance: Consider overhead for high-frequency operations</li> </ol>"},{"location":"api/protocols/#protocol-patterns","title":"Protocol Patterns","text":"<pre><code># Pattern 1: Basic entity with tracking\nclass TrackedEntity(BaseModel, IdentifiableMixin, TemporalMixin):\n    pass\n\n# Pattern 2: AI/ML document\nclass AIDocument(BaseModel, IdentifiableMixin, TemporalMixin, EmbeddableMixin):\n    pass\n\n# Pattern 3: Executable event\nclass ExecutableEvent(BaseModel, IdentifiableMixin, TemporalMixin, InvokableMixin):\n    pass\n\n# Pattern 4: Full event (all protocols)\nclass FullEvent(Event):  # Already combines all protocols\n    pass\n</code></pre>"},{"location":"api/protocols/#migration-guide","title":"Migration Guide","text":"<p>When upgrading from previous versions:</p> <ol> <li>Protocol Independence: Update code that assumed hierarchical inheritance</li> <li>Field Integration: Migrate to standardized field definitions</li> <li>Event Composition: Use Event class for comprehensive event tracking</li> <li>Mixin Usage: Prefer mixin classes over protocol interfaces for    implementation</li> </ol> <p>For detailed migration instructions, see the Migration Guide.</p>"},{"location":"guides/","title":"Pydapter Development Guides","text":""},{"location":"guides/#overview","title":"Overview","text":"<p>Pydapter is a protocol-driven data transformation framework that emphasizes stateless adapters, composable protocols, and type safety. These guides help LLM developers understand and extend the framework effectively.</p>"},{"location":"guides/#quick-navigation","title":"Quick Navigation","text":""},{"location":"guides/#core-concepts","title":"Core Concepts","text":"<ul> <li>Architecture: Protocol-driven design, stateless   adapters, dual sync/async APIs</li> <li>Protocols: Identifiable, Temporal, Embeddable protocols   with mixins</li> <li>Fields: Advanced field descriptors, composition methods,   adapter integration</li> </ul>"},{"location":"guides/#implementation-guides","title":"Implementation Guides","text":"<ul> <li>Creating Adapters: Custom adapter patterns, error   handling, metadata integration</li> <li>Async Patterns: Async adapters, concurrency control,   resource management</li> <li>Testing Strategies: Protocol testing, adapter   testing, async testing patterns</li> </ul>"},{"location":"guides/#end-to-end-examples","title":"End-to-End Examples","text":"<ul> <li>Multi-Database Backend: Building backends with   PostgreSQL, MongoDB, Neo4j</li> </ul>"},{"location":"guides/#getting-started-flow","title":"Getting Started Flow","text":""},{"location":"guides/#1-understand-the-architecture","title":"1. Understand the Architecture","text":"<p>Start with Architecture to grasp pydapter's core principles:</p> <ul> <li>Protocol + Mixin pattern for composable behaviors</li> <li>Stateless class methods for transformations</li> <li>Separate sync/async implementations</li> </ul>"},{"location":"guides/#2-define-your-models","title":"2. Define Your Models","text":"<p>Use Protocols to add standardized behaviors:</p> <pre><code>class User(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID\n    created_at: datetime\n    updated_at: datetime\n    name: str\n    email: str\n</code></pre>"},{"location":"guides/#3-configure-fields","title":"3. Configure Fields","text":"<p>Leverage Fields for reusable, validated field definitions:</p> <pre><code>from pydapter.fields import ID_FROZEN, DATETIME\n\nclass User(BaseModel):\n    id: UUID = ID_FROZEN.field_info\n    created_at: datetime = DATETIME.field_info\n</code></pre>"},{"location":"guides/#4-create-adapters","title":"4. Create Adapters","text":"<p>Follow Creating Adapters for data transformations:</p> <pre><code>class YamlAdapter(Adapter[T]):\n    obj_key = \"yaml\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: str, /, *, many=False, **kw):\n        data = yaml.safe_load(obj)\n        return subj_cls.model_validate(data)\n</code></pre>"},{"location":"guides/#5-handle-async-operations","title":"5. Handle Async Operations","text":"<p>Use Async Patterns for async data sources:</p> <pre><code>class ApiAdapter(AsyncAdapter[T]):\n    obj_key = \"api\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        async with aiohttp.ClientSession() as session:\n            async with session.get(obj[\"url\"]) as response:\n                data = await response.json()\n        return subj_cls.model_validate(data)\n</code></pre>"},{"location":"guides/#6-test-your-implementation","title":"6. Test Your Implementation","text":"<p>Apply Testing Strategies for robust validation:</p> <pre><code>def test_adapter_roundtrip():\n    original = User(name=\"test\", email=\"test@example.com\")\n    external = YamlAdapter.to_obj(original)\n    restored = YamlAdapter.from_obj(User, external)\n    assert restored == original\n</code></pre>"},{"location":"guides/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"guides/#protocol-composition","title":"Protocol Composition","text":"<pre><code># Standard entity pattern\nclass Entity(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID\n    created_at: datetime\n    updated_at: datetime\n\n# ML content pattern\nclass MLContent(BaseModel, IdentifiableMixin, EmbeddableMixin):\n    id: UUID\n    content: str | None = None\n    embedding: list[float] = Field(default_factory=list)\n</code></pre>"},{"location":"guides/#adapter-registry-pattern","title":"Adapter Registry Pattern","text":"<pre><code># Create registry\nregistry = AdapterRegistry()\n\n# Register adapters\nregistry.register(JsonAdapter)\nregistry.register(YamlAdapter)\nregistry.register(CsvAdapter)\n\n# Use through registry\nuser = registry.adapt_from(User, data, obj_key=\"json\")\noutput = registry.adapt_to(user, obj_key=\"yaml\")\n</code></pre>"},{"location":"guides/#repository-pattern","title":"Repository Pattern","text":"<pre><code>class UserRepository:\n    def __init__(self, registry: AsyncAdapterRegistry):\n        self.registry = registry\n\n    async def get_by_id(self, user_id: UUID) -&gt; User | None:\n        config = {\"query\": \"SELECT * FROM users WHERE id = $1\", \"params\": [user_id]}\n        return await self.registry.adapt_from(User, config, obj_key=\"postgres\")\n</code></pre>"},{"location":"guides/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/#1-multi-format-data-processing","title":"1. Multi-Format Data Processing","text":"<p>Handle JSON, CSV, YAML, XML with unified interface:</p> <pre><code>registry = AdapterRegistry()\nregistry.register(JsonAdapter)\nregistry.register(CsvAdapter)\nregistry.register(YamlAdapter)\n\n# Process any format\ndef process_data(data: str, format: str) -&gt; list[User]:\n    return registry.adapt_from(User, data, obj_key=format, many=True)\n</code></pre>"},{"location":"guides/#2-database-abstraction","title":"2. Database Abstraction","text":"<p>Work with multiple databases through consistent interface:</p> <pre><code># PostgreSQL for ACID transactions\nusers = await postgres_registry.adapt_from(User, postgres_config, obj_key=\"postgres\")\n\n# MongoDB for analytics\nawait mongo_registry.adapt_to(users, obj_key=\"mongo\", collection=\"user_analytics\")\n\n# Neo4j for relationships\nawait neo4j_registry.adapt_to(users, obj_key=\"neo4j\", relationship=\"KNOWS\")\n</code></pre>"},{"location":"guides/#3-api-integration","title":"3. API Integration","text":"<p>Transform between internal models and external APIs:</p> <pre><code># Fetch from external API\nexternal_users = await api_adapter.from_obj(ExternalUser, {\"url\": api_url}, many=True)\n\n# Transform to internal format\ninternal_users = [InternalUser.model_validate(user.model_dump()) for user in external_users]\n\n# Store in database\nawait db_adapter.to_obj(internal_users, many=True, table=\"users\")\n</code></pre>"},{"location":"guides/#advanced-topics","title":"Advanced Topics","text":""},{"location":"guides/#field-metadata-for-adapters","title":"Field Metadata for Adapters","text":"<pre><code>VECTOR_FIELD = Field(\n    name=\"embedding\",\n    annotation=list[float],\n    json_schema_extra={\n        \"vector_dim\": 768,\n        \"distance_metric\": \"cosine\"\n    }\n)\n\n# Adapters can use this metadata\nclass VectorDBAdapter(Adapter[T]):\n    @classmethod\n    def to_obj(cls, subj: T, **kw):\n        for field_name, field_info in subj.model_fields.items():\n            extra = field_info.json_schema_extra or {}\n            if extra.get(\"vector_dim\"):\n                # Create vector column with specified dimension\n                pass\n</code></pre>"},{"location":"guides/#custom-protocol-creation","title":"Custom Protocol Creation","text":"<pre><code>@runtime_checkable\nclass Auditable(Protocol):\n    audit_log: list[str]\n\nclass AuditableMixin:\n    def add_audit_entry(self, action: str) -&gt; None:\n        if not hasattr(self, 'audit_log'):\n            self.audit_log = []\n        self.audit_log.append(f\"{datetime.now()}: {action}\")\n</code></pre>"},{"location":"guides/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Connection pooling for database adapters\nclass PooledDBAdapter(AsyncAdapter[T]):\n    _pool: asyncpg.Pool = None\n\n    @classmethod\n    async def _get_pool(cls):\n        if cls._pool is None:\n            cls._pool = await asyncpg.create_pool(connection_string)\n        return cls._pool\n\n# Concurrent processing\nasync def process_multiple_sources(sources: list[dict]) -&gt; list[T]:\n    semaphore = asyncio.Semaphore(5)  # Limit concurrency\n\n    async def process_one(source):\n        async with semaphore:\n            return await SomeAdapter.from_obj(MyModel, source)\n\n    results = await asyncio.gather(*[process_one(s) for s in sources])\n    return [r for r in results if r is not None]\n</code></pre>"},{"location":"guides/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"guides/#1-design-principles","title":"1. Design Principles","text":"<ul> <li>Use protocols for behavioral contracts</li> <li>Keep adapters stateless with class methods</li> <li>Compose behaviors through mixins</li> <li>Separate sync and async implementations</li> </ul>"},{"location":"guides/#2-error-handling","title":"2. Error Handling","text":"<ul> <li>Use specific exception types (<code>ParseError</code>, <code>ValidationError</code>)</li> <li>Provide detailed error context</li> <li>Test error paths thoroughly</li> </ul>"},{"location":"guides/#3-testing-strategy","title":"3. Testing Strategy","text":"<ul> <li>Test protocol compliance</li> <li>Verify adapter roundtrips</li> <li>Mock external dependencies</li> <li>Use property-based testing for edge cases</li> </ul>"},{"location":"guides/#4-performance","title":"4. Performance","text":"<ul> <li>Use connection pooling for databases</li> <li>Implement concurrency control with semaphores</li> <li>Add timeout and retry logic for external services</li> <li>Monitor and optimize hot paths</li> </ul> <p>For detailed information on any topic, see the specific guide linked above.</p>"},{"location":"guides/architecture/","title":"Pydapter Architecture and Design Philosophy","text":""},{"location":"guides/architecture/#core-principles","title":"Core Principles","text":"<p>Pydapter follows protocol-driven architecture with stateless transformations and composition over inheritance.</p>"},{"location":"guides/architecture/#1-protocol-mixin-pattern","title":"1. Protocol + Mixin Pattern","text":"<pre><code>@runtime_checkable\nclass Identifiable(Protocol):\n    id: UUID\n\nclass IdentifiableMixin:\n    def __hash__(self) -&gt; int:\n        return hash(self.id)\n</code></pre> <p>Key Benefits:</p> <ul> <li>Type safety without inheritance coupling</li> <li>Runtime validation when needed</li> <li>Composable behaviors</li> </ul>"},{"location":"guides/architecture/#2-stateless-class-methods","title":"2. Stateless Class Methods","text":"<pre><code>class Adapter(Protocol[T]):\n    obj_key: ClassVar[str]\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: Any, /, *, many=False, **kw) -&gt; T | list[T]: ...\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many=False, **kw) -&gt; Any: ...\n</code></pre> <p>Why Class Methods:</p> <ul> <li>Thread safety (no shared state)</li> <li>No instantiation overhead</li> <li>Simple testing</li> <li>Clear interfaces</li> </ul>"},{"location":"guides/architecture/#3-dual-syncasync-apis","title":"3. Dual Sync/Async APIs","text":"<p>Separate implementations without mixing concerns:</p> <ul> <li>Sync: <code>Adapter</code>, <code>AdapterRegistry</code>, <code>Adaptable</code></li> <li>Async: <code>AsyncAdapter</code>, <code>AsyncAdapterRegistry</code>, <code>AsyncAdaptable</code></li> </ul> <p>Benefits:</p> <ul> <li>No async overhead in sync code</li> <li>Clear separation of concerns</li> <li>Type safety in both contexts</li> </ul>"},{"location":"guides/architecture/#component-layers","title":"Component Layers","text":""},{"location":"guides/architecture/#core-layer","title":"Core Layer","text":"<ul> <li><code>Adapter</code>: Transformation protocol</li> <li><code>AdapterRegistry</code>: Adapter management</li> <li><code>Adaptable</code>: Model mixin for adapter access</li> </ul>"},{"location":"guides/architecture/#protocol-layer","title":"Protocol Layer","text":"<ul> <li><code>Identifiable</code>: UUID-based identity</li> <li><code>Temporal</code>: Timestamp management</li> <li><code>Embeddable</code>: Vector embeddings</li> <li><code>Event</code>: Event-driven patterns</li> </ul>"},{"location":"guides/architecture/#field-system","title":"Field System","text":"<ul> <li><code>Field</code>: Advanced field descriptors</li> <li>Pre-configured fields: <code>ID_FROZEN</code>, <code>DATETIME</code>, <code>EMBEDDING</code></li> <li>Composition methods: <code>as_nullable()</code>, <code>as_listable()</code></li> </ul>"},{"location":"guides/architecture/#adapter-ecosystem","title":"Adapter Ecosystem","text":"<ul> <li>Built-in: JSON, CSV, TOML</li> <li>Extended: PostgreSQL, MongoDB, Neo4j, Qdrant, Weaviate</li> </ul>"},{"location":"guides/architecture/#design-philosophy","title":"Design Philosophy","text":""},{"location":"guides/architecture/#composition-over-inheritance","title":"Composition Over Inheritance","text":"<pre><code>class Document(BaseModel, IdentifiableMixin, TemporalMixin):\n    title: str\n    content: str\n</code></pre>"},{"location":"guides/architecture/#progressive-complexity","title":"Progressive Complexity","text":"<pre><code># Simple: Direct usage\nperson = JsonAdapter.from_obj(Person, json_data)\n\n# Advanced: Registry-based\nregistry.adapt_from(Person, data, obj_key=\"json\")\n</code></pre>"},{"location":"guides/architecture/#explicit-configuration","title":"Explicit Configuration","text":"<pre><code># Clear interfaces, explicit parameters\nperson = JsonAdapter.from_obj(Person, data, many=False, strict=True)\n</code></pre>"},{"location":"guides/architecture/#extension-points","title":"Extension Points","text":"<ol> <li>Custom Adapters: Implement <code>Adapter</code>/<code>AsyncAdapter</code> protocol</li> <li>Custom Protocols: Extend existing or create new protocols</li> <li>Field Descriptors: Domain-specific fields with <code>Field</code></li> <li>Migration Adapters: Schema evolution support</li> <li>Registry Extensions: Specialized adapter collections</li> </ol> <p>This architecture enables both simple use cases and complex production systems through clear abstractions and composable components.</p>"},{"location":"guides/async-patterns/","title":"Async/Sync Patterns in Pydapter","text":""},{"location":"guides/async-patterns/#architecture-decision","title":"Architecture Decision","text":"<p>Pydapter provides separate sync and async implementations without mixing concerns:</p> <ul> <li>Sync: <code>Adapter</code>, <code>AdapterRegistry</code>, <code>Adaptable</code></li> <li>Async: <code>AsyncAdapter</code>, <code>AsyncAdapterRegistry</code>, <code>AsyncAdaptable</code></li> </ul> <p>Benefits:</p> <ul> <li>No async overhead in sync operations</li> <li>Clear separation of concerns</li> <li>Type safety in both contexts</li> <li>Simple, focused interfaces</li> </ul>"},{"location":"guides/async-patterns/#core-async-components","title":"Core Async Components","text":""},{"location":"guides/async-patterns/#asyncadapter-protocol","title":"AsyncAdapter Protocol","text":"<pre><code>from pydapter.async_core import AsyncAdapter\n\nclass MyAsyncAdapter(AsyncAdapter[T]):\n    obj_key = \"my_async_format\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: Any, /, *, many=False, **kw) -&gt; T | list[T]:\n        # Async operations (HTTP, database, etc.)\n        pass\n\n    @classmethod\n    async def to_obj(cls, subj: T | list[T], /, *, many=False, **kw) -&gt; Any:\n        # Async output operations\n        pass\n</code></pre>"},{"location":"guides/async-patterns/#asyncadapterregistry","title":"AsyncAdapterRegistry","text":"<pre><code>from pydapter.async_core import AsyncAdapterRegistry\n\nasync_registry = AsyncAdapterRegistry()\nasync_registry.register(MyAsyncAdapter)\n\nresult = await async_registry.adapt_from(MyModel, data, obj_key=\"my_async_format\")\n</code></pre>"},{"location":"guides/async-patterns/#asyncadaptable-mixin","title":"AsyncAdaptable Mixin","text":"<pre><code>from pydapter.async_core import AsyncAdaptable\n\nclass MyModel(BaseModel, AsyncAdaptable):\n    name: str\n    value: int\n\nMyModel.register_async_adapter(MyAsyncAdapter)\ninstance = await MyModel.adapt_from_async(data, obj_key=\"my_async_format\")\n</code></pre>"},{"location":"guides/async-patterns/#common-async-patterns","title":"Common Async Patterns","text":""},{"location":"guides/async-patterns/#http-api-adapter","title":"HTTP API Adapter","text":"<pre><code>class RestApiAdapter(AsyncAdapter[T]):\n    obj_key = \"rest_api\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        url = f\"{obj['base_url']}/{obj['endpoint']}\"\n\n        async with aiohttp.ClientSession() as session:\n            async with session.get(url) as response:\n                response.raise_for_status()\n                data = await response.json()\n\n        if many:\n            items = data.get(\"items\", data) if isinstance(data, dict) else data\n            return [subj_cls.model_validate(item) for item in items]\n        return subj_cls.model_validate(data)\n</code></pre>"},{"location":"guides/async-patterns/#database-adapter","title":"Database Adapter","text":"<pre><code>class AsyncPostgresAdapter(AsyncAdapter[T]):\n    obj_key = \"async_postgres\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        conn = await asyncpg.connect(obj[\"connection_string\"])\n        try:\n            if many:\n                rows = await conn.fetch(obj[\"query\"], *obj.get(\"params\", []))\n                return [subj_cls.model_validate(dict(row)) for row in rows]\n            else:\n                row = await conn.fetchrow(obj[\"query\"], *obj.get(\"params\", []))\n                return subj_cls.model_validate(dict(row)) if row else None\n        finally:\n            await conn.close()\n</code></pre>"},{"location":"guides/async-patterns/#concurrent-operations","title":"Concurrent Operations","text":"<pre><code>class ConcurrentAdapter(AsyncAdapter[T]):\n    obj_key = \"concurrent\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        sources = obj[\"sources\"]\n        max_concurrent = kw.get(\"max_concurrent\", 5)\n\n        semaphore = asyncio.Semaphore(max_concurrent)\n\n        async def fetch_one(source):\n            async with semaphore:\n                # Fetch from individual source\n                return await SomeAdapter.from_obj(subj_cls, source)\n\n        results = await asyncio.gather(\n            *[fetch_one(source) for source in sources],\n            return_exceptions=True\n        )\n\n        # Filter successful results\n        successful = [r for r in results if not isinstance(r, Exception)]\n        return successful if many else (successful[0] if successful else None)\n</code></pre>"},{"location":"guides/async-patterns/#sharing-logic-between-syncasync","title":"Sharing Logic Between Sync/Async","text":"<p>Use mixins for shared transformation logic:</p> <pre><code>class DataTransformationMixin:\n    @staticmethod\n    def normalize_data(data: dict) -&gt; dict:\n        return {\n            \"id\": data.get(\"identifier\") or data.get(\"id\"),\n            \"name\": (data.get(\"name\") or \"\").strip(),\n            \"active\": data.get(\"status\") == \"active\",\n        }\n\n# Sync adapter\nclass MySyncAdapter(Adapter[T], DataTransformationMixin):\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        normalized = cls.normalize_data(obj)\n        return subj_cls.model_validate(normalized)\n\n# Async adapter using same logic\nclass MyAsyncAdapter(AsyncAdapter[T], DataTransformationMixin):\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        data = await cls._fetch_async_data(obj[\"url\"])\n        normalized = cls.normalize_data(data)\n        return subj_cls.model_validate(normalized)\n</code></pre>"},{"location":"guides/async-patterns/#error-handling-patterns","title":"Error Handling Patterns","text":""},{"location":"guides/async-patterns/#timeout-management","title":"Timeout Management","text":"<pre><code>class TimeoutAwareAdapter(AsyncAdapter[T]):\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        timeout_seconds = kw.get(\"timeout\", 30)\n\n        try:\n            async with asyncio.timeout(timeout_seconds):\n                return await cls._fetch_data(obj, subj_cls)\n        except asyncio.TimeoutError:\n            raise ParseError(f\"Operation timed out after {timeout_seconds}s\")\n</code></pre>"},{"location":"guides/async-patterns/#retry-logic","title":"Retry Logic","text":"<pre><code>class RetryableAdapter(AsyncAdapter[T]):\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        max_retries = kw.get(\"max_retries\", 3)\n\n        for attempt in range(max_retries + 1):\n            try:\n                return await cls._attempt_fetch(subj_cls, obj)\n            except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n                if attempt &lt; max_retries:\n                    await asyncio.sleep(2 ** attempt)  # Exponential backoff\n                    continue\n                raise ParseError(f\"Failed after {max_retries + 1} attempts: {e}\")\n</code></pre>"},{"location":"guides/async-patterns/#testing-async-adapters","title":"Testing Async Adapters","text":"<pre><code>@pytest.mark.asyncio\nclass TestAsyncAdapters:\n    async def test_http_adapter(self, respx_mock):\n        respx_mock.get(\"http://api.example.com/data\").mock(\n            return_value=httpx.Response(200, json={\"name\": \"test\"})\n        )\n\n        result = await RestApiAdapter.from_obj(\n            MyModel, {\"base_url\": \"http://api.example.com\", \"endpoint\": \"data\"}\n        )\n        assert result.name == \"test\"\n\n    async def test_timeout_handling(self):\n        with pytest.raises(ParseError, match=\"timed out\"):\n            await TimeoutAwareAdapter.from_obj(MyModel, config, timeout=0.1)\n</code></pre>"},{"location":"guides/async-patterns/#key-tips-for-llm-developers","title":"Key Tips for LLM Developers","text":""},{"location":"guides/async-patterns/#1-resource-management","title":"1. Resource Management","text":"<pre><code># \u2713 Always use context managers\nasync with aiohttp.ClientSession() as session:\n    async with session.get(url) as response:\n        data = await response.json()\n\n# \u2713 Proper cleanup\nconn = await asyncpg.connect(connection_string)\ntry:\n    result = await conn.fetch(query)\nfinally:\n    await conn.close()\n</code></pre>"},{"location":"guides/async-patterns/#2-concurrency-control","title":"2. Concurrency Control","text":"<pre><code># \u2713 Use semaphore for rate limiting\nsemaphore = asyncio.Semaphore(max_concurrent)\n\nasync def process_item(item):\n    async with semaphore:\n        # Process item without overwhelming external services\n        pass\n</code></pre>"},{"location":"guides/async-patterns/#3-common-async-caveats","title":"3. Common Async Caveats","text":"<ul> <li>Context managers: Always use <code>async with</code> for resource cleanup</li> <li>Timeouts: Set reasonable timeouts for external operations</li> <li>Error handling: Distinguish between retryable and non-retryable errors</li> <li>Concurrency limits: Use semaphores to avoid overwhelming external services</li> <li>Testing: Use <code>pytest.mark.asyncio</code> and mock async dependencies</li> </ul>"},{"location":"guides/async-patterns/#4-separation-of-concerns","title":"4. Separation of Concerns","text":"<pre><code># \u2713 Keep sync and async adapters separate\nclass SyncAdapter(Adapter[T]): pass\nclass AsyncAdapter(AsyncAdapter[T]): pass\n\n# \u2717 Avoid mixing sync/async in same class\n</code></pre>"},{"location":"guides/async-patterns/#5-performance-patterns","title":"5. Performance Patterns","text":"<ul> <li>Use <code>asyncio.gather()</code> for concurrent operations</li> <li>Implement connection pooling for database adapters</li> <li>Add circuit breakers for external service calls</li> <li>Use exponential backoff for retry logic</li> </ul> <p>This dual-API approach ensures optimal performance and clear separation between synchronous and asynchronous contexts.</p>"},{"location":"guides/creating-adapters/","title":"Creating Custom Adapters","text":""},{"location":"guides/creating-adapters/#adapter-interface","title":"Adapter Interface","text":"<p>All adapters implement the <code>Adapter</code> protocol:</p> <pre><code>from pydapter.core import Adapter\nfrom typing import ClassVar, TypeVar, Any\n\nT = TypeVar(\"T\", bound=BaseModel)\n\nclass MyAdapter(Adapter[T]):\n    obj_key: ClassVar[str] = \"my_format\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: Any, /, *, many=False, **kw) -&gt; T | list[T]:\n        \"\"\"Convert from external format to model\"\"\"\n        pass\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many=False, **kw) -&gt; Any:\n        \"\"\"Convert from model to external format\"\"\"\n        pass\n</code></pre>"},{"location":"guides/creating-adapters/#basic-implementation-pattern","title":"Basic Implementation Pattern","text":"<pre><code>from pydapter.exceptions import ParseError, ValidationError as AdapterValidationError\n\nclass YamlAdapter(Adapter[T]):\n    obj_key = \"yaml\"\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: str | Path, /, *, many=False, **kw):\n        try:\n            # Handle input types\n            text = obj.read_text() if isinstance(obj, Path) else obj\n\n            # Parse format\n            data = yaml.safe_load(text)\n\n            # Validate and convert\n            if many:\n                return [subj_cls.model_validate(item) for item in data]\n            return subj_cls.model_validate(data)\n\n        except yaml.YAMLError as e:\n            raise ParseError(f\"Invalid YAML: {e}\", source=str(obj)[:100])\n        except ValidationError as e:\n            raise AdapterValidationError(f\"Validation failed: {e}\", errors=e.errors())\n\n    @classmethod\n    def to_obj(cls, subj: T | list[T], /, *, many=False, **kw) -&gt; str:\n        items = subj if isinstance(subj, list) else [subj]\n        payload = [item.model_dump() for item in items] if many else items[0].model_dump()\n        return yaml.dump(payload, **kw)\n</code></pre>"},{"location":"guides/creating-adapters/#error-handling","title":"Error Handling","text":""},{"location":"guides/creating-adapters/#exception-hierarchy","title":"Exception Hierarchy","text":"<ul> <li><code>ParseError</code>: Invalid format/data structure</li> <li><code>ValidationError</code>: Model validation failures</li> <li><code>AdapterError</code>: General adapter issues</li> </ul>"},{"location":"guides/creating-adapters/#error-context-pattern","title":"Error Context Pattern","text":"<pre><code>try:\n    # Adapter logic\n    pass\nexcept ParseError:\n    raise  # Re-raise pydapter exceptions\nexcept ValidationError as e:\n    raise AdapterValidationError(\"Validation failed\", data=data, errors=e.errors())\nexcept Exception as e:\n    raise ParseError(f\"Unexpected error: {e}\", source=str(obj)[:100])\n</code></pre>"},{"location":"guides/creating-adapters/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/creating-adapters/#configuration-support","title":"Configuration Support","text":"<pre><code>class DatabaseAdapter(Adapter[T]):\n    DEFAULT_CONFIG = {\"batch_size\": 1000, \"timeout\": 30}\n\n    @classmethod\n    def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        config = {**cls.DEFAULT_CONFIG, **kw}\n        # Use config for connection settings, timeouts, etc.\n</code></pre>"},{"location":"guides/creating-adapters/#metadata-integration","title":"Metadata Integration","text":"<pre><code>@classmethod\ndef to_obj(cls, subj: T | list[T], /, *, many=False, **kw):\n    for field_name, field_info in subj.model_fields.items():\n        extra = field_info.json_schema_extra or {}\n\n        # Use field metadata for custom formatting\n        if extra.get(\"db_column\"):\n            # Map to different column name\n        if extra.get(\"vector_dim\"):\n            # Handle vector data specially\n</code></pre>"},{"location":"guides/creating-adapters/#async-adapters","title":"Async Adapters","text":"<pre><code>from pydapter.async_core import AsyncAdapter\n\nclass HttpApiAdapter(AsyncAdapter[T]):\n    obj_key = \"http_api\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[T], obj: dict, /, *, many=False, **kw):\n        url = obj[\"url\"]\n\n        async with aiohttp.ClientSession() as session:\n            async with session.get(url) as response:\n                data = await response.json()\n\n        if many:\n            return [subj_cls.model_validate(item) for item in data]\n        return subj_cls.model_validate(data)\n</code></pre>"},{"location":"guides/creating-adapters/#testing-strategy","title":"Testing Strategy","text":"<pre><code>class TestMyAdapter:\n    def test_roundtrip(self):\n        \"\"\"Test data survives roundtrip conversion\"\"\"\n        original = MyModel(name=\"test\", value=42)\n        external = MyAdapter.to_obj(original)\n        restored = MyAdapter.from_obj(MyModel, external)\n        assert restored == original\n\n    def test_error_handling(self):\n        with pytest.raises(ParseError):\n            MyAdapter.from_obj(MyModel, \"invalid_data\")\n</code></pre>"},{"location":"guides/creating-adapters/#registry-integration","title":"Registry Integration","text":"<pre><code>from pydapter.core import AdapterRegistry\n\nregistry = AdapterRegistry()\nregistry.register(YamlAdapter)\n\n# Use through registry\nuser = registry.adapt_from(User, yaml_data, obj_key=\"yaml\")\n</code></pre>"},{"location":"guides/creating-adapters/#key-tips-for-llm-developers","title":"Key Tips for LLM Developers","text":""},{"location":"guides/creating-adapters/#1-stateless-design","title":"1. Stateless Design","text":"<ul> <li>Use <code>@classmethod</code> for all adapter methods</li> <li>No instance variables or shared state</li> <li>Thread-safe by design</li> </ul>"},{"location":"guides/creating-adapters/#2-error-handling","title":"2. Error Handling","text":"<ul> <li>Always provide context in error messages</li> <li>Use specific exception types</li> <li>Include source data preview (truncated)</li> </ul>"},{"location":"guides/creating-adapters/#3-input-validation","title":"3. Input Validation","text":"<pre><code># Validate input type and structure\nif not isinstance(obj, expected_types):\n    raise ParseError(f\"Expected {expected_types}, got {type(obj)}\")\n\n# Handle edge cases\nif obj is None:\n    return [] if many else None\n</code></pre>"},{"location":"guides/creating-adapters/#4-configuration-patterns","title":"4. Configuration Patterns","text":"<pre><code># Merge defaults with user config\nconfig = {**cls.DEFAULT_CONFIG, **kw}\n\n# Extract specific options\ntimeout = config.pop(\"timeout\", 30)\nbatch_size = config.pop(\"batch_size\", 1000)\n</code></pre>"},{"location":"guides/creating-adapters/#5-common-caveats","title":"5. Common Caveats","text":"<ul> <li>Many parameter: Handle both single items and lists consistently</li> <li>Empty inputs: Return appropriate empty values</li> <li>Path vs string: Support both file paths and direct content</li> <li>Async context: Proper resource cleanup with context managers</li> <li>Error propagation: Re-raise pydapter exceptions, wrap others</li> </ul>"},{"location":"guides/creating-adapters/#6-field-metadata-usage","title":"6. Field Metadata Usage","text":"<pre><code># Access field metadata for custom behavior\nfor field_name, field_info in model.model_fields.items():\n    extra = field_info.json_schema_extra or {}\n    if extra.get(\"custom_format\"):\n        # Apply custom formatting\n</code></pre> <p>This pattern ensures adapters integrate seamlessly with pydapter's ecosystem while maintaining consistency and reliability.</p>"},{"location":"guides/end-to-end-backend/","title":"Building Multi-Database Backends with Pydapter","text":""},{"location":"guides/end-to-end-backend/#overview","title":"Overview","text":"<p>This guide shows how to build flexible backends that work with multiple database backends using pydapter's protocol and adapter system, with async PostgreSQL as the primary database.</p>"},{"location":"guides/end-to-end-backend/#architecture-pattern","title":"Architecture Pattern","text":"<pre><code>Models (Protocols) \u2192 Adapters (DB-specific) \u2192 Registry (Unified Interface)\n</code></pre> <p>Core Concept: Define your models with protocols, implement database-specific adapters, use registry for database abstraction.</p>"},{"location":"guides/end-to-end-backend/#1-define-protocol-based-models","title":"1. Define Protocol-Based Models","text":"<pre><code>from pydantic import BaseModel\nfrom pydapter.protocols import IdentifiableMixin, TemporalMixin\nfrom uuid import UUID\nfrom datetime import datetime\n\nclass User(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID\n    created_at: datetime\n    updated_at: datetime\n    name: str\n    email: str\n    active: bool = True\n\nclass Post(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID\n    created_at: datetime\n    updated_at: datetime\n    title: str\n    content: str\n    author_id: UUID\n    published: bool = False\n</code></pre>"},{"location":"guides/end-to-end-backend/#2-database-adapter-pattern","title":"2. Database Adapter Pattern","text":"<pre><code>from pydapter.async_core import AsyncAdapter\n\nclass AsyncPostgresUserAdapter(AsyncAdapter[User]):\n    obj_key = \"postgres_user\"\n\n    @classmethod\n    async def from_obj(cls, subj_cls: type[User], obj: dict, /, *, many=False, **kw):\n        conn_string = obj[\"connection_string\"]\n        # Query logic here\n        pass\n\n    @classmethod\n    async def to_obj(cls, subj: User | list[User], /, *, many=False, **kw):\n        # Insert/update logic here\n        pass\n\n# Similar adapters for other databases\nclass AsyncMongoUserAdapter(AsyncAdapter[User]):\n    obj_key = \"mongo_user\"\n    # MongoDB-specific implementation\n\nclass AsyncNeo4jUserAdapter(AsyncAdapter[User]):\n    obj_key = \"neo4j_user\"\n    # Neo4j-specific implementation\n</code></pre>"},{"location":"guides/end-to-end-backend/#3-repository-pattern-with-registry","title":"3. Repository Pattern with Registry","text":"<pre><code>from pydapter.async_core import AsyncAdapterRegistry\n\nclass UserRepository:\n    def __init__(self, registry: AsyncAdapterRegistry, default_adapter: str = \"postgres_user\"):\n        self.registry = registry\n        self.default_adapter = default_adapter\n\n    async def get_by_id(self, user_id: UUID, adapter_key: str = None) -&gt; User | None:\n        adapter_key = adapter_key or self.default_adapter\n        query_config = {\n            \"connection_string\": self._get_connection_string(adapter_key),\n            \"query\": \"SELECT * FROM users WHERE id = $1\",\n            \"params\": [user_id]\n        }\n        return await self.registry.adapt_from(User, query_config, obj_key=adapter_key)\n\n    async def create(self, user: User, adapter_key: str = None) -&gt; User:\n        adapter_key = adapter_key or self.default_adapter\n        config = {\n            \"connection_string\": self._get_connection_string(adapter_key),\n            \"table\": \"users\"\n        }\n        return await self.registry.adapt_to(user, obj_key=adapter_key, **config)\n\n    def _get_connection_string(self, adapter_key: str) -&gt; str:\n        # Configuration lookup\n        pass\n</code></pre>"},{"location":"guides/end-to-end-backend/#4-service-layer","title":"4. Service Layer","text":"<pre><code>class UserService:\n    def __init__(self, user_repo: UserRepository, post_repo: PostRepository):\n        self.user_repo = user_repo\n        self.post_repo = post_repo\n\n    async def create_user_with_welcome_post(self, name: str, email: str) -&gt; tuple[User, Post]:\n        # Create user in primary database (PostgreSQL)\n        user = User(id=uuid4(), name=name, email=email,\n                   created_at=datetime.now(), updated_at=datetime.now())\n        created_user = await self.user_repo.create(user)\n\n        # Create welcome post\n        welcome_post = Post(\n            id=uuid4(), title=\"Welcome!\", content=f\"Welcome {name}!\",\n            author_id=created_user.id, created_at=datetime.now(), updated_at=datetime.now()\n        )\n        created_post = await self.post_repo.create(welcome_post)\n\n        # Optionally replicate to other databases for analytics\n        await self._replicate_to_analytics(created_user)\n\n        return created_user, created_post\n\n    async def _replicate_to_analytics(self, user: User):\n        \"\"\"Replicate user data to analytics database (e.g., MongoDB)\"\"\"\n        try:\n            await self.user_repo.create(user, adapter_key=\"mongo_user\")\n        except Exception as e:\n            # Log error but don't fail main operation\n            logger.warning(f\"Failed to replicate user to analytics: {e}\")\n</code></pre>"},{"location":"guides/end-to-end-backend/#5-configuration-and-setup","title":"5. Configuration and Setup","text":"<pre><code># Database configuration\nDATABASE_CONFIG = {\n    \"postgres\": {\n        \"connection_string\": \"postgresql://user:pass@localhost/main\",\n        \"primary\": True\n    },\n    \"mongo\": {\n        \"connection_string\": \"mongodb://localhost:27017/analytics\",\n        \"use_for\": [\"analytics\", \"caching\"]\n    },\n    \"neo4j\": {\n        \"connection_string\": \"bolt://localhost:7687\",\n        \"use_for\": [\"relationships\", \"recommendations\"]\n    }\n}\n\n# Registry setup\nasync def create_registry() -&gt; AsyncAdapterRegistry:\n    registry = AsyncAdapterRegistry()\n\n    # Register all adapters\n    registry.register(AsyncPostgresUserAdapter)\n    registry.register(AsyncMongoUserAdapter)\n    registry.register(AsyncNeo4jUserAdapter)\n    # ... register adapters for other models\n\n    return registry\n\n# Application factory\nasync def create_app():\n    registry = await create_registry()\n\n    user_repo = UserRepository(registry, default_adapter=\"postgres_user\")\n    post_repo = PostRepository(registry, default_adapter=\"postgres_post\")\n\n    user_service = UserService(user_repo, post_repo)\n\n    return user_service\n</code></pre>"},{"location":"guides/end-to-end-backend/#6-fastapi-integration","title":"6. FastAPI Integration","text":"<pre><code>from fastapi import FastAPI, Depends\n\napp = FastAPI()\n\nasync def get_user_service() -&gt; UserService:\n    return await create_app()\n\n@app.post(\"/users\")\nasync def create_user(\n    user_data: dict,\n    service: UserService = Depends(get_user_service)\n):\n    user, welcome_post = await service.create_user_with_welcome_post(\n        name=user_data[\"name\"],\n        email=user_data[\"email\"]\n    )\n    return {\"user\": user.model_dump(), \"welcome_post\": welcome_post.model_dump()}\n\n@app.get(\"/users/{user_id}\")\nasync def get_user(\n    user_id: UUID,\n    source: str = \"postgres\",  # Allow client to specify database\n    service: UserService = Depends(get_user_service)\n):\n    adapter_map = {\n        \"postgres\": \"postgres_user\",\n        \"mongo\": \"mongo_user\",\n        \"neo4j\": \"neo4j_user\"\n    }\n\n    user = await service.user_repo.get_by_id(\n        user_id,\n        adapter_key=adapter_map.get(source, \"postgres_user\")\n    )\n\n    if not user:\n        raise HTTPException(404, \"User not found\")\n\n    return user.model_dump()\n</code></pre>"},{"location":"guides/end-to-end-backend/#key-benefits","title":"Key Benefits","text":""},{"location":"guides/end-to-end-backend/#1-database-flexibility","title":"1. Database Flexibility","text":"<ul> <li>Primary Database: PostgreSQL for ACID transactions</li> <li>Analytics Database: MongoDB for flexible schema and aggregations</li> <li>Graph Database: Neo4j for relationship queries</li> <li>Easy Migration: Change adapters without changing business logic</li> </ul>"},{"location":"guides/end-to-end-backend/#2-protocol-consistency","title":"2. Protocol Consistency","text":"<ul> <li>Models work across all databases through protocols</li> <li>Type safety maintained regardless of storage backend</li> <li>Consistent validation and serialization</li> </ul>"},{"location":"guides/end-to-end-backend/#3-incremental-adoption","title":"3. Incremental Adoption","text":"<ul> <li>Start with one database, add others as needed</li> <li>Gradual migration between databases</li> <li>A/B testing with different storage backends</li> </ul>"},{"location":"guides/end-to-end-backend/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/end-to-end-backend/#multi-database-queries","title":"Multi-Database Queries","text":"<pre><code>async def get_user_with_recommendations(self, user_id: UUID):\n    # Get user from primary database\n    user = await self.user_repo.get_by_id(user_id, \"postgres_user\")\n\n    # Get recommendations from graph database\n    recommendations = await self.recommendation_service.get_for_user(\n        user_id, adapter_key=\"neo4j_recommendation\"\n    )\n\n    return {\"user\": user, \"recommendations\": recommendations}\n</code></pre>"},{"location":"guides/end-to-end-backend/#data-synchronization","title":"Data Synchronization","text":"<pre><code>async def sync_user_across_databases(self, user: User):\n    \"\"\"Ensure user exists in all required databases\"\"\"\n    tasks = [\n        self.user_repo.create(user, \"postgres_user\"),\n        self.user_repo.create(user, \"mongo_user\"),\n        self.user_repo.create(user, \"neo4j_user\"),\n    ]\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Handle partial failures\n    for i, result in enumerate(results):\n        if isinstance(result, Exception):\n            logger.error(f\"Failed to sync user to database {i}: {result}\")\n</code></pre>"},{"location":"guides/end-to-end-backend/#deployment-considerations","title":"Deployment Considerations","text":""},{"location":"guides/end-to-end-backend/#environment-configuration","title":"Environment Configuration","text":"<pre><code># Production: Use environment variables\nPOSTGRES_URL = os.getenv(\"DATABASE_URL\")\nMONGO_URL = os.getenv(\"MONGO_URL\")\nNEO4J_URL = os.getenv(\"NEO4J_URL\")\n\n# Development: Use local databases\n# Testing: Use in-memory or test databases\n</code></pre>"},{"location":"guides/end-to-end-backend/#monitoring-and-observability","title":"Monitoring and Observability","text":"<ul> <li>Log database adapter usage</li> <li>Monitor query performance across databases</li> <li>Track replication lag and sync errors</li> <li>Use health checks for each database</li> </ul> <p>This pattern provides a robust foundation for building scalable backends that can evolve with changing requirements while maintaining clean separation of concerns.</p>"},{"location":"guides/fields/","title":"Working with Pydapter Fields","text":""},{"location":"guides/fields/#field-system-overview","title":"Field System Overview","text":"<p>Pydapter extends Pydantic fields with:</p> <ul> <li>Advanced descriptors</li> <li>Composition methods (<code>as_nullable()</code>, <code>as_listable()</code>)</li> <li>Pre-configured fields for common patterns</li> <li>Metadata for adapter integration</li> </ul>"},{"location":"guides/fields/#core-field-class","title":"Core Field Class","text":"<pre><code>from pydapter.fields import Field\n\n# Basic field definition\nname_field = Field(\n    name=\"name\",\n    annotation=str,\n    description=\"User's full name\",\n    validator=lambda cls, v: v.strip() if v else v\n)\n</code></pre>"},{"location":"guides/fields/#pre-configured-fields","title":"Pre-configured Fields","text":""},{"location":"guides/fields/#essential-fields","title":"Essential Fields","text":"<ul> <li><code>ID_FROZEN</code>: Immutable UUID field</li> <li><code>ID_NULLABLE</code>: Optional UUID field</li> <li><code>DATETIME</code>: Timezone-aware datetime with ISO serialization</li> <li><code>DATETIME_NULLABLE</code>: Optional datetime field</li> <li><code>EMBEDDING</code>: Vector embedding field with validation</li> <li><code>EXECUTION</code>: Execution tracking field</li> </ul>"},{"location":"guides/fields/#usage-pattern","title":"Usage Pattern","text":"<pre><code>from pydapter.fields import ID_FROZEN, DATETIME, EMBEDDING\n\nclass Document(BaseModel):\n    id: UUID = ID_FROZEN.field_info\n    created_at: datetime = DATETIME.field_info\n    embedding: list[float] = EMBEDDING.field_info\n</code></pre>"},{"location":"guides/fields/#field-composition","title":"Field Composition","text":""},{"location":"guides/fields/#nullable-transformation","title":"Nullable Transformation","text":"<pre><code>email_field = Field(name=\"email\", annotation=str, validator=validate_email)\noptional_email = email_field.as_nullable()\n\nclass User(BaseModel):\n    email: str = email_field.field_info\n    backup_email: str | None = optional_email.field_info\n</code></pre>"},{"location":"guides/fields/#list-transformation","title":"List Transformation","text":"<pre><code>tag_field = Field(name=\"tag\", annotation=str, validator=lambda cls, v: v.lower())\ntags_field = tag_field.as_listable()          # Accepts single value or list\nstrict_tags = tag_field.as_listable(strict=True)  # Only accepts lists\n</code></pre>"},{"location":"guides/fields/#custom-field-patterns","title":"Custom Field Patterns","text":""},{"location":"guides/fields/#domain-specific-fields","title":"Domain-Specific Fields","text":"<pre><code>CURRENCY_AMOUNT = Field(\n    name=\"amount\",\n    annotation=Decimal,\n    validator=lambda cls, v: validate_positive_decimal(v),\n    description=\"Positive currency amount\",\n    json_schema_extra={\"format\": \"decimal\", \"multipleOf\": 0.01}\n)\n\nPERCENTAGE = Field(\n    name=\"percentage\",\n    annotation=float,\n    validator=lambda cls, v: max(0.0, min(100.0, float(v))),\n    json_schema_extra={\"minimum\": 0, \"maximum\": 100}\n)\n</code></pre>"},{"location":"guides/fields/#field-families","title":"Field Families","text":"<pre><code># Base field\nEMAIL_BASE = Field(name=\"email\", annotation=str, validator=validate_email)\n\n# Variations\nEMAIL_REQUIRED = EMAIL_BASE\nEMAIL_OPTIONAL = EMAIL_BASE.as_nullable()\nEMAIL_LIST = EMAIL_BASE.as_listable()\n</code></pre>"},{"location":"guides/fields/#adapter-integration","title":"Adapter Integration","text":""},{"location":"guides/fields/#metadata-for-database-adapters","title":"Metadata for Database Adapters","text":"<pre><code>VECTOR_FIELD = Field(\n    name=\"embedding\",\n    annotation=list[float],\n    json_schema_extra={\n        \"vector_dim\": 768,\n        \"distance_metric\": \"cosine\",\n        \"db_index_type\": \"hnsw\"\n    }\n)\n\nUSERNAME_FIELD = Field(\n    name=\"username\",\n    annotation=str,\n    json_schema_extra={\n        \"db_index\": True,\n        \"db_unique\": True,\n        \"db_column\": \"user_name\"\n    }\n)\n</code></pre>"},{"location":"guides/fields/#accessing-field-metadata","title":"Accessing Field Metadata","text":"<pre><code>def create_table_schema(model_class):\n    for field_name, field_info in model_class.model_fields.items():\n        extra = field_info.json_schema_extra or {}\n\n        if extra.get(\"db_index\"):\n            print(f\"CREATE INDEX ON {field_name}\")\n        if extra.get(\"vector_dim\"):\n            print(f\"CREATE VECTOR COLUMN dimension={extra['vector_dim']}\")\n</code></pre>"},{"location":"guides/fields/#dynamic-field-creation","title":"Dynamic Field Creation","text":"<pre><code>def create_string_field(name: str, max_length: int = None):\n    \"\"\"Factory for string fields with optional length validation\"\"\"\n    validator = lambda cls, v: v[:max_length] if max_length and v else v\n\n    return Field(\n        name=name,\n        annotation=str,\n        validator=validator if max_length else None,\n        json_schema_extra={\"maxLength\": max_length} if max_length else {}\n    )\n\n# Usage\ntitle_field = create_string_field(\"title\", max_length=100)\ndescription_field = create_string_field(\"description\", max_length=500)\n</code></pre>"},{"location":"guides/fields/#key-tips-for-llm-developers","title":"Key Tips for LLM Developers","text":""},{"location":"guides/fields/#1-field-reuse-strategy","title":"1. Field Reuse Strategy","text":"<pre><code># Create fields module for your domain\n# fields.py\nUSER_ID = ID_FROZEN.copy(name=\"user_id\")\nCREATED_AT = DATETIME.copy(name=\"created_at\")\nPRICE = Field(name=\"price\", annotation=Decimal, validator=validate_positive)\n\n# Use across models\nclass User(BaseModel):\n    id: UUID = USER_ID.field_info\n    created_at: datetime = CREATED_AT.field_info\n\nclass Order(BaseModel):\n    id: UUID = ID_FROZEN.field_info  # or create ORDER_ID variant\n    user_id: UUID = USER_ID.field_info\n    total: Decimal = PRICE.field_info\n</code></pre>"},{"location":"guides/fields/#2-composition-over-duplication","title":"2. Composition Over Duplication","text":"<pre><code># Base field\nBASE_TEXT = Field(name=\"text\", annotation=str, validator=lambda cls, v: v.strip())\n\n# Composed variations\nTITLE = BASE_TEXT.copy(name=\"title\", description=\"Title text\")\nOPTIONAL_DESCRIPTION = BASE_TEXT.copy(name=\"description\").as_nullable()\nTAG_LIST = BASE_TEXT.as_listable(strict=True)\n</code></pre>"},{"location":"guides/fields/#3-validation-patterns","title":"3. Validation Patterns","text":"<pre><code>def validate_email(cls, v):\n    if v and \"@\" not in v:\n        raise ValueError(\"Invalid email format\")\n    return v\n\ndef validate_positive(cls, v):\n    if v is not None and v &lt; 0:\n        raise ValueError(\"Must be positive\")\n    return v\n</code></pre>"},{"location":"guides/fields/#4-testing-field-behavior","title":"4. Testing Field Behavior","text":"<pre><code>def test_field_composition():\n    base = Field(name=\"test\", annotation=str)\n    nullable = base.as_nullable()\n    listable = base.as_listable()\n\n    assert nullable.annotation != base.annotation  # Should be Union[str, None]\n    assert base.name == nullable.name == listable.name\n</code></pre>"},{"location":"guides/fields/#5-common-caveats","title":"5. Common Caveats","text":"<ul> <li>Field copying: Use <code>.copy()</code> to avoid modifying original fields</li> <li>Validator scope: Validators receive <code>(cls, value)</code>, not just <code>value</code></li> <li>Composition order: <code>as_nullable().as_listable()</code> vs   <code>as_listable().as_nullable()</code></li> <li>Metadata inheritance: <code>json_schema_extra</code> is preserved through composition</li> </ul>"},{"location":"guides/fields/#6-integration-with-protocols","title":"6. Integration with Protocols","text":"<pre><code>class StandardEntity(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID = ID_FROZEN.field_info\n    created_at: datetime = DATETIME.field_info\n    updated_at: datetime = DATETIME.field_info\n</code></pre> <p>The field system provides a powerful foundation for creating reusable, validated field definitions that integrate seamlessly with pydapter's adapter ecosystem.</p>"},{"location":"guides/protocols/","title":"Working with Pydapter Protocols","text":""},{"location":"guides/protocols/#protocol-mixin-pattern","title":"Protocol + Mixin Pattern","text":"<p>Each protocol provides:</p> <ol> <li>Protocol: Interface for type checking</li> <li>Mixin: Implementation with behavior</li> </ol> <pre><code>@runtime_checkable\nclass Identifiable(Protocol):\n    id: UUID\n\nclass IdentifiableMixin:\n    def __hash__(self) -&gt; int:\n        return hash(self.id)\n</code></pre>"},{"location":"guides/protocols/#available-protocols","title":"Available Protocols","text":""},{"location":"guides/protocols/#identifiable","title":"Identifiable","text":"<ul> <li>Purpose: UUID-based identity management</li> <li>Fields: <code>id: UUID</code></li> <li>Methods: <code>__hash__()</code>, UUID serialization</li> <li>Usage: Base for all tracked entities</li> </ul>"},{"location":"guides/protocols/#temporal","title":"Temporal","text":"<ul> <li>Purpose: Timestamp management</li> <li>Fields: <code>created_at: datetime</code>, <code>updated_at: datetime</code></li> <li>Methods: <code>update_timestamp()</code>, ISO datetime serialization</li> <li>Usage: Audit trails, versioning</li> </ul>"},{"location":"guides/protocols/#embeddable","title":"Embeddable","text":"<ul> <li>Purpose: Vector embeddings for ML/AI</li> <li>Fields: <code>content: str | None</code>, <code>embedding: list[float]</code></li> <li>Methods: Content processing, embedding validation</li> <li>Usage: RAG systems, semantic search</li> </ul>"},{"location":"guides/protocols/#event","title":"Event","text":"<ul> <li>Purpose: Comprehensive event tracking</li> <li>Inherits: Combines Identifiable, Temporal, Embeddable</li> <li>Usage: Event sourcing, audit logs</li> </ul>"},{"location":"guides/protocols/#composition-patterns","title":"Composition Patterns","text":""},{"location":"guides/protocols/#basic-composition","title":"Basic Composition","text":"<pre><code>class User(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID\n    created_at: datetime\n    updated_at: datetime\n    name: str\n    email: str\n</code></pre>"},{"location":"guides/protocols/#inheritance-order-matters","title":"Inheritance Order Matters","text":"<pre><code># \u2713 Correct: BaseModel first, dependency order for mixins\nclass Document(BaseModel, IdentifiableMixin, TemporalMixin, EmbeddableMixin):\n    # Protocol fields first\n    id: UUID\n    created_at: datetime\n    updated_at: datetime\n    content: str | None = None\n    embedding: list[float] = Field(default_factory=list)\n\n    # Domain fields\n    title: str\n</code></pre>"},{"location":"guides/protocols/#custom-protocol-creation","title":"Custom Protocol Creation","text":"<pre><code>@runtime_checkable\nclass Versionable(Protocol):\n    version: int\n    version_history: list[int]\n\nclass VersionableMixin:\n    def increment_version(self) -&gt; None:\n        if hasattr(self, 'version'):\n            self.version_history.append(self.version)\n            self.version += 1\n</code></pre>"},{"location":"guides/protocols/#type-checking","title":"Type Checking","text":""},{"location":"guides/protocols/#static-type-checking","title":"Static Type Checking","text":"<pre><code>def process_identifiable_items(items: list[Identifiable]) -&gt; list[UUID]:\n    return [item.id for item in items]\n</code></pre>"},{"location":"guides/protocols/#runtime-type-checking","title":"Runtime Type Checking","text":"<pre><code>def safe_get_id(obj: object) -&gt; UUID | None:\n    if isinstance(obj, Identifiable):\n        return obj.id\n    return None\n</code></pre>"},{"location":"guides/protocols/#integration-with-fields","title":"Integration with Fields","text":"<pre><code>from pydapter.fields import ID_FROZEN, DATETIME\n\nclass AdvancedModel(BaseModel, IdentifiableMixin, TemporalMixin):\n    id: UUID = ID_FROZEN.field_info\n    created_at: datetime = DATETIME.field_info\n    updated_at: datetime = DATETIME.field_info\n</code></pre>"},{"location":"guides/protocols/#key-tips-for-llm-developers","title":"Key Tips for LLM Developers","text":""},{"location":"guides/protocols/#1-protocol-contract-compliance","title":"1. Protocol Contract Compliance","text":"<ul> <li>Always implement all required protocol fields</li> <li>Use proper type annotations</li> <li>Test protocol compliance with <code>isinstance()</code></li> </ul>"},{"location":"guides/protocols/#2-mixin-order","title":"2. Mixin Order","text":"<ul> <li><code>BaseModel</code> first</li> <li>Protocol mixins in dependency order</li> <li>Custom mixins last</li> </ul>"},{"location":"guides/protocols/#3-automatic-serialization","title":"3. Automatic Serialization","text":"<ul> <li><code>IdentifiableMixin</code>: UUID \u2192 string</li> <li><code>TemporalMixin</code>: datetime \u2192 ISO string</li> <li>Use <code>model_dump_json()</code> for proper serialization</li> </ul>"},{"location":"guides/protocols/#4-common-patterns","title":"4. Common Patterns","text":"<pre><code># Standard composition for entities\nclass Entity(BaseModel, IdentifiableMixin, TemporalMixin):\n    pass\n\n# Standard composition for ML content\nclass MLContent(BaseModel, IdentifiableMixin, EmbeddableMixin):\n    pass\n\n# Standard composition for events\nclass EventRecord(BaseModel, Event):  # Event includes all protocols\n    pass\n</code></pre>"},{"location":"guides/protocols/#5-testing-protocol-implementation","title":"5. Testing Protocol Implementation","text":"<pre><code>def test_protocol_compliance(model_instance):\n    assert isinstance(model_instance, Identifiable)\n    assert hasattr(model_instance, 'id')\n    assert callable(getattr(model_instance, '__hash__'))\n</code></pre> <p>This protocol system enables consistent, type-safe behavior composition across your models while maintaining clean separation of concerns.</p>"},{"location":"guides/testing-strategies/","title":"Testing Strategies for Pydapter","text":""},{"location":"guides/testing-strategies/#protocol-testing","title":"Protocol Testing","text":""},{"location":"guides/testing-strategies/#protocol-compliance","title":"Protocol Compliance","text":"<pre><code>from pydapter.protocols import Identifiable, Temporal\n\ndef test_protocol_compliance():\n    model = MyModel(id=uuid4(), created_at=datetime.now(), updated_at=datetime.now())\n\n    # Runtime protocol checks\n    assert isinstance(model, Identifiable)\n    assert isinstance(model, Temporal)\n\n    # Test mixin functionality\n    original_updated = model.updated_at\n    model.update_timestamp()\n    assert model.updated_at &gt; original_updated\n</code></pre>"},{"location":"guides/testing-strategies/#adapter-testing","title":"Adapter Testing","text":""},{"location":"guides/testing-strategies/#roundtrip-testing","title":"Roundtrip Testing","text":"<pre><code>def test_adapter_roundtrip():\n    \"\"\"Test data survives roundtrip conversion\"\"\"\n    original = MyModel(name=\"test\", value=42)\n\n    external = MyAdapter.to_obj(original)\n    restored = MyAdapter.from_obj(MyModel, external)\n\n    assert restored.name == original.name\n    assert restored.value == original.value\n</code></pre>"},{"location":"guides/testing-strategies/#error-handling","title":"Error Handling","text":"<pre><code>def test_adapter_error_handling():\n    \"\"\"Test error scenarios\"\"\"\n    with pytest.raises(ParseError, match=\"Invalid format\"):\n        MyAdapter.from_obj(MyModel, \"invalid_data\")\n\n    with pytest.raises(ValidationError):\n        MyAdapter.from_obj(MyModel, {\"missing\": \"required_fields\"})\n</code></pre>"},{"location":"guides/testing-strategies/#async-testing","title":"Async Testing","text":"<pre><code>@pytest.mark.asyncio\nasync def test_async_adapter(respx_mock):\n    \"\"\"Test async adapters with mocked HTTP\"\"\"\n    respx_mock.get(\"http://api.example.com/data\").mock(\n        return_value=httpx.Response(200, json={\"name\": \"test\"})\n    )\n\n    result = await MyAsyncAdapter.from_obj(MyModel, {\"url\": \"http://api.example.com/data\"})\n    assert result.name == \"test\"\n</code></pre>"},{"location":"guides/testing-strategies/#registry-testing","title":"Registry Testing","text":"<pre><code>def test_registry_operations():\n    \"\"\"Test adapter registry functionality\"\"\"\n    registry = AdapterRegistry()\n    registry.register(MyAdapter)\n\n    # Test retrieval\n    adapter = registry.get(\"my_adapter\")\n    assert adapter == MyAdapter\n\n    # Test missing adapter\n    with pytest.raises(AdapterNotFoundError):\n        registry.get(\"nonexistent\")\n</code></pre>"},{"location":"guides/testing-strategies/#property-based-testing","title":"Property-Based Testing","text":"<pre><code>from hypothesis import given, strategies as st\n\n@given(st.text(min_size=1))\ndef test_field_validation_robustness(text_value):\n    \"\"\"Test field validators with random data\"\"\"\n    field = Field(name=\"test\", validator=lambda cls, v: v.strip())\n    # Test edge cases with generated data\n</code></pre>"},{"location":"guides/testing-strategies/#key-testing-patterns-for-llm-developers","title":"Key Testing Patterns for LLM Developers","text":""},{"location":"guides/testing-strategies/#1-test-fixtures","title":"1. Test Fixtures","text":"<pre><code>@pytest.fixture\ndef sample_user():\n    return User(\n        id=uuid4(),\n        created_at=datetime.now(),\n        updated_at=datetime.now(),\n        name=\"Test User\",\n        email=\"test@example.com\"\n    )\n\n@pytest.fixture\ndef user_registry():\n    registry = AdapterRegistry()\n    registry.register(JsonAdapter)\n    registry.register(CsvAdapter)\n    return registry\n</code></pre>"},{"location":"guides/testing-strategies/#2-mock-external-dependencies","title":"2. Mock External Dependencies","text":"<pre><code># HTTP APIs\n@pytest.fixture\ndef mock_api(respx_mock):\n    respx_mock.get(\"http://api.example.com/users\").mock(\n        return_value=httpx.Response(200, json=[{\"name\": \"John\", \"age\": 30}])\n    )\n\n# Database connections\n@pytest.fixture\ndef mock_db():\n    with patch('asyncpg.connect') as mock_connect:\n        mock_conn = AsyncMock()\n        mock_connect.return_value = mock_conn\n        yield mock_conn\n</code></pre>"},{"location":"guides/testing-strategies/#3-error-path-testing","title":"3. Error Path Testing","text":"<pre><code>def test_all_error_scenarios():\n    \"\"\"Comprehensive error testing\"\"\"\n    # Empty input\n    with pytest.raises(ParseError, match=\"Empty.*content\"):\n        MyAdapter.from_obj(MyModel, \"\")\n\n    # Invalid format\n    with pytest.raises(ParseError, match=\"Invalid.*format\"):\n        MyAdapter.from_obj(MyModel, \"invalid_format\")\n\n    # Validation failure\n    with pytest.raises(ValidationError):\n        MyAdapter.from_obj(MyModel, {\"missing_required_field\": True})\n</code></pre>"},{"location":"guides/testing-strategies/#4-async-testing-patterns","title":"4. Async Testing Patterns","text":"<pre><code>@pytest.mark.asyncio\nclass TestAsyncOperations:\n    async def test_concurrent_operations(self):\n        \"\"\"Test concurrent adapter operations\"\"\"\n        tasks = [\n            MyAsyncAdapter.from_obj(MyModel, {\"id\": i})\n            for i in range(10)\n        ]\n        results = await asyncio.gather(*tasks)\n        assert len(results) == 10\n\n    async def test_timeout_handling(self):\n        \"\"\"Test timeout scenarios\"\"\"\n        with pytest.raises(ParseError, match=\"timed out\"):\n            await MyAsyncAdapter.from_obj(MyModel, config, timeout=0.01)\n</code></pre>"},{"location":"guides/testing-strategies/#common-testing-caveats","title":"Common Testing Caveats","text":""},{"location":"guides/testing-strategies/#1-async-context","title":"1. Async Context","text":"<ul> <li>Use <code>pytest.mark.asyncio</code> for async tests</li> <li>Mock external services (HTTP, database)</li> <li>Test timeout and retry logic</li> </ul>"},{"location":"guides/testing-strategies/#2-protocol-mixins","title":"2. Protocol Mixins","text":"<ul> <li>Test both interface and implementation</li> <li>Verify field serializers</li> <li>Check inheritance order effects</li> </ul>"},{"location":"guides/testing-strategies/#3-registry-isolation","title":"3. Registry Isolation","text":"<ul> <li>Use fresh registries per test</li> <li>Clean up registered adapters</li> <li>Test adapter precedence</li> </ul>"},{"location":"guides/testing-strategies/#4-error-context","title":"4. Error Context","text":"<ul> <li>Verify specific exception types</li> <li>Check error message content</li> <li>Test error data preservation</li> </ul>"},{"location":"guides/testing-strategies/#testing-tips","title":"Testing Tips","text":"<ul> <li>Fixtures: Use pytest fixtures for common setups</li> <li>Mocking: Mock external dependencies consistently</li> <li>Error paths: Test failures as thoroughly as success</li> <li>Property-based: Use Hypothesis for edge case discovery</li> <li>Type safety: Run mypy in CI to catch type errors</li> <li>Isolation: Ensure tests don't affect each other</li> </ul>"},{"location":"tutorials/using_migrations/","title":"Tutorial: Managing Database Schema Evolution with Migrations","text":"<p>This tutorial demonstrates how to use pydapter's migrations module to manage database schema changes in a SQLAlchemy-based application. We'll create a simple user management system and evolve its schema over time using migrations.</p>"},{"location":"tutorials/using_migrations/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have installed pydapter with the migrations-sql extension:</p> <pre><code>pip install pydapter[migrations-sql]\n</code></pre> <p>This will install the required dependencies, including SQLAlchemy and Alembic.</p>"},{"location":"tutorials/using_migrations/#step-1-set-up-the-project-structure","title":"Step 1: Set Up the Project Structure","text":"<p>First, let's create a basic project structure:</p> <pre><code>user_management/\n\u251c\u2500\u2500 migrations/        # Will be created by the migration tool\n\u251c\u2500\u2500 models.py          # SQLAlchemy models\n\u251c\u2500\u2500 database.py        # Database connection setup\n\u2514\u2500\u2500 main.py            # Application entry point\n</code></pre>"},{"location":"tutorials/using_migrations/#step-2-define-the-initial-database-models","title":"Step 2: Define the Initial Database Models","text":"<p>Let's create our initial database models in <code>models.py</code>:</p> <pre><code># models.py\nfrom sqlalchemy import Column, Integer, String, DateTime, func\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = \"users\"\n\n    id = Column(Integer, primary_key=True)\n    username = Column(String(50), unique=True, nullable=False)\n    email = Column(String(100), unique=True, nullable=False)\n    created_at = Column(DateTime, default=func.now())\n</code></pre> <p>Next, let's set up the database connection in <code>database.py</code>:</p> <pre><code># database.py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\n# Use SQLite for simplicity in this tutorial\nDATABASE_URL = \"sqlite:///./user_management.db\"\n\nengine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n</code></pre>"},{"location":"tutorials/using_migrations/#step-3-initialize-migrations","title":"Step 3: Initialize Migrations","text":"<p>Now, let's initialize the migrations environment. Create a simple script in <code>main.py</code>:</p> <pre><code># main.py\nimport os\nfrom pydapter.migrations import AlembicAdapter\nimport models\n\ndef init_migrations():\n    \"\"\"Initialize the migrations environment.\"\"\"\n    os.makedirs(\"migrations\", exist_ok=True)\n\n    AlembicAdapter.init_migrations(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\",\n        models_module=models\n    )\n    print(\"Migrations initialized successfully!\")\n\nif __name__ == \"__main__\":\n    init_migrations()\n</code></pre> <p>Run this script to initialize the migrations environment:</p> <pre><code>python main.py\n</code></pre> <p>This will create the necessary directory structure and configuration files for Alembic in the <code>migrations</code> directory.</p>"},{"location":"tutorials/using_migrations/#step-4-create-the-initial-migration","title":"Step 4: Create the Initial Migration","text":"<p>Now, let's create our first migration to set up the initial database schema:</p> <pre><code># main.py (add this function)\ndef create_initial_migration():\n    \"\"\"Create the initial migration.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Create users table\",\n        autogenerate=True,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created initial migration: {revision}\")\n\nif __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    create_initial_migration()\n</code></pre> <p>Run the script again to create the initial migration:</p> <pre><code>python main.py\n</code></pre> <p>This will create a new migration file in the <code>migrations/versions/</code> directory with a unique revision ID.</p>"},{"location":"tutorials/using_migrations/#step-5-apply-the-migration","title":"Step 5: Apply the Migration","text":"<p>Now, let's apply the migration to create the database schema:</p> <pre><code># main.py (add this function)\ndef apply_migrations():\n    \"\"\"Apply all pending migrations.\"\"\"\n    AlembicAdapter.upgrade(\n        revision=\"head\",\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(\"Migrations applied successfully!\")\n\nif __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    apply_migrations()\n</code></pre> <p>Run the script to apply the migration:</p> <pre><code>python main.py\n</code></pre> <p>This will create the <code>users</code> table in the database according to our model definition.</p>"},{"location":"tutorials/using_migrations/#step-6-evolve-the-schema","title":"Step 6: Evolve the Schema","text":"<p>Now, let's evolve our schema by adding new fields to the <code>User</code> model:</p> <pre><code># models.py (updated)\nfrom sqlalchemy import Column, Integer, String, DateTime, Boolean, func\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = \"users\"\n\n    id = Column(Integer, primary_key=True)\n    username = Column(String(50), unique=True, nullable=False)\n    email = Column(String(100), unique=True, nullable=False)\n    full_name = Column(String(100))  # New field\n    is_active = Column(Boolean, default=True)  # New field\n    created_at = Column(DateTime, default=func.now())\n    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())  # New field\n</code></pre> <p>Now, let's create a new migration to reflect these changes:</p> <pre><code># main.py (add this function)\ndef create_schema_update_migration():\n    \"\"\"Create a migration for schema updates.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Add user profile fields\",\n        autogenerate=True,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created schema update migration: {revision}\")\n\nif __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # apply_migrations()  # Comment this out after first run\n    create_schema_update_migration()\n</code></pre> <p>Run the script to create the new migration:</p> <pre><code>python main.py\n</code></pre> <p>Then apply the migration:</p> <pre><code>if __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # create_schema_update_migration()  # Comment this out after first run\n    apply_migrations()\n</code></pre> <pre><code>python main.py\n</code></pre>"},{"location":"tutorials/using_migrations/#step-7-add-a-new-model","title":"Step 7: Add a New Model","text":"<p>Let's add a new model to our application for user roles:</p> <pre><code># models.py (updated)\nfrom sqlalchemy import Column, Integer, String, DateTime, Boolean, ForeignKey, func\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = \"users\"\n\n    id = Column(Integer, primary_key=True)\n    username = Column(String(50), unique=True, nullable=False)\n    email = Column(String(100), unique=True, nullable=False)\n    full_name = Column(String(100))\n    is_active = Column(Boolean, default=True)\n    created_at = Column(DateTime, default=func.now())\n    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())\n\n    # Add relationship to roles\n    roles = relationship(\"UserRole\", back_populates=\"user\")\n\n\nclass UserRole(Base):\n    __tablename__ = \"user_roles\"\n\n    id = Column(Integer, primary_key=True)\n    user_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n    role_name = Column(String(50), nullable=False)\n    created_at = Column(DateTime, default=func.now())\n\n    # Add relationship to user\n    user = relationship(\"User\", back_populates=\"roles\")\n</code></pre> <p>Now, let's create a migration for this new model:</p> <pre><code># main.py (add this function)\ndef create_roles_migration():\n    \"\"\"Create a migration for the roles model.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Add user roles table\",\n        autogenerate=True,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created roles migration: {revision}\")\n\nif __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # apply_migrations()  # Comment this out after first run\n    # create_schema_update_migration()  # Comment this out after first run\n    create_roles_migration()\n</code></pre> <p>Run the script to create the new migration:</p> <pre><code>python main.py\n</code></pre> <p>Then apply the migration:</p> <pre><code>if __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # create_schema_update_migration()  # Comment this out after first run\n    # create_roles_migration()  # Comment this out after first run\n    apply_migrations()\n</code></pre> <pre><code>python main.py\n</code></pre>"},{"location":"tutorials/using_migrations/#step-8-check-migration-status","title":"Step 8: Check Migration Status","text":"<p>Let's add functionality to check the current migration status:</p> <pre><code># main.py (add this function)\ndef check_migration_status():\n    \"\"\"Check the current migration status.\"\"\"\n    current = AlembicAdapter.get_current_revision(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Current migration revision: {current}\")\n\n    history = AlembicAdapter.get_migration_history(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(\"\\nMigration history:\")\n    for migration in history:\n        print(f\"- {migration['revision']}: {migration.get('description', 'No description')}\")\n\nif __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # apply_migrations()  # Comment this out after first run\n    # create_schema_update_migration()  # Comment this out after first run\n    # create_roles_migration()  # Comment this out after first run\n    check_migration_status()\n</code></pre> <p>Run the script to check the migration status:</p> <pre><code>python main.py\n</code></pre>"},{"location":"tutorials/using_migrations/#step-9-downgrade-to-a-previous-version","title":"Step 9: Downgrade to a Previous Version","text":"<p>Sometimes you might need to revert to a previous version of your schema. Let's add functionality to downgrade:</p> <pre><code># main.py (add this function)\ndef downgrade_migration(revision):\n    \"\"\"Downgrade to a specific revision.\"\"\"\n    AlembicAdapter.downgrade(\n        revision=revision,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Downgraded to revision: {revision}\")\n\nif __name__ == \"__main__\":\n    # Get the second-to-last revision from history\n    history = AlembicAdapter.get_migration_history(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    if len(history) &gt;= 2:\n        previous_revision = history[-2]['revision']\n        downgrade_migration(previous_revision)\n    else:\n        print(\"Not enough migrations to downgrade\")\n</code></pre> <p>Run the script to downgrade to the previous migration:</p> <pre><code>python main.py\n</code></pre>"},{"location":"tutorials/using_migrations/#step-10-create-a-custom-migration","title":"Step 10: Create a Custom Migration","text":"<p>Sometimes you need to create custom migrations that aren't just schema changes. Let's create a data migration:</p> <pre><code># main.py (add this function)\ndef create_custom_migration():\n    \"\"\"Create a custom data migration.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Add default admin user\",\n        autogenerate=False,  # Don't auto-generate from models\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created custom migration: {revision}\")\n    print(f\"Edit the migration file in migrations/versions/{revision}_add_default_admin_user.py\")\n\nif __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # apply_migrations()  # Comment this out after first run\n    # create_schema_update_migration()  # Comment this out after first run\n    # create_roles_migration()  # Comment this out after first run\n    # check_migration_status()  # Comment this out after first run\n    create_custom_migration()\n</code></pre> <p>Run the script to create the custom migration:</p> <pre><code>python main.py\n</code></pre> <p>Now, edit the generated migration file in <code>migrations/versions/</code> to add custom SQL operations:</p> <pre><code>\"\"\"Add default admin user\n\nRevision ID: &lt;your_revision_id&gt;\nRevises: &lt;previous_revision_id&gt;\nCreate Date: 2025-05-16 12:00:00.000000\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n# revision identifiers\nrevision = '&lt;your_revision_id&gt;'\ndown_revision = '&lt;previous_revision_id&gt;'\nbranch_labels = None\ndepends_on = None\n\ndef upgrade():\n    # Add a default admin user\n    op.execute(\"\"\"\n    INSERT INTO users (username, email, full_name, is_active)\n    VALUES ('admin', 'admin@example.com', 'System Administrator', 1)\n    \"\"\")\n\n    # Get the user ID\n    conn = op.get_bind()\n    result = conn.execute(\"SELECT id FROM users WHERE username = 'admin'\").fetchone()\n    if result:\n        user_id = result[0]\n        # Add admin role\n        op.execute(f\"\"\"\n        INSERT INTO user_roles (user_id, role_name)\n        VALUES ({user_id}, 'admin')\n        \"\"\")\n\ndef downgrade():\n    # Remove the admin role and user\n    op.execute(\"DELETE FROM user_roles WHERE role_name = 'admin'\")\n    op.execute(\"DELETE FROM users WHERE username = 'admin'\")\n</code></pre> <p>Then apply the migration:</p> <pre><code>if __name__ == \"__main__\":\n    # init_migrations()  # Comment this out after first run\n    # create_initial_migration()  # Comment this out after first run\n    # create_schema_update_migration()  # Comment this out after first run\n    # create_roles_migration()  # Comment this out after first run\n    # check_migration_status()  # Comment this out after first run\n    # create_custom_migration()  # Comment this out after first run\n    apply_migrations()\n</code></pre> <pre><code>python main.py\n</code></pre>"},{"location":"tutorials/using_migrations/#step-11-using-async-migrations","title":"Step 11: Using Async Migrations","text":"<p>If your application uses asynchronous database connections, you can use the async migration adapter. Let's modify our code to use async migrations:</p> <pre><code># database.py (updated for async)\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.orm import sessionmaker\n\n# Use SQLite for simplicity in this tutorial\nDATABASE_URL = \"sqlite+aiosqlite:///./user_management_async.db\"\n\nengine = create_async_engine(DATABASE_URL)\nAsyncSessionLocal = sessionmaker(\n    class_=AsyncSession,\n    autocommit=False,\n    autoflush=False,\n    bind=engine\n)\n\nasync def get_db():\n    async with AsyncSessionLocal() as db:\n        yield db\n</code></pre> <pre><code># main_async.py\nimport asyncio\nimport os\nfrom pydapter.migrations import AsyncAlembicAdapter\nimport models\n\nasync def init_migrations():\n    \"\"\"Initialize the migrations environment.\"\"\"\n    os.makedirs(\"migrations_async\", exist_ok=True)\n\n    await AsyncAlembicAdapter.init_migrations(\n        directory=\"migrations_async\",\n        connection_string=\"sqlite+aiosqlite:///./user_management_async.db\",\n        models_module=models\n    )\n    print(\"Async migrations initialized successfully!\")\n\nasync def create_initial_migration():\n    \"\"\"Create the initial migration.\"\"\"\n    revision = await AsyncAlembicAdapter.create_migration(\n        message=\"Create users table\",\n        autogenerate=True,\n        directory=\"migrations_async\",\n        connection_string=\"sqlite+aiosqlite:///./user_management_async.db\"\n    )\n    print(f\"Created initial async migration: {revision}\")\n\nasync def apply_migrations():\n    \"\"\"Apply all pending migrations.\"\"\"\n    await AsyncAlembicAdapter.upgrade(\n        revision=\"head\",\n        directory=\"migrations_async\",\n        connection_string=\"sqlite+aiosqlite:///./user_management_async.db\"\n    )\n    print(\"Async migrations applied successfully!\")\n\nasync def main():\n    await init_migrations()\n    await create_initial_migration()\n    await apply_migrations()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Run the async script:</p> <pre><code>python main_async.py\n</code></pre>"},{"location":"tutorials/using_migrations/#complete-example","title":"Complete Example","text":"<p>Here's a complete example of the main.py file that includes all the migration operations:</p> <pre><code># main.py\nimport os\nfrom pydapter.migrations import AlembicAdapter\nimport models\n\ndef init_migrations():\n    \"\"\"Initialize the migrations environment.\"\"\"\n    os.makedirs(\"migrations\", exist_ok=True)\n\n    AlembicAdapter.init_migrations(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\",\n        models_module=models\n    )\n    print(\"Migrations initialized successfully!\")\n\ndef create_initial_migration():\n    \"\"\"Create the initial migration.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Create users table\",\n        autogenerate=True,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created initial migration: {revision}\")\n\ndef apply_migrations():\n    \"\"\"Apply all pending migrations.\"\"\"\n    AlembicAdapter.upgrade(\n        revision=\"head\",\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(\"Migrations applied successfully!\")\n\ndef create_schema_update_migration():\n    \"\"\"Create a migration for schema updates.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Add user profile fields\",\n        autogenerate=True,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created schema update migration: {revision}\")\n\ndef create_roles_migration():\n    \"\"\"Create a migration for the roles model.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Add user roles table\",\n        autogenerate=True,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created roles migration: {revision}\")\n\ndef check_migration_status():\n    \"\"\"Check the current migration status.\"\"\"\n    current = AlembicAdapter.get_current_revision(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Current migration revision: {current}\")\n\n    history = AlembicAdapter.get_migration_history(\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(\"\\nMigration history:\")\n    for migration in history:\n        print(f\"- {migration['revision']}: {migration.get('description', 'No description')}\")\n\ndef downgrade_migration(revision):\n    \"\"\"Downgrade to a specific revision.\"\"\"\n    AlembicAdapter.downgrade(\n        revision=revision,\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Downgraded to revision: {revision}\")\n\ndef create_custom_migration():\n    \"\"\"Create a custom data migration.\"\"\"\n    revision = AlembicAdapter.create_migration(\n        message=\"Add default admin user\",\n        autogenerate=False,  # Don't auto-generate from models\n        directory=\"migrations\",\n        connection_string=\"sqlite:///./user_management.db\"\n    )\n    print(f\"Created custom migration: {revision}\")\n    print(f\"Edit the migration file in migrations/versions/{revision}_add_default_admin_user.py\")\n\nif __name__ == \"__main__\":\n    # Uncomment the function you want to run\n    # init_migrations()\n    # create_initial_migration()\n    # apply_migrations()\n    # create_schema_update_migration()\n    # create_roles_migration()\n    # check_migration_status()\n\n    # Downgrade example\n    # history = AlembicAdapter.get_migration_history(\n    #     directory=\"migrations\",\n    #     connection_string=\"sqlite:///./user_management.db\"\n    # )\n    # if len(history) &gt;= 2:\n    #     previous_revision = history[-2]['revision']\n    #     downgrade_migration(previous_revision)\n    # else:\n    #     print(\"Not enough migrations to downgrade\")\n\n    # create_custom_migration()\n\n    # Final upgrade to latest\n    apply_migrations()\n</code></pre>"},{"location":"tutorials/using_migrations/#summary","title":"Summary","text":"<p>In this tutorial, we've demonstrated how to use pydapter's migrations module to manage database schema evolution. We've covered:</p> <ol> <li>Setting up a migrations environment</li> <li>Creating and applying initial migrations</li> <li>Evolving the schema with new fields</li> <li>Adding new models</li> <li>Checking migration status</li> <li>Downgrading to previous versions</li> <li>Creating custom data migrations</li> <li>Using async migrations</li> </ol> <p>The migrations module provides a powerful way to manage database schema changes in a controlled, versioned manner, making it easier to evolve your application's data model over time.</p>"},{"location":"tutorials/using_migrations/#best-practices","title":"Best Practices","text":"<p>Here are some best practices to follow when working with migrations:</p> <ol> <li>Always back up your database before applying migrations in production</li> <li>Keep migrations small and focused on specific changes</li> <li>Test migrations thoroughly in development before applying to production</li> <li>Include descriptive messages for each migration</li> <li>Use version control for your migration files</li> <li>Consider using separate migration environments for different deployment    stages</li> <li>Document complex migrations with comments</li> <li>Include both upgrade and downgrade operations when possible</li> </ol>"},{"location":"tutorials/using_protocols/","title":"Tutorial: Using Protocols to Create Standardized Models","text":"<p>This tutorial demonstrates how to use the pydapter protocols module to create models with standardized capabilities. We'll build a simple document management system that leverages the protocol interfaces to provide consistent behavior across different types of documents.</p>"},{"location":"tutorials/using_protocols/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have installed pydapter with the protocols extension:</p> <pre><code>pip install pydapter[protocols]\n</code></pre>"},{"location":"tutorials/using_protocols/#step-1-define-base-document-models","title":"Step 1: Define Base Document Models","text":"<p>First, let's define our base document models using the protocols:</p> <pre><code>from datetime import datetime\nfrom uuid import UUID\n\nfrom pydapter.protocols import Identifiable, Temporal, Embedable\n\nclass BaseDocument(Identifiable, Temporal):\n    \"\"\"Base document class with ID and timestamp tracking.\"\"\"\n\n    title: str\n    author: str\n\n    def __str__(self) -&gt; str:\n        return f\"{self.title} by {self.author}\"\n\n\nclass EmbeddableDocument(BaseDocument, Embedable):\n    \"\"\"Document that supports vector embeddings.\"\"\"\n\n    content: str\n\n    def create_content(self) -&gt; str:\n        \"\"\"Create content for embedding from document metadata and content.\"\"\"\n        return f\"{self.title}\\n{self.author}\\n{self.content}\"\n</code></pre>"},{"location":"tutorials/using_protocols/#step-2-create-specific-document-types","title":"Step 2: Create Specific Document Types","text":"<p>Now, let's create specific document types that inherit from our base classes:</p> <pre><code>class TextDocument(EmbeddableDocument):\n    \"\"\"A simple text document.\"\"\"\n\n    format: str = \"text\"\n\n\nclass PDFDocument(EmbeddableDocument):\n    \"\"\"A PDF document with additional metadata.\"\"\"\n\n    format: str = \"pdf\"\n    page_count: int\n\n\nclass ImageDocument(BaseDocument):\n    \"\"\"An image document that doesn't need text embedding.\"\"\"\n\n    format: str = \"image\"\n    width: int\n    height: int\n    file_path: str\n</code></pre>"},{"location":"tutorials/using_protocols/#step-3-create-a-document-repository","title":"Step 3: Create a Document Repository","text":"<p>Let's create a simple repository to manage our documents:</p> <pre><code>from typing import Dict, List, Optional, Type, TypeVar\n\nT = TypeVar('T', bound=BaseDocument)\n\nclass DocumentRepository:\n    \"\"\"Repository for managing documents.\"\"\"\n\n    def __init__(self):\n        self.documents: Dict[UUID, BaseDocument] = {}\n\n    def add(self, document: BaseDocument) -&gt; None:\n        \"\"\"Add a document to the repository.\"\"\"\n        self.documents[document.id] = document\n\n    def get(self, document_id: UUID) -&gt; Optional[BaseDocument]:\n        \"\"\"Get a document by ID.\"\"\"\n        return self.documents.get(document_id)\n\n    def list_all(self) -&gt; List[BaseDocument]:\n        \"\"\"List all documents.\"\"\"\n        return list(self.documents.values())\n\n    def find_by_type(self, doc_type: Type[T]) -&gt; List[T]:\n        \"\"\"Find documents by type.\"\"\"\n        return [doc for doc in self.documents.values() if isinstance(doc, doc_type)]\n\n    def find_by_author(self, author: str) -&gt; List[BaseDocument]:\n        \"\"\"Find documents by author.\"\"\"\n        return [doc for doc in self.documents.values() if doc.author == author]\n\n    def update(self, document: BaseDocument) -&gt; None:\n        \"\"\"Update a document.\"\"\"\n        if document.id in self.documents:\n            # Update the timestamp\n            document.update_timestamp()\n            self.documents[document.id] = document\n</code></pre>"},{"location":"tutorials/using_protocols/#step-4-working-with-documents","title":"Step 4: Working with Documents","text":"<p>Now let's use our document models and repository:</p> <pre><code># Create a repository\nrepo = DocumentRepository()\n\n# Create some documents\ntext_doc = TextDocument(\n    title=\"Getting Started with Protocols\",\n    author=\"Jane Smith\",\n    content=\"This document explains how to use protocols effectively.\"\n)\n\npdf_doc = PDFDocument(\n    title=\"Advanced Protocol Patterns\",\n    author=\"John Doe\",\n    content=\"Detailed explanation of advanced protocol usage patterns.\",\n    page_count=42\n)\n\nimage_doc = ImageDocument(\n    title=\"Protocol Architecture Diagram\",\n    author=\"Jane Smith\",\n    width=1920,\n    height=1080,\n    file_path=\"/images/protocol_diagram.png\"\n)\n\n# Add documents to the repository\nrepo.add(text_doc)\nrepo.add(pdf_doc)\nrepo.add(image_doc)\n\n# List all documents\nprint(\"All documents:\")\nfor doc in repo.list_all():\n    print(f\"- {doc}\")\n\n# Find documents by author\nprint(\"\\nDocuments by Jane Smith:\")\nfor doc in repo.find_by_author(\"Jane Smith\"):\n    print(f\"- {doc}\")\n\n# Find documents by type\nprint(\"\\nText documents:\")\nfor doc in repo.find_by_type(TextDocument):\n    print(f\"- {doc}\")\n\n# Update a document\ntext_doc.title = \"Updated: Getting Started with Protocols\"\nrepo.update(text_doc)\nprint(f\"\\nUpdated document timestamp: {text_doc.updated_at}\")\n</code></pre>"},{"location":"tutorials/using_protocols/#step-5-working-with-embeddings","title":"Step 5: Working with Embeddings","text":"<p>Let's extend our example to work with embeddings:</p> <pre><code>import numpy as np\nfrom typing import List, Tuple\n\ndef generate_mock_embedding(text: str) -&gt; List[float]:\n    \"\"\"Generate a mock embedding for demonstration purposes.\"\"\"\n    # In a real application, you would use a proper embedding model\n    # This is just a simple hash-based approach for demonstration\n    np_array = np.array([ord(c) for c in text], dtype=np.float32)\n    return (np_array / np.linalg.norm(np_array)).tolist()[:10]  # Normalize and take first 10 dims\n\ndef cosine_similarity(a: List[float], b: List[float]) -&gt; float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    dot_product = sum(x * y for x, y in zip(a, b))\n    norm_a = sum(x * x for x in a) ** 0.5\n    norm_b = sum(y * y for y in b) ** 0.5\n    return dot_product / (norm_a * norm_b)\n\n# Generate embeddings for our documents\nfor doc in repo.find_by_type(EmbeddableDocument):\n    content = doc.create_content()\n    doc.embedding = generate_mock_embedding(content)\n    print(f\"Generated embedding for '{doc.title}' with {len(doc.embedding)} dimensions\")\n\n# Find similar documents\ndef find_similar_documents(\n    query_doc: EmbeddableDocument,\n    candidates: List[EmbeddableDocument],\n    threshold: float = 0.7\n) -&gt; List[Tuple[EmbeddableDocument, float]]:\n    \"\"\"Find documents similar to the query document.\"\"\"\n    results = []\n    for doc in candidates:\n        if doc.id != query_doc.id:  # Skip the query document itself\n            similarity = cosine_similarity(query_doc.embedding, doc.embedding)\n            if similarity &gt;= threshold:\n                results.append((doc, similarity))\n    return sorted(results, key=lambda x: x[1], reverse=True)\n\n# Find documents similar to our text document\nembedable_docs = repo.find_by_type(EmbeddableDocument)\nsimilar_docs = find_similar_documents(text_doc, embedable_docs)\n\nprint(\"\\nDocuments similar to 'Getting Started with Protocols':\")\nfor doc, similarity in similar_docs:\n    print(f\"- {doc.title} (similarity: {similarity:.2f})\")\n</code></pre>"},{"location":"tutorials/using_protocols/#step-6-adding-event-tracking","title":"Step 6: Adding Event Tracking","text":"<p>Let's extend our system to track document events using the <code>Invokable</code> protocol:</p> <pre><code>import asyncio\nfrom pydapter.protocols import Invokable, Event\nfrom datetime import datetime\n\nclass DocumentEvent(Event):\n    \"\"\"Event for tracking document operations.\"\"\"\n\n    event_type: str\n    document_id: UUID\n    user_id: str\n\n    async def process(self):\n        \"\"\"Process the event.\"\"\"\n        # In a real application, this might log to a database or message queue\n        print(f\"Processing event: {self.event_type} for document {self.document_id}\")\n        return {\"processed\": True, \"timestamp\": datetime.now().isoformat()}\n\nasync def track_document_event(\n    event_type: str,\n    document: BaseDocument,\n    user_id: str\n) -&gt; DocumentEvent:\n    \"\"\"Track a document event.\"\"\"\n    event = DocumentEvent(\n        event_type=event_type,\n        document_id=document.id,\n        user_id=user_id,\n        content=f\"{event_type} operation on {document.title} by user {user_id}\"\n    )\n    event._invoke_function = event.process\n    await event.invoke()\n    return event\n\n# Example usage in an async context\nasync def main():\n    # Track a view event\n    view_event = await track_document_event(\"view\", text_doc, \"user123\")\n    print(f\"Event status: {view_event.execution.status}\")\n    print(f\"Event duration: {view_event.execution.duration:.6f} seconds\")\n    print(f\"Event response: {view_event.execution.response}\")\n\n    # Track an edit event\n    edit_event = await track_document_event(\"edit\", text_doc, \"user123\")\n    print(f\"Event status: {edit_event.execution.status}\")\n\n# Run the async example\nasyncio.run(main())\n</code></pre>"},{"location":"tutorials/using_protocols/#complete-example","title":"Complete Example","text":"<p>Here's the complete example combining all the steps:</p> <pre><code>import asyncio\nimport numpy as np\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Type, TypeVar\nfrom uuid import UUID\n\nfrom pydapter.protocols import Identifiable, Temporal, Embedable, Invokable, Event\n\n# Step 1: Define Base Document Models\nclass BaseDocument(Identifiable, Temporal):\n    \"\"\"Base document class with ID and timestamp tracking.\"\"\"\n\n    title: str\n    author: str\n\n    def __str__(self) -&gt; str:\n        return f\"{self.title} by {self.author}\"\n\n\nclass EmbeddableDocument(BaseDocument, Embedable):\n    \"\"\"Document that supports vector embeddings.\"\"\"\n\n    content: str\n\n    def create_content(self) -&gt; str:\n        \"\"\"Create content for embedding from document metadata and content.\"\"\"\n        return f\"{self.title}\\n{self.author}\\n{self.content}\"\n\n\n# Step 2: Create Specific Document Types\nclass TextDocument(EmbeddableDocument):\n    \"\"\"A simple text document.\"\"\"\n\n    format: str = \"text\"\n\n\nclass PDFDocument(EmbeddableDocument):\n    \"\"\"A PDF document with additional metadata.\"\"\"\n\n    format: str = \"pdf\"\n    page_count: int\n\n\nclass ImageDocument(BaseDocument):\n    \"\"\"An image document that doesn't need text embedding.\"\"\"\n\n    format: str = \"image\"\n    width: int\n    height: int\n    file_path: str\n\n\n# Step 3: Create a Document Repository\nT = TypeVar('T', bound=BaseDocument)\n\nclass DocumentRepository:\n    \"\"\"Repository for managing documents.\"\"\"\n\n    def __init__(self):\n        self.documents: Dict[UUID, BaseDocument] = {}\n\n    def add(self, document: BaseDocument) -&gt; None:\n        \"\"\"Add a document to the repository.\"\"\"\n        self.documents[document.id] = document\n\n    def get(self, document_id: UUID) -&gt; Optional[BaseDocument]:\n        \"\"\"Get a document by ID.\"\"\"\n        return self.documents.get(document_id)\n\n    def list_all(self) -&gt; List[BaseDocument]:\n        \"\"\"List all documents.\"\"\"\n        return list(self.documents.values())\n\n    def find_by_type(self, doc_type: Type[T]) -&gt; List[T]:\n        \"\"\"Find documents by type.\"\"\"\n        return [doc for doc in self.documents.values() if isinstance(doc, doc_type)]\n\n    def find_by_author(self, author: str) -&gt; List[BaseDocument]:\n        \"\"\"Find documents by author.\"\"\"\n        return [doc for doc in self.documents.values() if doc.author == author]\n\n    def update(self, document: BaseDocument) -&gt; None:\n        \"\"\"Update a document.\"\"\"\n        if document.id in self.documents:\n            # Update the timestamp\n            document.update_timestamp()\n            self.documents[document.id] = document\n\n\n# Step 6: Define Document Event\nclass DocumentEvent(Event):\n    \"\"\"Event for tracking document operations.\"\"\"\n\n    event_type: str\n    document_id: UUID\n    user_id: str\n\n    async def process(self):\n        \"\"\"Process the event.\"\"\"\n        # In a real application, this might log to a database or message queue\n        print(f\"Processing event: {self.event_type} for document {self.document_id}\")\n        return {\"processed\": True, \"timestamp\": datetime.now().isoformat()}\n\n\n# Helper functions\ndef generate_mock_embedding(text: str) -&gt; List[float]:\n    \"\"\"Generate a mock embedding for demonstration purposes.\"\"\"\n    # In a real application, you would use a proper embedding model\n    # This is just a simple hash-based approach for demonstration\n    np_array = np.array([ord(c) for c in text], dtype=np.float32)\n    return (np_array / np.linalg.norm(np_array)).tolist()[:10]  # Normalize and take first 10 dims\n\n\ndef cosine_similarity(a: List[float], b: List[float]) -&gt; float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    dot_product = sum(x * y for x, y in zip(a, b))\n    norm_a = sum(x * x for x in a) ** 0.5\n    norm_b = sum(y * y for y in b) ** 0.5\n    return dot_product / (norm_a * norm_b)\n\n\ndef find_similar_documents(\n    query_doc: EmbeddableDocument,\n    candidates: List[EmbeddableDocument],\n    threshold: float = 0.7\n) -&gt; List[Tuple[EmbeddableDocument, float]]:\n    \"\"\"Find documents similar to the query document.\"\"\"\n    results = []\n    for doc in candidates:\n        if doc.id != query_doc.id:  # Skip the query document itself\n            similarity = cosine_similarity(query_doc.embedding, doc.embedding)\n            if similarity &gt;= threshold:\n                results.append((doc, similarity))\n    return sorted(results, key=lambda x: x[1], reverse=True)\n\n\nasync def track_document_event(\n    event_type: str,\n    document: BaseDocument,\n    user_id: str\n) -&gt; DocumentEvent:\n    \"\"\"Track a document event.\"\"\"\n    event = DocumentEvent(\n        event_type=event_type,\n        document_id=document.id,\n        user_id=user_id,\n        content=f\"{event_type} operation on {document.title} by user {user_id}\"\n    )\n    event._invoke_function = event.process\n    await event.invoke()\n    return event\n\n\n# Main function to demonstrate usage\nasync def main():\n    # Create a repository\n    repo = DocumentRepository()\n\n    # Create some documents\n    text_doc = TextDocument(\n        title=\"Getting Started with Protocols\",\n        author=\"Jane Smith\",\n        content=\"This document explains how to use protocols effectively.\"\n    )\n\n    pdf_doc = PDFDocument(\n        title=\"Advanced Protocol Patterns\",\n        author=\"John Doe\",\n        content=\"Detailed explanation of advanced protocol usage patterns.\",\n        page_count=42\n    )\n\n    image_doc = ImageDocument(\n        title=\"Protocol Architecture Diagram\",\n        author=\"Jane Smith\",\n        width=1920,\n        height=1080,\n        file_path=\"/images/protocol_diagram.png\"\n    )\n\n    # Add documents to the repository\n    repo.add(text_doc)\n    repo.add(pdf_doc)\n    repo.add(image_doc)\n\n    # List all documents\n    print(\"All documents:\")\n    for doc in repo.list_all():\n        print(f\"- {doc}\")\n\n    # Find documents by author\n    print(\"\\nDocuments by Jane Smith:\")\n    for doc in repo.find_by_author(\"Jane Smith\"):\n        print(f\"- {doc}\")\n\n    # Find documents by type\n    print(\"\\nText documents:\")\n    for doc in repo.find_by_type(TextDocument):\n        print(f\"- {doc}\")\n\n    # Update a document\n    text_doc.title = \"Updated: Getting Started with Protocols\"\n    repo.update(text_doc)\n    print(f\"\\nUpdated document timestamp: {text_doc.updated_at}\")\n\n    # Generate embeddings for our documents\n    for doc in repo.find_by_type(EmbeddableDocument):\n        content = doc.create_content()\n        doc.embedding = generate_mock_embedding(content)\n        print(f\"Generated embedding for '{doc.title}' with {len(doc.embedding)} dimensions\")\n\n    # Find similar documents\n    embedable_docs = repo.find_by_type(EmbeddableDocument)\n    similar_docs = find_similar_documents(text_doc, embedable_docs)\n\n    print(\"\\nDocuments similar to 'Updated: Getting Started with Protocols':\")\n    for doc, similarity in similar_docs:\n        print(f\"- {doc.title} (similarity: {similarity:.2f})\")\n\n    # Track document events\n    view_event = await track_document_event(\"view\", text_doc, \"user123\")\n    print(f\"\\nEvent status: {view_event.execution.status}\")\n    print(f\"Event duration: {view_event.execution.duration:.6f} seconds\")\n    print(f\"Event response: {view_event.execution.response}\")\n\n    edit_event = await track_document_event(\"edit\", text_doc, \"user123\")\n    print(f\"Event status: {edit_event.execution.status}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"tutorials/using_protocols/#summary","title":"Summary","text":"<p>In this tutorial, we've demonstrated how to use pydapter's protocols to create standardized models with consistent behavior. We've covered:</p> <ol> <li>Creating base document models with <code>Identifiable</code> and <code>Temporal</code> protocols</li> <li>Adding embedding support with the <code>Embedable</code> protocol</li> <li>Building a document repository to manage our models</li> <li>Working with document embeddings for similarity search</li> <li>Tracking document events with the <code>Invokable</code> and <code>Event</code> protocols</li> </ol> <p>The protocols module provides a powerful way to add standardized capabilities to your models, making your code more consistent and easier to maintain.</p>"}]}